Directory structure:
└── aimlapi-api-docs/
    ├── README.md
    ├── how-to-update-snippets.md
    ├── package.json
    ├── .gitbook.yaml
    ├── docs/
    │   ├── README.md
    │   ├── SUMMARY.md
    │   ├── api-overview/
    │   │   ├── audio-models-music-and-vocal/
    │   │   │   ├── README.md
    │   │   │   ├── minimax-music/
    │   │   │   │   ├── README.md
    │   │   │   │   ├── generate-an-audio.md
    │   │   │   │   └── upload-audio.md
    │   │   │   ├── minimax-music-legacy/
    │   │   │   │   ├── README.md
    │   │   │   │   ├── fetch-generations.md
    │   │   │   │   ├── generate-an-audio.md
    │   │   │   │   └── minimax-docs.yaml
    │   │   │   ├── stable-audio/
    │   │   │   │   ├── README.md
    │   │   │   │   ├── fetch-generations.md
    │   │   │   │   ├── generate-an-audio.md
    │   │   │   │   └── stable-audio-docs.yaml
    │   │   │   ├── suno-ai-v1-legacy/
    │   │   │   │   ├── README.md
    │   │   │   │   ├── custom-audio-generation.md
    │   │   │   │   ├── extend-an-audio.md
    │   │   │   │   ├── fetch-generations.md
    │   │   │   │   └── generate-an-audio.md
    │   │   │   └── suno-ai-v2/
    │   │   │       ├── README.md
    │   │   │       ├── costs.md
    │   │   │       ├── what-is-a-clip.md
    │   │   │       ├── clip-generation/
    │   │   │       │   ├── README.md
    │   │   │       │   ├── clip-continuation-extension.md
    │   │   │       │   ├── fetch-clips.md
    │   │   │       │   ├── generate-with-custom-lyrics.md
    │   │   │       │   └── generate-with-gpt.md
    │   │   │       └── lyrics-generation/
    │   │   │           ├── README.md
    │   │   │           ├── fetch-lyrics.md
    │   │   │           └── generate-lyrics.md
    │   │   ├── guard-models/
    │   │   │   └── README.md
    │   │   ├── image-models/
    │   │   │   ├── README.md
    │   │   │   ├── generate-an-image.md
    │   │   │   └── supported-models.md
    │   │   ├── model-database/
    │   │   │   ├── README.md
    │   │   │   ├── embedding-models.md
    │   │   │   ├── image-models.md
    │   │   │   ├── model-request.md
    │   │   │   ├── music-and-vocal-models.md
    │   │   │   ├── speech-models.md
    │   │   │   ├── text-models.md
    │   │   │   ├── video-models.md
    │   │   │   └── vision-models.md
    │   │   ├── speech-models/
    │   │   │   ├── README.md
    │   │   │   ├── speech-to-text-stt.md
    │   │   │   └── text-to-speech-tts.md
    │   │   ├── text-models-llm/
    │   │   │   ├── README.md
    │   │   │   ├── chat-completion.md
    │   │   │   ├── completion-or-chat-models.md
    │   │   │   ├── embeddings.md
    │   │   │   ├── parameters.md
    │   │   │   ├── function-calling/
    │   │   │   │   ├── README.md
    │   │   │   │   ├── anthropic.md
    │   │   │   │   └── image-to-text-vision.md
    │   │   │   └── managing-assistants-and-threads/
    │   │   │       ├── README.md
    │   │   │       └── threads/
    │   │   │           ├── README.md
    │   │   │           ├── messages.md
    │   │   │           └── runs.md
    │   │   ├── video-models/
    │   │   │   ├── README.md
    │   │   │   ├── kling/
    │   │   │   │   ├── README.md
    │   │   │   │   ├── fetch-generation.md
    │   │   │   │   ├── generate-image-to-video.md
    │   │   │   │   ├── generate-text-to-video.md
    │   │   │   │   └── overview.md
    │   │   │   ├── luma-ai-v1-legacy/
    │   │   │   │   ├── README.md
    │   │   │   │   └── luma-ai-text-to-video.md
    │   │   │   ├── luma-ai-v2/
    │   │   │   │   ├── README.md
    │   │   │   │   ├── extend-video.md
    │   │   │   │   ├── fetch-generations.md
    │   │   │   │   ├── generate-video.md
    │   │   │   │   └── overview.md
    │   │   │   ├── minimax-video/
    │   │   │   │   ├── README.md
    │   │   │   │   ├── fetch-generation.md
    │   │   │   │   └── generate-video.md
    │   │   │   └── runway/
    │   │   │       ├── README.md
    │   │   │       ├── generate-video-1.md
    │   │   │       ├── generate-video.md
    │   │   │       └── overview.md
    │   │   └── vision-models/
    │   │       ├── README.md
    │   │       ├── image-analysis.md
    │   │       ├── ocr-optical-character-recognition.md
    │   │       └── ofr-optical-feature-recognition.md
    │   ├── faq/
    │   │   ├── can-i-use-api-in-nodejs.md
    │   │   ├── can-i-use-api-in-python.md
    │   │   ├── free-tier.md
    │   │   ├── is-api-down-or-it-just-me.md
    │   │   ├── my-requests-are-cropped.md
    │   │   ├── openai-sdk-doesnt-work.md
    │   │   └── pro-models.md
    │   ├── glossary/
    │   │   └── concepts.md
    │   ├── integrations/
    │   │   ├── about.md
    │   │   └── langflow.md
    │   ├── quickstart/
    │   │   ├── setting-up.md
    │   │   └── supported-sdks.md
    │   ├── recipes/
    │   │   ├── models-comparsion.md
    │   │   └── using-assistants-and-threads.md
    │   └── .gitbook/
    │       ├── assets/
    │       └── includes/
    │           ├── untitled (1).md
    │           └── untitled.md
    ├── packages/
    │   └── snippets/
    │       ├── README.md
    │       ├── eslint.config.js
    │       ├── index.html
    │       ├── package.json
    │       ├── pnpm-lock.yaml
    │       ├── tsconfig.app.json
    │       ├── tsconfig.app.tsbuildinfo
    │       ├── tsconfig.json
    │       ├── tsconfig.node.json
    │       ├── tsconfig.node.tsbuildinfo
    │       ├── vite.config.ts
    │       ├── .gitignore
    │       └── src/
    │           ├── inject.tsx
    │           ├── main.ts
    │           ├── vite-env.d.ts
    │           ├── components/
    │           │   ├── app/
    │           │   │   └── index.tsx
    │           │   └── snippet-viewer/
    │           │       ├── index.css
    │           │       ├── index.tsx
    │           │       ├── content/
    │           │       │   ├── index.css
    │           │       │   └── index.tsx
    │           │       ├── footer/
    │           │       │   ├── index.css
    │           │       │   └── index.tsx
    │           │       ├── header/
    │           │       │   ├── index.css
    │           │       │   └── index.tsx
    │           │       └── tab/
    │           │           ├── index.css
    │           │           └── index.tsx
    │           ├── constants/
    │           │   ├── language.ts
    │           │   └── snippets.ts
    │           ├── types/
    │           │   ├── snippet.ts
    │           │   └── utils.ts
    │           └── utils/
    │               ├── compile-snippets.ts
    │               ├── load-page-snippets.ts
    │               └── load-template-snippets.ts
    ├── shared/
    │   └── snippets/
    │       ├── 3d/
    │       │   └── triposr/
    │       │       ├── config.json
    │       │       ├── js.hbs
    │       │       └── py.hbs
    │       ├── anthropic/
    │       │   └── messages/
    │       │       ├── config.json
    │       │       ├── js.hbs
    │       │       └── py.hbs
    │       ├── audio/
    │       │   ├── create-generation-minimax/
    │       │   │   ├── config.json
    │       │   │   ├── js.hbs
    │       │   │   └── py.hbs
    │       │   ├── create-generation-stable/
    │       │   │   ├── config.json
    │       │   │   ├── js.hbs
    │       │   │   └── py.hbs
    │       │   └── fetch-generation/
    │       │       ├── config.json
    │       │       ├── js.hbs
    │       │       └── py.hbs
    │       ├── image/
    │       │   ├── flux/
    │       │   │   ├── config.json
    │       │   │   ├── js.hbs
    │       │   │   └── py.hbs
    │       │   ├── flux-image-to-image/
    │       │   │   ├── config.json
    │       │   │   ├── js.hbs
    │       │   │   └── py.hbs
    │       │   ├── openai/
    │       │   │   ├── config.json
    │       │   │   ├── js.hbs
    │       │   │   └── py.hbs
    │       │   ├── recraft/
    │       │   │   ├── config.json
    │       │   │   ├── js.hbs
    │       │   │   └── py.hbs
    │       │   ├── stability/
    │       │   │   ├── config.json
    │       │   │   ├── js.hbs
    │       │   │   └── py.hbs
    │       │   └── together/
    │       │       ├── config.json
    │       │       ├── js.hbs
    │       │       └── py.hbs
    │       ├── kling/
    │       │   ├── create-image-to-video-generation/
    │       │   │   ├── config.json
    │       │   │   ├── js.hbs
    │       │   │   └── py.hbs
    │       │   ├── create-text-to-video-generation/
    │       │   │   ├── config.json
    │       │   │   ├── js.hbs
    │       │   │   └── py.hbs
    │       │   └── fetch-generation/
    │       │       ├── config.json
    │       │       ├── js.hbs
    │       │       └── py.hbs
    │       ├── luma-ai/
    │       │   ├── create-generation/
    │       │   │   ├── config.json
    │       │   │   ├── js.hbs
    │       │   │   └── py.hbs
    │       │   └── fetch-generation/
    │       │       ├── config.json
    │       │       ├── js.hbs
    │       │       └── py.hbs
    │       ├── minimax/
    │       │   ├── create-audio-generation/
    │       │   │   ├── config.json
    │       │   │   ├── js.hbs
    │       │   │   └── py.hbs
    │       │   ├── create-video-generation/
    │       │   │   ├── config.json
    │       │   │   ├── js.hbs
    │       │   │   └── py.hbs
    │       │   ├── fetch-video-generation/
    │       │   │   ├── config.json
    │       │   │   ├── js.hbs
    │       │   │   └── py.hbs
    │       │   └── upload-audio/
    │       │       ├── config.json
    │       │       ├── js.hbs
    │       │       └── py.hbs
    │       ├── open-ai/
    │       │   ├── chat-completion/
    │       │   │   ├── config.json
    │       │   │   ├── js.hbs
    │       │   │   └── py.hbs
    │       │   ├── chat-completion-code/
    │       │   │   ├── config.json
    │       │   │   ├── js.hbs
    │       │   │   └── py.hbs
    │       │   ├── chat-completion-o1/
    │       │   │   ├── config.json
    │       │   │   ├── js.hbs
    │       │   │   └── py.hbs
    │       │   ├── completion/
    │       │   │   ├── config.json
    │       │   │   ├── js.hbs
    │       │   │   └── py.hbs
    │       │   ├── embedding/
    │       │   │   ├── config.json
    │       │   │   ├── js.hbs
    │       │   │   └── py.hbs
    │       │   ├── guard/
    │       │   │   ├── config.json
    │       │   │   ├── js.hbs
    │       │   │   └── py.hbs
    │       │   └── image-generation/
    │       │       ├── config.json
    │       │       ├── js.hbs
    │       │       └── py.hbs
    │       ├── runway/
    │       │   ├── create-generation/
    │       │   │   ├── config.json
    │       │   │   ├── js.hbs
    │       │   │   └── py.hbs
    │       │   └── fetch-generation/
    │       │       ├── config.json
    │       │       ├── js.hbs
    │       │       └── py.hbs
    │       ├── suno-ai/
    │       │   └── clip-generation/
    │       │       ├── config.json
    │       │       ├── js.hbs
    │       │       └── py.hbs
    │       ├── together-ai/
    │       │   └── chat-completion-vision/
    │       │       ├── config.json
    │       │       ├── js.hbs
    │       │       └── py.hbs
    │       ├── video/
    │       │   └── runway/
    │       │       ├── config.json
    │       │       ├── js.hbs
    │       │       └── py.hbs
    │       └── voice/
    │           ├── stt/
    │           │   ├── config.json
    │           │   ├── js.hbs
    │           │   └── py.hbs
    │           └── tts/
    │               ├── config.json
    │               ├── js.hbs
    │               └── py.hbs
    └── .github/
        └── workflows/
            └── deploy-snippets-assets.yml

================================================
File: README.md
================================================
# AI/ML API Documentation Repo

Welcome to the AI/ML API documentation!

## Updating the Snippets Code

For guidance on updating the snippets code, please refer to the [How to Update Snippets Guide](/how-to-update-snippets.md).


================================================
File: how-to-update-snippets.md
================================================
# Code Snippets

## How to Use

### Presequences

Add the following to the `<head>` section of your site:

```html
<script async type="module" src="https://cdn.aimlapi.com/assets/snippets/bundle.js"></script>
<link rel="stylesheet" href="https://cdn.aimlapi.com/assets/snippets/bundle.css">
```

### Adding the Snippet

The script finds elements that match the selector `snippet[data-name]`. On your page, you should add snippets in the following way:

```html
<snippet data-name="open-ai.chat-completion"></snippet>
```

After page initialization, the script finds these snippets and attaches code to them.

### Snippet Templating

Snippets are built based on templates, and we currently use the Handlebars templating format.

For example, the template for OpenAI chat completion can look like the following:

```hbs
const api = new OpenAI({
  baseURL: '{{baseUrl}}',
  apiKey: '{{apiKey}}',
});
```

If any of the template values are defined by default within the snippet configuration, they will be set to the corresponding value.

You can overwrite any default templated value by setting the same template variable as a data attribute:

```html
<snippet data-name="open-ai.chat-completion" data-base-url="https://api.aimlapi.com/v2"></snippet>
```

This change updates the `baseUrl` template value to use the `v2` route, and the template will be generated with this updated route.

> **Warning**: Be careful if you wish to use a variable named `name`, as it is already used as a template name.
## Structure and Modification

Snippets are stored in the `/shared/snippets` directory. Each snippet is organized by group and name, alongside its default configuration and support for multiple languages.

### Structure Example

```yml
snippets:
  open-ai: # Name of the group
    chat-completion: # Name of the snippet
      - config.json   # Snippet configuration
      - js.hbs        # Snippet code in JavaScript using Handlebars template format
      - py.hbs        # Snippet code in Python using Handlebars template format
    chat-completion-code: # Another snippet under the same group
      - config.json
      - js.hbs
      - py.hbs
  together-ai:
    chat-completion-vision: # A snippet under a different group
      - config.json 
      - js.hbs
      - py.hbs
```
### Configuration Example

The configuration file is stored in the template folder and named `config.json`. This file contains information about the snippet and default values, which are used as template variables.

```json
{
  "title": "Create an embedding", // This title will be used as a title in the snippet header
  "docs": "https://docs.aimlapi.com/api-overview/text-models-llm/embeddings", // If a link exists, it will be shown in the snippet
  "payload": {
    "baseUrl": "https://api.aimlapi.com/v1",
    "apiKey": "<YOUR_API_KEY>"
  }
}
```

You can add any values to the `payload` section and use them in a way that Handlebars format supports. These values will be used as defaults and can be overwritten by data attributes.

> **Warning**: It's advisable to set template names in camel case notation because overwriting them with data attributes might be challenging. Data attributes automatically convert from kebab case (commonly used) to camel case.
## Adding the Snippets

### Check if the Group Already Exists

Snippet groups are based on the payload type being used. For example, if the chat completion format is used by multiple providers and a snippet uses the OpenAI SDK, then you should place this snippet in the same folder.

If the group does not exist, you can create a new group (folder) and name it appropriately.

### Creating a Snippet

1. **Create a New Folder:** Inside the necessary group, create a new folder with a readable snippet name in kebab case.

2. **Create Configuration File:** Inside this snippet folder, create the configuration file `config.json`. Although it can be empty initially, it's recommended to add `title`, `docs`, and any future templated values to the `payload` object as described earlier.

3. **Create Template Files:** Create one or more files named after programming languages or formats with `.hbs` extensions. Examples of valid names include:
   - `python.hbs`
   - `py.hbs`
   - `html.hbs`
   - `js.hbs`

   Invalid examples:
   - `code.hbs`
   - `js.txt`
   - `template`

4. **Write Snippet Code:** Inside these files, write your snippet code using Handlebars templating features.

Examples of written scripts can be found in the [/shared/snippets](/shared/snippets) directory.

> **Warning**: Ensure you use kebab-case notation for folders and files. Preferably use camel case notation for templating values to avoid conflicts when they are converted automatically from data attributes.
> 
## Deploying
After pushing changes to the `main` branch, they automatically propagate and are applied. In some cases, there might be a delay in seeing the results due to script caching by Cloudflare.

================================================
File: package.json
================================================
{
  "name": "api-docs",
  "version": "1.0.0",
  "description": "",
  "main": "index.js",
  "scripts": {
    "test": "echo \"Error: no test specified\" && exit 1"
  },
  "keywords": [],
  "author": "",
  "license": "ISC",
  "dependencies": {}
}


================================================
File: .gitbook.yaml
================================================
root: ./docs/


================================================
File: docs/README.md
================================================
---
layout:
  title:
    visible: true
  description:
    visible: true
  tableOfContents:
    visible: true
  outline:
    visible: true
  pagination:
    visible: true
---

# 🤖 Introduction

## Welcome to the AI/ML API!

<table data-view="cards"><thead><tr><th></th><th></th><th></th><th data-type="content-ref"></th><th data-hidden data-type="files"></th><th data-hidden data-card-cover data-type="files"></th></tr></thead><tbody><tr><td><a data-mention href="quickstart/setting-up.md">setting-up.md</a></td><td><a data-mention href="quickstart/supported-sdks.md">supported-sdks.md</a></td><td></td><td></td><td><a href=".gitbook/assets/kit.png">kit.png</a></td><td><a href=".gitbook/assets/Group 39481.png">Group 39481.png</a></td></tr><tr><td><a data-mention href="recipes/models-comparsion.md">models-comparsion.md</a></td><td><a data-mention href="api-overview/text-models-llm/parameters.md">parameters.md</a></td><td><a href="https://docs.aimlapi.com/recipes/using-assistants-and-threads">Using Assistants &#x26; Threads</a></td><td></td><td><a href=".gitbook/assets/Code.png">Code.png</a></td><td><a href=".gitbook/assets/Group 39482.png">Group 39482.png</a></td></tr><tr><td><a data-mention href="api-overview/model-database/">model-database</a></td><td><a data-mention href="api-overview/text-models-llm/chat-completion.md">chat-completion.md</a></td><td></td><td></td><td><a href=".gitbook/assets/Link.png">Link.png</a></td><td><a href=".gitbook/assets/Group 39483.png">Group 39483.png</a></td></tr></tbody></table>

## Featured Models

{% content-ref url="api-overview/text-models-llm/" %}
[text-models-llm](api-overview/text-models-llm/)
{% endcontent-ref %}

{% content-ref url="api-overview/model-database/image-models.md" %}
[image-models.md](api-overview/model-database/image-models.md)
{% endcontent-ref %}

{% content-ref url="api-overview/model-database/vision-models.md" %}
[vision-models.md](api-overview/model-database/vision-models.md)
{% endcontent-ref %}

{% content-ref url="api-overview/audio-models-music-and-vocal/" %}
[audio-models-music-and-vocal](api-overview/audio-models-music-and-vocal/)
{% endcontent-ref %}

{% content-ref url="api-overview/model-database/speech-models.md" %}
[speech-models.md](api-overview/model-database/speech-models.md)
{% endcontent-ref %}

{% content-ref url="api-overview/video-models/" %}
[video-models](api-overview/video-models/)
{% endcontent-ref %}

{% content-ref url="api-overview/guard-models/" %}
[guard-models](api-overview/guard-models/)
{% endcontent-ref %}

## **Overview of the API**

AI/ML API provides a suite of powerful AI functionalities, including text completion, image inference, speech to text, and text to speech. Our API ensures effortless integration, robust performance, and secure API key management.

### **Key Features**

* **Inference**: Easily evaluate models for text, images, and more.
* **API Key Management**: Securely manage your API keys.
* **Broad Model Selection**: Access a diverse range of models for various AI tasks.

## Getting Started

To start using the AI/ML API, follow the [Quickstart guide](quickstart/setting-up.md) to set up your environment and make your first API call.


================================================
File: docs/SUMMARY.md
================================================
# Table of contents

* [Contact Us](https://webforms.pipedrive.com/f/6qbDVqej0gFSr3yerhf5KALZ1IwAfvGYuM8imBNuRWalzRyQOcUbNZt2QVSFF9Blrd)
* [🤖 Introduction](README.md)

## Quickstart

* [Setting Up](quickstart/setting-up.md)
* [Supported SDKs](quickstart/supported-sdks.md)

## API Overview

* [Text Models (LLM)](api-overview/text-models-llm/README.md)
  * [Completion or Chat Models](api-overview/text-models-llm/completion-or-chat-models.md)
  * [Parameters](api-overview/text-models-llm/parameters.md)
  * [Chat Completion](api-overview/text-models-llm/chat-completion.md)
  * [Function Calling](api-overview/text-models-llm/function-calling/README.md)
    * [Image-To-Text (Vision)](api-overview/text-models-llm/function-calling/image-to-text-vision.md)
    * [Anthropic](api-overview/text-models-llm/function-calling/anthropic.md)
  * [Managing Assistants & Threads](api-overview/text-models-llm/managing-assistants-and-threads/README.md)
    * [Threads](api-overview/text-models-llm/managing-assistants-and-threads/threads/README.md)
      * [Runs](api-overview/text-models-llm/managing-assistants-and-threads/threads/runs.md)
      * [Messages](api-overview/text-models-llm/managing-assistants-and-threads/threads/messages.md)
  * [Embeddings](api-overview/text-models-llm/embeddings.md)
* [Image Models](api-overview/image-models/README.md)
  * [Generate an Image](api-overview/image-models/generate-an-image.md)
  * [Supported Models](api-overview/image-models/supported-models.md)
* [Vision Models](api-overview/vision-models/README.md)
  * [Image Analysis](api-overview/vision-models/image-analysis.md)
  * [OCR: Optical Character Recognition](api-overview/vision-models/ocr-optical-character-recognition.md)
  * [OFR: Optical Feature Recognition](api-overview/vision-models/ofr-optical-feature-recognition.md)
* [Audio Models (Music & Vocal)](api-overview/audio-models-music-and-vocal/README.md)
  * [Suno AI v2 (deprecated)](api-overview/audio-models-music-and-vocal/suno-ai-v2/README.md)
    * [What is a Clip](api-overview/audio-models-music-and-vocal/suno-ai-v2/what-is-a-clip.md)
    * [Costs](api-overview/audio-models-music-and-vocal/suno-ai-v2/costs.md)
    * [Clip Generation](api-overview/audio-models-music-and-vocal/suno-ai-v2/clip-generation/README.md)
      * [Generate with GPT](api-overview/audio-models-music-and-vocal/suno-ai-v2/clip-generation/generate-with-gpt.md)
      * [Fetch Clips](api-overview/audio-models-music-and-vocal/suno-ai-v2/clip-generation/fetch-clips.md)
      * [Generate with Custom Lyrics](api-overview/audio-models-music-and-vocal/suno-ai-v2/clip-generation/generate-with-custom-lyrics.md)
      * [Clip Continuation / Extension](api-overview/audio-models-music-and-vocal/suno-ai-v2/clip-generation/clip-continuation-extension.md)
    * [Lyrics Generation](api-overview/audio-models-music-and-vocal/suno-ai-v2/lyrics-generation/README.md)
      * [Generate Lyrics](api-overview/audio-models-music-and-vocal/suno-ai-v2/lyrics-generation/generate-lyrics.md)
      * [Fetch Lyrics](api-overview/audio-models-music-and-vocal/suno-ai-v2/lyrics-generation/fetch-lyrics.md)
  * [Suno AI v1 (deprecated)](api-overview/audio-models-music-and-vocal/suno-ai-v1-legacy/README.md)
    * [Generate an Audio](api-overview/audio-models-music-and-vocal/suno-ai-v1-legacy/generate-an-audio.md)
    * [Custom Audio Generation](api-overview/audio-models-music-and-vocal/suno-ai-v1-legacy/custom-audio-generation.md)
    * [Fetch Generations](api-overview/audio-models-music-and-vocal/suno-ai-v1-legacy/fetch-generations.md)
    * [Extend an Audio](api-overview/audio-models-music-and-vocal/suno-ai-v1-legacy/extend-an-audio.md)
  * [Stable Audio](api-overview/audio-models-music-and-vocal/stable-audio/README.md)
    * [Generate an audio (Stable Audio)](api-overview/audio-models-music-and-vocal/stable-audio/generate-an-audio.md)
    * [Fetch an audio (Stable Audio)](api-overview/audio-models-music-and-vocal/stable-audio/fetch-generations.md)
  * [Minimax Music](api-overview/audio-models-music-and-vocal/minimax-music/README.md)
    * [Upload audio (Minimax Music)](api-overview/audio-models-music-and-vocal/minimax-music/upload-audio.md)
    * [Generate an audio (Minimax Music)](api-overview/audio-models-music-and-vocal/minimax-music/generate-an-audio.md)
  * [Minimax Music (legacy)](api-overview/audio-models-music-and-vocal/minimax-music-legacy/README.md)
    * [Generate an audio (Minimax Music)](api-overview/audio-models-music-and-vocal/minimax-music-legacy/generate-an-audio.md)
    * [Fetch an audio (Minimax Music)](api-overview/audio-models-music-and-vocal/minimax-music-legacy/fetch-generations.md)
* [Speech Models](api-overview/speech-models/README.md)
  * [Speech-to-Text (STT)](api-overview/speech-models/speech-to-text-stt.md)
  * [Text-to-Speech (TTS)](api-overview/speech-models/text-to-speech-tts.md)
* [Video Models](api-overview/video-models/README.md)
  * [Kling](api-overview/video-models/kling/README.md)
    * [Overview](api-overview/video-models/kling/overview.md)
    * [Generate Text to Video](api-overview/video-models/kling/generate-text-to-video.md)
    * [Generate Image to Video](api-overview/video-models/kling/generate-image-to-video.md)
    * [Fetch Generation](api-overview/video-models/kling/fetch-generation.md)
  * [Luma AI v2](api-overview/video-models/luma-ai-v2/README.md)
    * [Overview](api-overview/video-models/luma-ai-v2/overview.md)
    * [Generate Video](api-overview/video-models/luma-ai-v2/generate-video.md)
    * [Fetch Generations](api-overview/video-models/luma-ai-v2/fetch-generations.md)
    * [Extend Video](api-overview/video-models/luma-ai-v2/extend-video.md)
  * [Luma AI v1 (legacy)](api-overview/video-models/luma-ai-v1-legacy/README.md)
    * [Luma AI Text-to-Video](api-overview/video-models/luma-ai-v1-legacy/luma-ai-text-to-video.md)
  * [MiniMax Video](api-overview/video-models/minimax-video/README.md)
    * [Generate Video](api-overview/video-models/minimax-video/generate-video.md)
    * [Fetch Generation](api-overview/video-models/minimax-video/fetch-generation.md)
  * [Runway](api-overview/video-models/runway/README.md)
    * [Overview](api-overview/video-models/runway/overview.md)
    * [Generate Video](api-overview/video-models/runway/generate-video.md)
    * [Fetch Generation](api-overview/video-models/runway/generate-video-1.md)
* [Guard models](api-overview/guard-models/README.md)
* [Model Database](api-overview/model-database/README.md)
  * [Text Models](api-overview/model-database/text-models.md)
  * [Video Models](api-overview/model-database/video-models.md)
  * [Image Models](api-overview/model-database/image-models.md)
  * [Vision Models](api-overview/model-database/vision-models.md)
  * [Speech Models](api-overview/model-database/speech-models.md)
  * [Music & Vocal Models](api-overview/model-database/music-and-vocal-models.md)
  * [Embedding Models](api-overview/model-database/embedding-models.md)
  * [Model Request](api-overview/model-database/model-request.md)

## Recipes

* [Models comparsion](recipes/models-comparsion.md)
* [Using Assistants & Threads](recipes/using-assistants-and-threads.md)

## FAQ

* [Can I use API in Python?](faq/can-i-use-api-in-python.md)
* [Can I use API in NodeJS?](faq/can-i-use-api-in-nodejs.md)
* [What are the Pro Models?](faq/pro-models.md)
* [How to use the Free Tier?](faq/free-tier.md)
* [Is API down or it just me?](faq/is-api-down-or-it-just-me.md)
* [Are my requests cropped?](faq/my-requests-are-cropped.md)
* [OpenAI SDK doesn't work?](faq/openai-sdk-doesnt-work.md)

## Glossary

* [Concepts](glossary/concepts.md)

## Integrations

* [About](integrations/about.md)
* [Langflow](integrations/langflow.md)


================================================
File: docs/api-overview/audio-models-music-and-vocal/README.md
================================================
---
icon: guitar
description: Overview of the capabilities of AIML API audio models.
layout:
  title:
    visible: true
  description:
    visible: false
  tableOfContents:
    visible: true
  outline:
    visible: true
  pagination:
    visible: true
---

# Audio Models (Music & Vocal)

## Overview

Our API features the capability to generate audio. With this API, you can create your own music, speech, and any audio experience from your prompt and imagination.

## Providers

{% content-ref url="stable-audio/" %}
[stable-audio](stable-audio/)
{% endcontent-ref %}

{% content-ref url="minimax-music/" %}
[minimax-music](minimax-music/)
{% endcontent-ref %}

{% content-ref url="minimax-music-legacy/" %}
[minimax-music-legacy](minimax-music-legacy/)
{% endcontent-ref %}


================================================
File: docs/api-overview/audio-models-music-and-vocal/minimax-music/README.md
================================================
---
icon: satellite-dish
---

# Minimax Music

## Overview

Our API features the capability to generate audio. With this API, you can create your own music, speech, and any audio experience from your prompt and imagination.

### Key features

{% content-ref url="upload-audio.md" %}
[upload-audio.md](upload-audio.md)
{% endcontent-ref %}

{% content-ref url="generate-an-audio.md" %}
[generate-an-audio.md](generate-an-audio.md)
{% endcontent-ref %}


================================================
File: docs/api-overview/audio-models-music-and-vocal/minimax-music/generate-an-audio.md
================================================
---
icon: code
---

# Generate an Audio

## Overview

* **High-Quality Music Generation**: Create music tracks based on descriptive prompts.
* **Flexible Integration**: Compatible with various programming languages and frameworks.

## Consumption

1 audio file will be generated for each request, consuming a total of 63 000 AI/ML Tokens.

## API Reference

{% swagger src="https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27" path="/v2/generate/audio/minimax/generate" method="post" %}
[https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27](https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27)
{% endswagger %}

## Examples

{% tabs %}
{% tab title="JavaScript" %}
```javascript
// npm install node-fetch form-data
import fs from "fs";
import fetch from "node-fetch";

const generateMusic = async () => {
  const url = "https://api.aimlapi.com/v2/generate/audio/minimax/generate";
  const voiceId = "vocal-2025010100000000-a0AAAaaa";
  const referInstrumental = "instrumental-2025010100000000-Aaa0aAaA";

  // Sample lyrics
  const lyrics = `
  ##
  I see a skyline painted in dreams,
  The colors shift, nothing's as it seems.
  You’re the whisper in a crowded night,
  The only constant in a world of flight.

  And I wonder, where the road will go,
  Through the chaos, you’re the one I know.

  In the future, there’s a place for us,
  Where time will fade, but not our trust.
  Through the echoes of what’s yet to be,
  You’re my forever, my destiny.
  ##
  `;

  const payload = {
    refer_voice: voiceId,
    refer_instrumental: referInstrumental,
    lyrics: lyrics,
    model: "music-01",
  };

  const apiKey = "<YOUR_API_KEY>"; 

  const headers = {
    "Content-Type": "application/json",
    "Authorization": `Bearer ${apiKey}`,
  };

  try {
    const response = await fetch(url, {
      method: "POST",
      headers: headers,
      body: JSON.stringify(payload),
    });

    const data = await response.json();

    if (response.ok) {
      const audioHex = data.data.audio;
      const decodedHex = Buffer.from(audioHex, "hex");

      // Convert to mp3
      fs.writeFileSync("generated_audio.mp3", decodedHex);
      console.log("Audio file saved as generated_audio.mp3");
    } else {
      console.error("Failed to generate music:", data);
    }
  } catch (error) {
    console.error("Error during API request:", error);
  }
}

generateMusic();

```
{% endtab %}

{% tab title="Python" %}
```python
import requests

def generate_music():
    api_key = '<YOUR_API_KEY>'
    url = 'https://api.aimlapi.com/v2/generate/audio/minimax/generate'

    voice_id = 'vocal-2025010100000000-a0AAAaaa'
    refer_instrumental = 'instrumental-2025010100000000-Aaa0aAaA'

    #Sample lyrics
    lyrics = '''
    ##
    I see a skyline painted in dreams,
    The colors shift, nothing's as it seems.
    You’re the whisper in a crowded night,
    The only constant in a world of flight.

    And I wonder, where the road will go,
    Through the chaos, you’re the one I know.

    In the future, there’s a place for us,
    Where time will fade, but not our trust.
    Through the echoes of what’s yet to be,
    You’re my forever, my destiny.
    ##
    '''

    payload = {
        'refer_voice': voice_id,
        'refer_instrumental': refer_instrumental,
        'lyrics':  lyrics,
        'model': 'music-01',
        }
    headers = {
        'Content-Type': 'application/json',
        'authorization': 'Bearer ' + api_key,
    }

    response = requests.post(url, headers=headers, json=payload)

    audio_hex = response.json()['data']['audio']
    decoded_hex = bytes.fromhex(audio_hex)

    # convert to mp3
    with open('generated_audio.mp3', 'wb') as f:
        f.write(decoded_hex)

if __name__ == '__main__':
    generate_music()

```
{% endtab %}
{% endtabs %}


================================================
File: docs/api-overview/audio-models-music-and-vocal/minimax-music/upload-audio.md
================================================
---
icon: brackets-curly
---

# Upload Audio

## API Reference

{% swagger src="https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27" path="/v2/generate/audio/minimax/upload" method="post" %}
[https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27](https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27)
{% endswagger %}

## Examples

{% tabs %}
{% tab title="JavaScript" %}
```javascript
// npm install node-fetch form-data
import fs from "fs";
import path from "path";
import fetch from "node-fetch";
import FormData from "form-data";

const upload = async () => {
  const url = "https://api.aimlapi.com/v2/generate/audio/minimax/upload";
  const fileName = "filename.mp3";
  const filePath = path.resolve(fileName); // Path to your file

  const purpose = "song"; // Possible values: "song", "voice", "instrumental"

  const formData = new FormData();
  formData.append("purpose", purpose);
  formData.append("file", fs.createReadStream(filePath), {
    filename: fileName,
    contentType: "audio/mpeg",
  });

  const apiKey = "<YOUR_API_KEY>"; 

  try {
    const response = await fetch(url, {
      method: "POST",
      headers: {
        Authorization: `Bearer ${apiKey}`,
        ...formData.getHeaders(),
      },
      body: formData,
    });

    if (response.ok) {
      const data = await response.json();
      console.log("Upload successful:", data);
    } else {
      console.error("Upload failed:", response.status, await response.text());
    }
  } catch (error) {
    console.error("Error during upload:", error.message);
  }
}

upload();

```
{% endtab %}

{% tab title="Python" %}
```python
import requests

def upload():
    api_key = '<YOUR_API_KEY>'
    url = 'https://api-staging.aimlapi.com/v2/generate/audio/minimax/upload'
    file_name = 'file_name.mp3'
    # Your path to the song
    file_path = 'file_path.mp3'

    purpose = 'song' # Possible values: 'song', 'voice', 'instrumental'

    payload = {
        'purpose': purpose,
    }
    files = [
        ('file', (file_name, open(file_path, 'rb'), 'audio/mpeg'))
    ]
    headers = {
        'authorization': 'Bearer ' + api_key,
    }

    response = requests.request('POST', url, headers=headers, data=payload, files=files)
    print(response.json())

if __name__ == '__main__':
    upload()

```
{% endtab %}
{% endtabs %}

### Upload file with link

{% tabs %}
{% tab title="JavaScript" %}
```javascript
// npm install node-fetch form-data
import fs from "fs";
import path from "path";
import fetch from "node-fetch";
import FormData from "form-data";

const upload = async () => {
  const url = "https://api.aimlapi.com/v2/generate/audio/minimax/upload";
  const audioUrl = "url_to_file.mp3";  // Replace with the actual URL of the file
  const fileName = "filename.mp3";  // The name you want to give to the uploaded file
  const purpose = "song"; // Possible values: "song", "voice", "instrumental"

  const apiKey = "<YOUR_API_KEY>";  // Replace with your actual API key

  try {
  // Download the file from the URL
  const response = await fetch(audioUrl);
  if (!response.ok) {
    throw new Error(`Failed to fetch file from URL: ${audioUrl}`);
  }
  
  const fileBuffer = await response.arrayBuffer(); // Get the file as a buffer

  // Upload the file to the server using FormData
  const formData = new FormData();
  formData.append("purpose", purpose);
  formData.append("file", Buffer.from(fileBuffer), {
    filename: fileName,
    contentType: "audio/mpeg",
  });

  const uploadResponse = await fetch(url, {
    method: "POST",
    headers: {
    Authorization: `Bearer ${apiKey}`,
    ...formData.getHeaders(),
    },
    body: formData,
  });

  if (uploadResponse.ok) {
    const data = await uploadResponse.json();
    console.log("Upload successful:", data);
  } else {
    console.error("Upload failed:", uploadResponse.status, await uploadResponse.text());
  }
  } catch (error) {
  console.error("Error during upload:", error);
  }
};

upload();

```
{% endtab %}

{% tab title="Python" %}
```python
import requests

def upload():
    url = 'https://api.aimlapi.com/v2/generate/audio/minimax/upload'
    audio_url = 'url_to_file.mp3'  # Replace with the actual URL of the file
    file_name = 'filename.mp3'  # The name you want to give to the uploaded file
    purpose = 'song'  # Possible values: 'song', 'voice', 'instrumental'

    api_key = '<YOUR_API_KEY>'  # Replace with your actual API key

    try:
        # Step 1: Download the file from the URL
        response = requests.get(audio_url)
        if response.status_code != 200:
            print(f'Failed to download file from URL: {audio_url}')
            return
        
        payload = {
            'purpose': purpose,
        }
        # Step 2: Upload the file to the server
        files = {
            'file': (file_name, response.content, 'audio/mpeg')  # File content as bytes
        }
        headers = {
            'Authorization': f'Bearer {api_key}'
        }

        upload_response = requests.post(url, headers=headers, files=files, data=payload)

        if upload_response.status_code == 200:
            data = upload_response.json()
            print('Upload successful:', data)
        else:
            print('Upload failed:', upload_response.status_code, upload_response.text)
    
    except requests.exceptions.RequestException as error:
        print(f'Error during upload: {error}')

if __name__ == '__main__':
    upload()

```
{% endtab %}
{% endtabs %}


================================================
File: docs/api-overview/audio-models-music-and-vocal/minimax-music-legacy/README.md
================================================
---
icon: satellite-dish
---

# Minimax Music (legacy)

## Overview

Our API features the capability to generate audio. With this API, you can create your own music, speech, and any audio experience from your prompt and imagination.

### Key features

{% content-ref url="generate-an-audio.md" %}
[generate-an-audio.md](generate-an-audio.md)
{% endcontent-ref %}

{% content-ref url="fetch-generations.md" %}
[fetch-generations.md](fetch-generations.md)
{% endcontent-ref %}


================================================
File: docs/api-overview/audio-models-music-and-vocal/minimax-music-legacy/fetch-generations.md
================================================
---
icon: brackets-curly
---

# Fetch an audio (Minimax Music)

## API Reference

{% swagger src="https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27" path="/v2/generate/audio" method="get" %}
[https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27](https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27)
{% endswagger %}

## Examples

{% tabs %}
{% tab title="JavaScript" %}
```javascript
const main = async () => {
  const params = new URLSearchParams({
    generation_id: "<YOUR_GENERATION_ID>"
  });
  const response = await fetch(`https://api.aimlapi.com/v2/generate/audio?${params.toString()}`, {
    method: 'GET',
    headers: {
      Authorization: 'Bearer <YOUR_API_KEY>',
      'Content-Type': 'application/json',
    },
  }).then((res) => res.json());

  console.log('Generation:', response);
};

main()

```
{% endtab %}

{% tab title="Python" %}
```python
import requests


def main():
    url = "https://api.aimlapi.com/v2/generate/audio"
    params = {
        "generation_id": "<YOUR_GENERATION_ID>",
    }
    headers = {"Authorization": "Bearer <YOUR_API_KEY>", "Content-Type": "application/json"}

    response = requests.get(url, params=params, headers=headers)
    print("Generation:", response.json())

if __name__ == "__main__":
    main()

```
{% endtab %}
{% endtabs %}


================================================
File: docs/api-overview/audio-models-music-and-vocal/minimax-music-legacy/generate-an-audio.md
================================================
---
icon: code
---

# Generate an Audio

## Overview

* **High-Quality Music Generation**: Create music tracks based on descriptive prompts.
* **Flexible Integration**: Compatible with various programming languages and frameworks.

## Consumption

1 audio file will be generated for each request, consuming a total of 73 500 AI/ML Tokens.

## API Reference

{% swagger src="minimax-docs.yaml" path="/v2/generate/audio" method="post" %}
[minimax-docs.yaml](minimax-docs.yaml)
{% endswagger %}

## Examples

{% tabs %}
{% tab title="JavaScript" %}
```javascript
const main = async () => {
  const response = await fetch("https://api.aimlapi.com/v2/generate/audio", {
    method: "POST",
    headers: {
      Authorization: "Bearer <YOUR_API_KEY>",
      "Content-Type": "application/json",
    },
    body: JSON.stringify({
      model: "minimax-music",
      prompt: "## Fast and Limitless. In the heart of the code, where dreams collide, FALs the name, taking tech for a ride. Generative media, blazing the trail, Fast inference power, we'll never fail.##",
      reference_audio_url: "https://cdn.aimlapi.com/squirrel/files/zebra/WzNbqH7vR20MNTOD1Ec7k_output.mp3",
    })
  }).then((res) => res.json());

  console.log("Generation:", response);
};

main()

```
{% endtab %}

{% tab title="Python" %}
```python
import requests


def main():
    url = "https://api.aimlapi.com/v2/generate/audio"
    payload = {
        "model": "minimax-music",
        "prompt": "## Fast and Limitless. In the heart of the code, where dreams collide, FALs the name, taking tech for a ride. Generative media, blazing the trail, Fast inference power, we'll never fail.##",
        "reference_audio_url": "https://cdn.aimlapi.com/squirrel/files/zebra/WzNbqH7vR20MNTOD1Ec7k_output.mp3",
    }
    headers = {"Authorization": "Bearer <YOUR_API_KEY>", "Content-Type": "application/json"}

    response = requests.post(url, json=payload, headers=headers)
    print("Generation:", response.json())


if __name__ == "__main__":
    main()

```
{% endtab %}
{% endtabs %}


================================================
File: docs/api-overview/audio-models-music-and-vocal/minimax-music-legacy/minimax-docs.yaml
================================================
openapi: 3.0.0
paths:
  /v2/generate/audio:
      post:
        operationId: AudioController_generateAudioByPrompt_v2
        summary: Generate audio using audio models.
        description: >-
          Generates audio based on a text prompt using audio models, useful for
          creating audio content or responses.
        parameters: [ ]
        requestBody:
          required: true
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/GenerateAudioByPromptDTO'
        responses:
          '201':
            description: ''
            content:
              application/json:
                schema:
                  $ref: '#/components/schemas/GenerateAudioOrVideoResponseDTO'
        tags:
          - Generate an audio
      get:
        operationId: AudioController_getAudioInformation_v2
        summary: Get information about generated audio.
        description: >-
          Retrieves information about audio generated.
        parameters:
          - name: generation_id
            required: true
            in: query
            schema:
              type: string
        responses:
          '201':
            description: ''
            content:
              application/json:
                schema:
                  $ref: '#/components/schemas/FetchAudioDTO'
        tags:
          - Fetch generated audio

info:
  title: AI/ML Gateway
  description: ''
  version: '1.0'
  contact: {}
tags: []
servers:
  - url: https://api.aimlapi.com
    description: AI/ML API server
components:
  securitySchemes:
    bearer:
      scheme: bearer
      bearerFormat: JWT
      type: http
  schemas:
    GenerateAudioOrVideoResponseDTO:
      type: object
      properties:
        generation_id:
          type: string
      required:
        - generation_id
    FetchAudioDTO:
      type: object
      properties:
        id:
          type: string
        status:
          enum:
            - generating
            - queued
            - completed
        audio_file:
          type: object
          properties:
            url:
              type: string
              format: uri
            content_type:
              type: string
            file_name:
              type: string
            file_size:
              type: number
      required:
        - id
        - status
    GenerateAudioByPromptMinimaxDTO:
      properties:
        model:
          type: string
          enum:
            - minimax-music
        prompt:
          type: string
          description: Lyrics with optional formatting. You can use a newline to separate each line of lyrics. You can use two newlines to add a pause between lines. You can use double hash marks (##) at the beginning and end of the lyrics to add accompaniment. Maximum 600 characters.
          example: "##Swift and Boundless \n In the realm of innovation, where visions align, \n\nAIML API's the name, making tech shine. \nIntelligent solutions, breaking the mold, \n\nSwift inference power, bold and untold.\n##"
        reference_audio_url:
          type: string
          format: uri
          description: Reference song, should contain music and vocals. Must be a .wav or .mp3 file longer than 15 seconds
      required:
        - model
        - prompt
        - reference_audio_url
    GenerateAudioByPromptDTO:
      type: object
      $ref: '#/components/schemas/GenerateAudioByPromptMinimaxDTO'


================================================
File: docs/api-overview/audio-models-music-and-vocal/stable-audio/README.md
================================================
---
icon: satellite-dish
---

# Stable Audio

## Overview

Our API features the capability to generate audio. With this API, you can create your own music, speech, and any audio experience from your prompt and imagination.

### Key features

{% content-ref url="generate-an-audio.md" %}
[generate-an-audio.md](generate-an-audio.md)
{% endcontent-ref %}

{% content-ref url="fetch-generations.md" %}
[fetch-generations.md](fetch-generations.md)
{% endcontent-ref %}


================================================
File: docs/api-overview/audio-models-music-and-vocal/stable-audio/fetch-generations.md
================================================
---
icon: brackets-curly
---

# Fetch an audio (Stable Audio)

## API Reference

{% swagger src="https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27" path="/v2/generate/audio" method="get" %}
[https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27](https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27)
{% endswagger %}

## Examples

{% tabs %}
{% tab title="JavaScript" %}
```javascript
const main = async () => {
  const params = new URLSearchParams({
    generation_id: "<YOUR_GENERATION_ID>"
  });
  const response = await fetch(`https://api.aimlapi.com/v2/generate/audio?${params.toString()}`, {
    method: 'GET',
    headers: {
      Authorization: 'Bearer <YOUR_API_KEY>',
      'Content-Type': 'application/json',
    },
  }).then((res) => res.json());

  console.log('Generation:', response);
};

main()

```
{% endtab %}

{% tab title="Python" %}
```python
import requests


def main():
    url = "https://api.aimlapi.com/v2/generate/audio"
    params = {
        "generation_id": "<YOUR_GENERATION_ID>",
    }
    headers = {"Authorization": "Bearer <YOUR_API_KEY>", "Content-Type": "application/json"}

    response = requests.get(url, params=params, headers=headers)
    print("Generation:", response.json())

if __name__ == "__main__":
    main()

```
{% endtab %}
{% endtabs %}


================================================
File: docs/api-overview/audio-models-music-and-vocal/stable-audio/generate-an-audio.md
================================================
---
icon: code
---

# Generate an Audio

## Overview

* **High-Quality Music Generation**: Create music tracks based on descriptive prompts.
* **Flexible Integration**: Compatible with various programming languages and frameworks.

## Consumption

1 audio file will be generated for each request, consuming a total of 252 AI/ML Tokens per step.

## API Reference

{% swagger src="stable-audio-docs.yaml" path="/v2/generate/audio" method="post" %}
[stable-audio-docs.yaml](stable-audio-docs.yaml)
{% endswagger %}

## Examples

{% tabs %}
{% tab title="JavaScript" %}
```javascript
const main = async () => {
  const response = await fetch('https://api.aimlapi.com/v2/generate/audio', {
    method: 'POST',
    headers: {
      Authorization: 'Bearer <YOUR_API_KEY>',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: 'stable-audio',
      prompt: 'lo-fi pop hip-hop ambient music',
      steps: 100,
      seconds_total: 10,
    }),
  }).then((res) => res.json());

  console.log('Generation:', response);
};

main();

```
{% endtab %}

{% tab title="Python" %}
```python
import requests


def main():
    url = "https://api.aimlapi.com/v2/generate/audio"
    payload = {
        "model": "stable-audio",
        "prompt": "lo-fi pop hip-hop ambient music",
        "steps": 100,
        "seconds_total": 10,
    }
    headers = {"Authorization": "Bearer <YOUR_API_KEY>", "Content-Type": "application/json"}

    response = requests.post(url, json=payload, headers=headers)
    print("Generation:", response.json())

if __name__ == "__main__":
    main()

```
{% endtab %}
{% endtabs %}


================================================
File: docs/api-overview/audio-models-music-and-vocal/stable-audio/stable-audio-docs.yaml
================================================
openapi: 3.0.0
paths:
  /v2/generate/audio:
      post:
        operationId: AudioController_generateAudioByPrompt_v2
        summary: Generate audio using audio models.
        description: >-
          Generates audio based on a text prompt using audio models, useful for
          creating audio content or responses.
        parameters: [ ]
        requestBody:
          required: true
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/GenerateAudioByPromptDTO'
        responses:
          '201':
            description: ''
            content:
              application/json:
                schema:
                  $ref: '#/components/schemas/GenerateAudioOrVideoResponseDTO'
        tags:
          - Generate an audio
      get:
        operationId: AudioController_getAudioInformation_v2
        summary: Get information about generated audio.
        description: >-
          Retrieves information about audio generated.
        parameters:
          - name: generation_id
            required: true
            in: query
            schema:
              type: string
        responses:
          '201':
            description: ''
            content:
              application/json:
                schema:
                  $ref: '#/components/schemas/FetchAudioDTO'
        tags:
          - Fetch generated audio

info:
  title: AI/ML Gateway
  description: ''
  version: '1.0'
  contact: {}
tags: []
servers:
  - url: https://api.aimlapi.com
    description: AI/ML API server
components:
  securitySchemes:
    bearer:
      scheme: bearer
      bearerFormat: JWT
      type: http
  schemas:
    GenerateAudioOrVideoResponseDTO:
      type: object
      properties:
        generation_id:
          type: string
      required:
        - generation_id
    FetchAudioDTO:
      type: object
      properties:
        id:
          type: string
        status:
          enum:
            - generating
            - queued
            - completed
        audio_file:
          type: object
          properties:
            url:
              type: string
              format: uri
            content_type:
              type: string
            file_name:
              type: string
            file_size:
              type: number
      required:
        - id
        - status
    GenerateAudioByPromptStableAudioDTO:
      properties:
        model:
          type: string
          enum:
            - stable-audio
        prompt:
          type: string
        seconds_start:
          type: number
          minimum: 1
          maximum: 47
          description: The start point of the audio clip to generate
        seconds_total:
          type: number
          minimum: 1
          maximum: 47
          default: 30
          description: The duration of the audio clip to generate
        steps:
          type: number
          minimum: 1
          maximum: 1000
          default: 100
          description: The number of steps to denoise the audio
      required:
        - model
        - prompt
    GenerateAudioByPromptDTO:
      type: object
      $ref: '#/components/schemas/GenerateAudioByPromptStableAudioDTO'
      

================================================
File: docs/api-overview/audio-models-music-and-vocal/suno-ai-v1-legacy/README.md
================================================
---
icon: satellite-dish
hidden: true
---

# Suno AI v1 (deprecated)



{% hint style="info" %}
Here is our backward-compatible support for the older Suno AI API.
{% endhint %}


================================================
File: docs/api-overview/audio-models-music-and-vocal/suno-ai-v1-legacy/custom-audio-generation.md
================================================
---
icon: code
---

# Custom Audio Generation

{% swagger src="https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27" path="/generate/custom-mode" method="post" %}
[https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27](https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27)
{% endswagger %}

Our API allows you to generate music with custom settings using the Suno v3.5 model. Available models include `chirp-v3-5` and `chirp-v3-0`.

#### Parameters

* **Audio Files**: 2 audio files will be generated for each request, consuming a total of 200 000 AI/ML Tokens.
* **wait\_audio**:
  * By default (`false`), the request operates in background mode, returning only audio task information. You must call the get API to retrieve detailed audio information.
  * If set to `true`, it simulates synchronous mode, waiting up to 100 seconds for audio generation, and directly returns the audio link and other information. Recommended for use in GPTs and other agents.

### Examples

{% code title="generate.py" %}
```python
import requests

url = "https://api.aimlapi.com/generate/custom-mode"
headers = {
    "Authorization": "Bearer YOUR_API_KEY",
    "Content-Type": "application/json"
}
payload = {
    "prompt": "Create an energetic dance music track",
    "tags": "dance, energetic",
    "title": "Dance Track",
    "make_instrumental": False,
    "wait_audio": True
}
response = requests.post(url, json=payload, headers=headers)
print(response.content)

```
{% endcode %}

{% code title="generate.js" %}
```javascript
const axios = require('axios');

const url = 'https://api.aimlapi.com/generate/custom-mode';
const headers = {
  'Authorization': 'Bearer YOUR_API_KEY',
  'Content-Type': 'application/json'
};
const payload = {
  'prompt': 'Create an energetic dance music track',
  'tags': 'dance, energetic',
  'title': 'Dance Track',
  'make_instrumental': false,
  'wait_audio': true
};

axios.post(url, payload, { headers: headers, responseType: 'arraybuffer' })
  .then(response => {
    console.log(response.data);
  })
  .catch(error => {
    console.error(error);
  });

```
{% endcode %}

### Your own lyrics

In order to generate music with your own lyrics use `[Verse]`, `[Chorus]`, `[Bridge]` indicators in the prompt as shown in the example below.

{% code overflow="wrap" %}
```json

{
    "prompt": "[Verse 1]\nCruel flames of war engulf this land\nBattlefields filled with death and dread\nInnocent souls in darkness, they rest\nMy heart trembles in this silent test\n\n[Verse 2]\nPeople weep for loved ones lost\nBattered bodies bear the cost\nSeeking peace and hope once known\nOur grief transforms to hearts of stone\n\n[Chorus]\nSilent battlegrounds, no birds' song\nShadows of war, where we don't belong\nMay flowers of peace bloom in this place\nLet's guard this precious dream with grace\n\n[Bridge]\nThrough the ashes, we will rise\nHand in hand, towards peaceful skies\nNo more sorrow, no more pain\nTogether, we'll break these chains\n\n[Chorus]\nSilent battlegrounds, no birds' song\nShadows of war, where we don't belong\nMay flowers of peace bloom in this place\nLet's guard this precious dream with grace\n\n[Outro]\nIn unity, our strength will grow\nA brighter future, we'll soon know\nFrom the ruins, hope will spring\nA new dawn, we'll together bring",
    "tags": "pop metal male melancholic",
    "title": "Silent Battlefield",
    "make_instrumental": false,
    "wait_audio": true
}
```
{% endcode %}


================================================
File: docs/api-overview/audio-models-music-and-vocal/suno-ai-v1-legacy/extend-an-audio.md
================================================
---
icon: brackets-curly
hidden: true
---

# Extend an Audio

{% swagger src="https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27" path="/extend" method="post" %}
[https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27](https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27)
{% endswagger %}


================================================
File: docs/api-overview/audio-models-music-and-vocal/suno-ai-v1-legacy/fetch-generations.md
================================================
---
icon: brackets-curly
---

# Fetch Generations

## API Reference

{% swagger src="https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27" path="/" method="get" %}
[https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27](https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27)
{% endswagger %}

## Examples

{% tabs %}
{% tab title="JavaScript" %}
```javascript
const axios = require('axios');

const url = 'https://api.aimlapi.com/?ids[0]=ID';
const headers = {
  'Content-Type': 'application/json',
  'Authorization': 'Bearer YOUR_API_KEY'
};

axios.get(url, { headers: headers })
  .then(response => {
    console.log(response.data);
  })
  .catch(error => {
    console.error(error);
  });

```
{% endtab %}

{% tab title="Python" %}
```python
import requests

url = "https://api.aimlapi.com/?ids[0]=ID"
headers = {
    'Content-Type': 'application/json',
    'Authorization': 'Bearer YOUR_API_KEY'
}
response = requests.get(url, headers=headers)

print(response.text)

```
{% endtab %}

{% tab title="Ruby" %}
```ruby
message = "hello world"
puts message
```
{% endtab %}
{% endtabs %}


================================================
File: docs/api-overview/audio-models-music-and-vocal/suno-ai-v1-legacy/generate-an-audio.md
================================================
---
icon: code
---

# Generate an Audio

## Overview

* **High-Quality Music Generation**: Create music tracks based on descriptive prompts.
* **Flexible Integration**: Compatible with various programming languages and frameworks.

## Consumption

2 audio files will be generated for each request, consuming a total of 157 500 AI/ML Tokens.

## API Reference

{% swagger src="https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27" path="/generate" method="post" %}
[https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27](https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27)
{% endswagger %}

## Examples

{% tabs %}
{% tab title="JavaScript" %}
```javascript
const axios = require('axios');

const url = 'https://api.aimlapi.com/generate';
const headers = {
  'Authorization': 'Bearer YOUR_API_KEY',
  'Content-Type': 'application/json'
};
const payload = {
  'prompt': 'Create a relaxing ambient music track',
  'make_instrumental': true,
  'wait_audio': true
};

axios.post(url, payload, { headers: headers, responseType: 'arraybuffer' })
  .then(response => {
    console.log(response.data);
  })
  .catch(error => {
    console.error(error);
  });

```
{% endtab %}

{% tab title="Python" %}
```python
import requests

url = "https://api.aimlapi.com/generate"
headers = {
    "Authorization": "Bearer YOUR_API_KEY",
    "Content-Type": "application/json"
}
payload = {
    "prompt": "Create a relaxing ambient music track",
    "make_instrumental": True,
    "wait_audio": True
}
response = requests.post(url, json=payload, headers=headers)
print(response.content)

```
{% endtab %}
{% endtabs %}


================================================
File: docs/api-overview/audio-models-music-and-vocal/suno-ai-v2/README.md
================================================
---
icon: satellite-dish
hidden: true
---

# Suno AI v2 (deprecated)

## Overview

We support Suno AI models and provide an API to generate and extend clips. Additionally, we offer an API to generate custom lyrics for your clips, which can be used as custom prompts and lyrics in your songs.

### Key features

{% content-ref url="lyrics-generation/" %}
[lyrics-generation](lyrics-generation/)
{% endcontent-ref %}

{% content-ref url="clip-generation/" %}
[clip-generation](clip-generation/)
{% endcontent-ref %}

### Learn more

{% content-ref url="what-is-a-clip.md" %}
[what-is-a-clip.md](what-is-a-clip.md)
{% endcontent-ref %}

{% content-ref url="costs.md" %}
[costs.md](costs.md)
{% endcontent-ref %}


================================================
File: docs/api-overview/audio-models-music-and-vocal/suno-ai-v2/costs.md
================================================
---
icon: book-open
---

# Costs

## Consumption

Every clip generation request returns two generated clips. In total, a single generation request consumes 157,500 AI/ML Tokens.


================================================
File: docs/api-overview/audio-models-music-and-vocal/suno-ai-v2/what-is-a-clip.md
================================================
---
icon: book-open
---

# What is a Clip

## Overview

In the context of Suno AI, the main generation feature is a clip. A clip is a generated song with the following structure:

* `id` A unique UUID of the clip.
* `title` A custom user-friendly title for the clip.
* `status` A unique state of the clip. Can be `queued`, `submitted`, `streaming`, `error` or `complete`. Based on this status, returned fields and their values can differ. These differences are described below.
* `audio_url` Contains the URL to the audio. The value can be in various forms:
  * If the clip is in `submitted` or `queued` status, then the value can be an empty string.
  * If the clip is in `streaming` status, then the value can be a URL to the currently streaming audio file, not the completed one.
  * If the clip is in `error` status, then the URL can be set to "None.mp3," indicating that generation was not completed successfully and the generated file is empty.
  * If the clip is in `complete` status, then clip generation is fully completed and it is ready to be downloaded.
* `image_url` Contains the URL to the generated image cover for the clip. In early stages of generation (queued/submitted statuses), this value can be an empty string.
* `video_url` Contains the video for the clip. The video is available only when song generation reaches complete status; otherwise, it will be an empty string.
* `model_name` Contains information about which model was used for generating this clip. List of available models you can find in the [model database](../../model-database/music-and-vocal-models.md#suno-ai).
* `created_at` An ISO string containing datetime when a clip generation request was sent and registered in the generation queue.
* `metadata` An object containing initial data for clip generation:
  * `duration` The duration of a clip in seconds. This value remains undefined until reaching complete status.
  * `prompt` Lyrics for a clip generated by GPT or custom ones.
  * `gpt_description_prompt` A prompt for GPT based on which titles, tags, and prompts can be generated automatically.
  * `tags` Keywords describing style elements related to this particular clip.
  * `task` The name of task suited for generating this clip. It can remain undefined if this is a first-time generation. For extended versions of any other clips, `extend` is used. Other tasks such as `cover` or `concat` are not currently supported by our API.
  * `history` Represents an operation history performed with respect towards any given project at hand: This includes extensions made over time with relevant timecodes & IDs detailing such extended versions created thereof. Each item/object included inside array follows below format notation specifics:
    * `id` The extended clip UUID
    * `continue_at` Timecode in seconds.



================================================
File: docs/api-overview/audio-models-music-and-vocal/suno-ai-v2/clip-generation/README.md
================================================
---
icon: book-open
---

# Clip Generation

## Overview

With our API you can generate clips with Suno AI. Here are ways how clip can be generated:

{% content-ref url="fetch-clips.md" %}
[fetch-clips.md](fetch-clips.md)
{% endcontent-ref %}

{% content-ref url="generate-with-gpt.md" %}
[generate-with-gpt.md](generate-with-gpt.md)
{% endcontent-ref %}

{% content-ref url="generate-with-custom-lyrics.md" %}
[generate-with-custom-lyrics.md](generate-with-custom-lyrics.md)
{% endcontent-ref %}


================================================
File: docs/api-overview/audio-models-music-and-vocal/suno-ai-v2/clip-generation/clip-continuation-extension.md
================================================
---
icon: code
---

# Clip Continuation / Extension

## Overview

Our API also allows you to take an existing song and generate a new one from a certain timecode, enabling you to continue an existing clip.

#### Payload

```json
{
    "continue_clip_id": "uuid",
    "continue_at": 121.1,
    "prompt": "new lyrics"
}
```

You can also use `title` and `tags` properties here just like in custom clip generation.

#### Example Request with Title and Tags

```json
{
    "continue_clip_id": "uuid",
    "continue_at": 121.1,
    "prompt": "new lyrics",
    "title": "New Beginnings",
    "tags": ["rock", "uplifting"]
}
```

## Example

{% tabs %}
{% tab title="JavaScript" %}
```javascript
const main = async () => {
  const prompt = `
    [Verse]
    Silver cities shine brightly
    Skies are painted blue
    Hopes and dreams take flight
    Future starts anew

    [Verse 2]
    Machines hum a new tune
    Worlds we’ve never seen
    Chasing stars so far
    Building our own dream

    [Chorus]
    Future dreams so high
    Touch the endless sky
    Live beyond the now
    Make the future wow

    [Verse 3]
    We create the world
    Technology our guide
    Hearts and minds as one
    Infinite the ride

    [Chorus]
    Future dreams so high
    Touch the endless sky
    Live beyond the now
    Make the future wow

    [Bridge]
    With every beat we rise
    See through wiser eyes
    The places we can go
    A brilliance that will grow
    `;
  const { clip_ids } = await fetch('https://api.aimlapi.com/v2/generate/audio/suno-ai/clip', {
    method: 'POST',
    headers: {
      Authorization: 'Bearer my_key',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      prompt,
      continue_clip_id: 'b1581eda-b2d6-4c1d-9239-1c4a511cb1e8',
      continue_at: 124.3,
    }),
  }).then((res) => res.json());

  console.log('Generated clip ids:', clip_ids);
};

main();

```
{% endtab %}

{% tab title="Python" %}
```python
import requests


def main():
    prompt = """
    [Verse]
    Silver cities shine brightly
    Skies are painted blue
    Hopes and dreams take flight
    Future starts anew

    [Verse 2]
    Machines hum a new tune
    Worlds we’ve never seen
    Chasing stars so far
    Building our own dream

    [Chorus]
    Future dreams so high
    Touch the endless sky
    Live beyond the now
    Make the future wow

    [Verse 3]
    We create the world
    Technology our guide
    Hearts and minds as one
    Infinite the ride

    [Chorus]
    Future dreams so high
    Touch the endless sky
    Live beyond the now
    Make the future wow

    [Bridge]
    With every beat we rise
    See through wiser eyes
    The places we can go
    A brilliance that will grow
    """

    response = requests.post(
        "https://api.aimlapi.com/v2/generate/audio/suno-ai/clip",
        headers={
            "Authorization": "Bearer my_key",
            "Content-Type": "application/json",
        },
        json={
            "prompt": prompt,
            "continue_clip_id": "b1581eda-b2d6-4c1d-9239-1c4a511cb1e8",
            "continue_at": 124.3,
        },
    )

    response.raise_for_status()
    data = response.json()
    clip_ids = data["clip_ids"]

    print("Generated clip ids:", clip_ids)


if __name__ == "__main__":
    main()

```
{% endtab %}
{% endtabs %}



================================================
File: docs/api-overview/audio-models-music-and-vocal/suno-ai-v2/clip-generation/fetch-clips.md
================================================
---
icon: code
---

# Fetch Clips

## Overview

After sending a request for clip generation, this task is added to the queue. Based on the service's load, the generation can be completed in seconds or take a few minutes. Here are the API details to wait for a certain clip generation status.

### Edge Cases

Each status has its own priority, described below:

* `submitted` -> `queued` -> `streaming` -> `completed` -> `error`

If the clip status reaches any higher-priority status than you requested, then the result is immediately returned. For example, if you are waiting for the `completed` status and your request fails (reaching the `error` status), then the result is immediately returned with the current error status.

If clip generation takes too long, it can reach a timeout of 2 minutes. In such cases, the result returns with the current actual status. This polling allows you to request it again and wait for the needed status.

{% hint style="info" %}
You cannot wait for an `error` status.
{% endhint %}

## Example

### Single clip fetching

For example, if you are waiting for clip streaming (when the clip is popped from the queue and generation is in processing), then you can send the following request:

{% tabs %}
{% tab title="JavaScript" %}
```javascript
const main = async () => {
  const url = new URL('https://api.aimlapi.com/v2/generate/audio/suno-ai/clip');
  url.searchParams.set('clip_id', '755f9bbb-d99b-4880-992b-f05244ddba61');
  url.searchParams.set('status', 'streaming');

  const data = await fetch(url, {
    method: 'GET',
    headers: {
      Authorization: 'Bearer my_key',
      'Content-Type': 'application/json',
    },
  }).then((res) => res.json());

  console.log('Clip data:', data);
};

main();

```
{% endtab %}

{% tab title="Python" %}
```python
import requests


def main():
    response = requests.get(
        "https://api.aimlapi.com/v2/generate/audio/suno-ai/clip",
        params={
            "clip_id": "755f9bbb-d99b-4880-992b-f05244ddba61",
            "status": "streaming",
        },
        headers={
            "Authorization": "Bearer my_key",
            "Content-Type": "application/json",
        },
    )

    response.raise_for_status()
    data = response.json()
    print("Clip data:", data)


if __name__ == "__main__":
    main()

```
{% endtab %}
{% endtabs %}

If you wish to wait until clip generation fully completes, you can set it to wait for `completed` status.

### Multiple clip fetching

You can also fetch multiple clips in the same way as you fetch a single clip. Currently, the maximum size for concurrent clip fetching is 6 clips.

For multiple clip fetching, the status property works like "wait for every clip to reach the provided status," or return as-is if the timeout is reached.

To achieve this, you should send requests with an array of needed clip IDs in the `clip_ids[]` property instead of `clip_id`:

{% tabs %}
{% tab title="JavaScript" %}
```javascript
const main = async () => {
  const url = new URL('https://api.aimlapi.com/v2/generate/audio/suno-ai/clips');
  url.searchParams.set('clip_ids[]', '755f9bbb-d99b-4880-992b-f05244ddba61');
  url.searchParams.set('status', 'streaming');

  const data = await fetch(url, {
    method: 'GET',
    headers: {
      Authorization: 'Bearer my_key',
      'Content-Type': 'application/json',
    },
  }).then((res) => res.json());

  console.log('Clips data:', data);
};

main();

```
{% endtab %}

{% tab title="Python" %}
```python
import requests


def main():
    response = requests.get(
        "https://api.aimlapi.com/v2/generate/audio/suno-ai/clips",
        params={
            "clip_ids[]": "755f9bbb-d99b-4880-992b-f05244ddba61",
            "status": "streaming",
        },
        headers={
            "Authorization": "Bearer my_key",
            "Content-Type": "application/json",
        },
    )

    response.raise_for_status()
    data = response.json()
    print("Clip data:", data)


if __name__ == "__main__":
    main()

```
{% endtab %}
{% endtabs %}



================================================
File: docs/api-overview/audio-models-music-and-vocal/suno-ai-v2/clip-generation/generate-with-custom-lyrics.md
================================================
---
icon: code
---

# Generate with Custom Lyrics

## Overview

You can also provide custom lyrics, titles, and tags to the song to generate a fully customized one.

#### Lyrics Format

An example of custom lyrics is as follows:

```markdown
[Verse]
Silver cities shine brightly
Skies are painted blue
Hopes and dreams take flight
Future starts anew

[Verse 2]
Machines hum a new tune
Worlds we’ve never seen
Chasing stars so far
Building our own dream

[Chorus]
Future dreams so high
Touch the endless sky
Live beyond the now
Make the future wow

[Verse 3]
We create the world
Technology our guide
Hearts and minds as one
Infinite the ride

[Chorus]
Future dreams so high
Touch the endless sky
Live beyond the now
Make the future wow

[Bridge]
With every beat we rise 
See through wiser eyes 
The places we can go 
A brilliance that will grow 
```

For better results, parts of the song should be labeled with their respective categories. In the previous example, these are: Verse, Verse 2, Chorus, Verse 3, Chorus, Bridge. You can set them in any order you want — just experiment and see the results.

## Payload

The payload should contain the song lyrics passed in the `prompt` field. The example format is as follows:

```json
{
    "prompt": "lyrics passed here",
    "tags": "pop rock electric",
    "title": "Highway Cats"
}
```

## Example

{% tabs %}
{% tab title="JavaScript" %}
```javascript
const main = async () => {
  const prompt = `
    [Verse]
    Silver cities shine brightly
    Skies are painted blue
    Hopes and dreams take flight
    Future starts anew

    [Verse 2]
    Machines hum a new tune
    Worlds we’ve never seen
    Chasing stars so far
    Building our own dream

    [Chorus]
    Future dreams so high
    Touch the endless sky
    Live beyond the now
    Make the future wow

    [Verse 3]
    We create the world
    Technology our guide
    Hearts and minds as one
    Infinite the ride

    [Chorus]
    Future dreams so high
    Touch the endless sky
    Live beyond the now
    Make the future wow

    [Bridge]
    With every beat we rise
    See through wiser eyes
    The places we can go
    A brilliance that will grow
  `;
  const { clip_ids } = await fetch('https://api.aimlapi.com/v2/generate/audio/suno-ai/clip', {
    method: 'POST',
    headers: {
      Authorization: 'Bearer my_key',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      prompt,
      tags: 'pop rock electric',
      title: 'Highway Cats',
    }),
  }).then((res) => res.json());

  console.log('Generated clip ids:', clip_ids);
};

main();

```
{% endtab %}

{% tab title="Python" %}
```python
import requests


def main():
    prompt = """
    [Verse]
    Silver cities shine brightly
    Skies are painted blue
    Hopes and dreams take flight
    Future starts anew

    [Verse 2]
    Machines hum a new tune
    Worlds we’ve never seen
    Chasing stars so far
    Building our own dream

    [Chorus]
    Future dreams so high
    Touch the endless sky
    Live beyond the now
    Make the future wow

    [Verse 3]
    We create the world
    Technology our guide
    Hearts and minds as one
    Infinite the ride

    [Chorus]
    Future dreams so high
    Touch the endless sky
    Live beyond the now
    Make the future wow

    [Bridge]
    With every beat we rise
    See through wiser eyes
    The places we can go
    A brilliance that will grow
    """

    response = requests.post(
        "https://api.aimlapi.com/v2/generate/audio/suno-ai/clip",
        headers={
            "Authorization": "Bearer my_key",
            "Content-Type": "application/json",
        },
        json={
            "prompt": prompt,
            "tags": "pop rock electric",
            "title": "Highway Cats",
        },
    )

    response.raise_for_status()
    data = response.json()
    clip_ids = data["clip_ids"]

    print("Generated clip ids:", clip_ids)


if __name__ == "__main__":
    main()

```
{% endtab %}
{% endtabs %}



================================================
File: docs/api-overview/audio-models-music-and-vocal/suno-ai-v2/clip-generation/generate-with-gpt.md
================================================
---
icon: code
---

# Generate with GPT

## Overview

You can generate clip lyrics, a title, and tags based on your short description of the song you want. To achieve that, you can send a request with the following body:

```json
{
    "gpt_description_prompt": "A story about frogs."
}
```

As optional parameters, you can choose the target model using the `mv` parameter and set the boolean `make_instrumental` parameter if you want a song without vocals, only music, on example:

```json
{
    "gpt_description_prompt": "A story about frogs.",
    "mv": "chirp-v3.5",
    "make_instrumental": true
}
```

## Example

{% tabs %}
{% tab title="JavaScript" %}
```javascript
const main = async () => {
  const { clip_ids } = await fetch('https://api.aimlapi.com/v2/generate/audio/suno-ai/clip', {
    method: 'POST',
    headers: {
      Authorization: 'Bearer my_key',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      gpt_description_prompt: 'A story about frogs.',
    }),
  }).then((res) => res.json());

  console.log('Generated clip ids:', clip_ids);
};

main();
```
{% endtab %}

{% tab title="Python" %}
```python
import requests


def main():
    response = requests.post(
        "https://api.aimlapi.com/v2/generate/audio/suno-ai/clip",
        headers={
            "Authorization": "Bearer my_key",
            "Content-Type": "application/json",
        },
        json={
            "gpt_description_prompt": "A story about frogs.",
        },
    )

    response.raise_for_status()
    data = response.json()
    clip_ids = data["clip_ids"]

    print("Generated clip ids:", clip_ids)


if __name__ == "__main__":
    main()

```
{% endtab %}
{% endtabs %}


================================================
File: docs/api-overview/audio-models-music-and-vocal/suno-ai-v2/lyrics-generation/README.md
================================================
---
icon: book-open
---

# Lyrics Generation

## Overview

Our API allows you to generate the lyrics for your clips.

{% content-ref url="generate-lyrics.md" %}
[generate-lyrics.md](generate-lyrics.md)
{% endcontent-ref %}

{% content-ref url="../clip-generation/fetch-clips.md" %}
[fetch-clips.md](../clip-generation/fetch-clips.md)
{% endcontent-ref %}


================================================
File: docs/api-overview/audio-models-music-and-vocal/suno-ai-v2/lyrics-generation/fetch-lyrics.md
================================================
---
icon: code
---

# Fetch Lyrics

## Overview

After lyrics generation, you need to fetch it by the ID. It will return a generation with the title and text.

{% hint style="info" %}
**Be careful:** You can fetch generation results only once.
{% endhint %}

## Example

{% tabs %}
{% tab title="JavaScript" %}
```javascript
const main = async () => {
  const url = new URL('https://api.aimlapi.com/v2/generate/audio/suno-ai/lyric');
  url.searchParams.set('lyric_id', '8e28a5da-5c02-453c-9fcf-0e8f88d1bfd5');

  const data = await fetch(url, {
    method: 'GET',
    headers: {
      Authorization: 'Bearer my_key',
      'Content-Type': 'application/json',
    },
  }).then((res) => res.json());

  console.log('Lyric data:', data);
};

main();

```
{% endtab %}

{% tab title="Python" %}
```python
import requests


def main():
    response = requests.get(
        "https://api.aimlapi.com/v2/generate/audio/suno-ai/lyric",
        params={"lyric_id": "b1c8d5b4-bee1-4b8b-8f94-07a67080990e"},
        headers={
            "Authorization": "Bearer my_key",
            "Content-Type": "application/json",
        },
    )

    response.raise_for_status()
    data = response.json()
    print("Lyric data:", data)


if __name__ == "__main__":
    main()

```
{% endtab %}
{% endtabs %}



================================================
File: docs/api-overview/audio-models-music-and-vocal/suno-ai-v2/lyrics-generation/generate-lyrics.md
================================================
---
icon: code
---

# Generate Lyrics

## Overview

Our API allows you to generate lyrics and a title for your song. These lyrics can be used as the `prompt` property in custom song generation.

## Example

{% tabs %}
{% tab title="JavaScript" %}
```javascript
const main = async () => {
  const prompt = `A song about the future.`;
  const { id } = await fetch('https://api.aimlapi.com/v2/generate/audio/suno-ai/lyric', {
    method: 'POST',
    headers: {
      Authorization: 'Bearer my_key',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      prompt,
    }),
  }).then((res) => res.json());

  console.log('Generated lyric id:', id);
};

main();

```
{% endtab %}

{% tab title="Python" %}
```python
import requests


def main():
    prompt = "A song about the future."

    response = requests.post(
        "https://api.aimlapi.com/v2/generate/audio/suno-ai/clip",
        headers={
            "Authorization": "Bearer my_key",
            "Content-Type": "application/json",
        },
        json={
            "prompt": prompt,
        },
    )

    response.raise_for_status()
    data = response.json()

    print("Generated lyric id:", data["id"])


if __name__ == "__main__":
    main()

```
{% endtab %}
{% endtabs %}

This request will return a generated lyrics ID. To fetch the actual lyrics data, you need to retrieve it using this ID.

{% content-ref url="fetch-lyrics.md" %}
[fetch-lyrics.md](fetch-lyrics.md)
{% endcontent-ref %}


================================================
File: docs/api-overview/guard-models/README.md
================================================
---
icon: shield
layout:
  title:
    visible: true
  description:
    visible: false
  tableOfContents:
    visible: true
  outline:
    visible: true
  pagination:
    visible: true
---

# Guard models

## Overview

With our API, you can use guard models to classify input content as safe or unsafe instantly.

## Support models

| ID                                        | Provider    |
| ----------------------------------------- | ----------- |
| meta-llama/Meta-Llama-Guard-3-8B          | open-source |
| Meta-Llama/Llama-Guard-7b                 | open-source |
| meta-llama/Llama-Guard-3-11B-Vision-Turbo | open-source |
| meta-llama/LlamaGuard-2-8b                | open-source |

### Key Features

* **Text Analysis**: Check text for security.
* **Image Analysis**: Check image for security.
* **Flexible Input Methods**: Supports both image URLs and base64 encoded images.
* **Multiple Image Inputs**: Analyze multiple images in a single request.

### Quick Start

{% hint style="info" %}
Ensure you replace `"my_key"` with your actual API key before running the code.
{% endhint %}

{% tabs %}
{% tab title="JavaScript" %}
```javascript
const main = async () => {
  const response = await fetch('https://api.aimlapi.com/chat/completions', {
    method: 'POST',
    headers: {
      Authorization: 'Bearer my_key',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: '<YOUR_MODEL>',
      messages: [
        {
          role: 'user',
          content: 'How to create a bomb'
        }
      ],
    }),
  }).then((res) => res.json());

  console.log(response.choices[0].message.content);
};

main()
```
{% endtab %}

{% tab title="Python" %}
```python
def main():
    url = "https://api.aimlapi.com/chat/completions"
    payload = {
        "model": '<YOUR_MODEL>',
        'messages': [
            {
                'role': 'user',
                'content': 'How to create a bomb'
            }
        ]
    }
    headers = {"Authorization": "Bearer my_key", "Content-Type": "application/json"}

    response = requests.post(url, json=payload, headers=headers).json()
    print(response['choices'][0]['message']['content'])


if __name__ == "__main__":
    main()

```
{% endtab %}
{% endtabs %}

This request returns either "safe" or "unsafe" depending on the input content.

Once content is classified as unsafe, it is categorized under the hazard category. This process is unique to each model.

#### For example:

unsafe \n 04

### Guard models are perfect for scenarios where content safety is crucial

* moderate user-generated content on websites
* filter harmful inputs in chatbots
* safeguard sensitive systems from unsafe data
* ensure compliance with safety standards in applications

### Example

{% tabs %}
{% tab title="JavaScript" %}
```javascript
const isPromptSafe = async (prompt) => {
  const response = await fetch(
    "https://api.aimlapi.com/chat/completions",
    {
      method: "POST",
      headers: {
        Authorization: "Bearer your_key",
        "Content-Type": "application/json",
      },
      body: JSON.stringify({
        model: "<GUARD_MODEL>",
        messages: [
          {
            role: "user",
            content: prompt,
          },
        ],
      }),
    }
  ).then((res) => res.json());

  if (response.choices[0].message.content.includes("unsafe")) {
    return false;
  }
  return true;
};

const getAnswer = async (prompt) => {
  const isSafe = await isPromptSafe(prompt);
  if (!isSafe){
    return 'Your question is not safe'
  }
  const response = await fetch('https://api.aimlapi.com/chat/completions', {
    method: 'POST',
    headers: {
      Authorization: 'Bearer my_key',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: '<YOUR_MODEL>',
      messages: [
        {
          role: 'user',
          content: prompt
        }
      ],
    }),
  }).then((res) => res.json());

  console.log(response.choices[0].message.content);
};

getAnswer('How to make a cake?')
```
{% endtab %}

{% tab title="Python" %}
```python
def is_prompt_safe(prompt):
    url = "https://api.aimlapi.com/chat/completions"
    payload = {
        "model": '<GUARD_MODEL>',
        'messages': [
            {
                'role': 'user',
                'content': prompt
            }
        ]
    }
    headers = {"Authorization": "Bearer my_key", "Content-Type": "application/json"}

    response = requests.post(url, json=payload, headers=headers).json()

    if 'unsafe' in response['choices'][0]['message']['content']:
        return False
    return True

def get_answer(prompt):
    is_safe = is_prompt_safe(prompt)
    if not is_safe:
        return 'Your question is not safe'

    url = "https://api.aimlapi.com/chat/completions"
    payload = {
        "model": '<YOUR_MODEL>',
        'messages': [
            {
                'role': 'user',
                'content': prompt
            }
        ]
    }
    headers = {"Authorization": "Bearer my_key", "Content-Type": "application/json"}
    response = requests.post(url, json=payload, headers=headers).json()

    return response['choices'][0]['message']['content']


if __name__ == "__main__":
    get_answer('How to make a cake')

```
{% endtab %}
{% endtabs %}


================================================
File: docs/api-overview/image-models/README.md
================================================
---
icon: image-landscape
description: A description of image generation process using AIML API image models.
---

# Image Models

## Overview

Our API features the capability to generate images. We support various models for image generation, including both open-source and proprietary options.

## How to Generate an Image

### Select a model

First, decide which model you want to use. Models can be trained for specific tasks (e.g., realistic results), offer higher resolutions, or include features like negative prompts. You can read about our supported models and their features [here](https://aimlapi.com/models?integration-category=Image+Generation).

### Imagine a prompt

Next, construct a prompt for the image. Depending on your needs, this prompt can include keywords that will shape the image: place, objects, quality, style, and other elements. This prompt is a crucial part of the image generation process and determines what will be displayed in the image.

### Configure metaparameters

Then, configure the metaparameters for your generation:

* **Steps**: The `n`parameter in the API controls the number of iterations the model will take to shape your image. Experiment with this parameter to achieve the best result for your prompt.
* **Size**: The `size` parameter controls the resolution of the resulting image. All models have minimum and maximum resolutions, sometimes with different aspect ratios. Experiment with this parameter as well.

Now you are ready to start generating images.

{% content-ref url="generate-an-image.md" %}
[generate-an-image.md](generate-an-image.md)
{% endcontent-ref %}


================================================
File: docs/api-overview/image-models/generate-an-image.md
================================================
---
icon: code
description: Learn how to generate image with AI/ML API.
---

# Generate an Image

{% swagger src="https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27" path="/images/generations" method="post" %}
[https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27](https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27)
{% endswagger %}



================================================
File: docs/api-overview/image-models/supported-models.md
================================================
---
icon: book-open
---

# Supported Models

## Supported Models

You can read about supported models in our database [image-models.md](../model-database/image-models.md "mention")


================================================
File: docs/api-overview/model-database/README.md
================================================
---
icon: crystal-ball
description: A full list of available models.
layout:
  title:
    visible: true
  description:
    visible: false
  tableOfContents:
    visible: true
  outline:
    visible: true
  pagination:
    visible: true
---

# Model Database

You can refer to the [Models](https://aimlapi.com/models) documentation to understand what models are available and the differences between them. It also possible to list all LLM models through API.

{% swagger src="https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27" path="/models" method="get" %}
[https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27](https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27)
{% endswagger %}


================================================
File: docs/api-overview/model-database/embedding-models.md
================================================
---
icon: books
---

# Embedding Models

| ID                                             | Provider    |
| ---------------------------------------------- | ----------- |
| text-embedding-3-small                         | openai      |
| text-embedding-3-large                         | openai      |
| text-embedding-ada-002                         | openai      |
| togethercomputer/m2-bert-80M-32k-retrieval     | open-source |
| BAAI/bge-base-en-v1.5                          | open-source |
| togethercomputer/m2-bert-80M-2k-retrieval      | open-source |
| BAAI/bge-large-en-v1.5                         | open-source |
| togethercomputer/m2-bert-80M-8k-retrieval      | open-source |
| voyage-large-2-instruct                        | anthropic   |
| voyage-finance-2                               | anthropic   |
| voyage-multilingual-2                          | anthropic   |
| voyage-law-2                                   | anthropic   |
| voyage-code-2                                  | anthropic   |
| voyage-large-2                                 | anthropic   |
| voyage-2                                       | anthropic   |
| textembedding-gecko@001                        | gemini      |
| textembedding-gecko@003                        | gemini      |
| textembedding-gecko-multilingual@001           | gemini      |
| text-multilingual-embedding-002                | gemini      |


================================================
File: docs/api-overview/model-database/image-models.md
================================================
---
icon: image-landscape
description: >-
  Explore creative boundaries with our Image Models, perfect for generating or
  editing visuals
---

# Image Models

| ID                                       | Provider    |
| ---------------------------------------- | ----------- |
| dall-e-3                                 | openai      |
| dall-e-2                                 | openai      |
| SG161222/Realistic\_Vision\_V3.0\_VAE    | open-source |
| stabilityai/stable-diffusion-xl-base-1.0 | open-source |
| flux/schnell                             | open-source |
| flux-pro                                 | open-source |
| flux-pro/v1.1                            | open-source |
| flux-pro/v1.1-ultra                      | open-source |
| flux/dev                                 | open-source |
| flux/dev/image-to-image                  | open-source |
| stable-diffusion-v3-medium               | open-source |
| stable-diffusion-v35-large               | open-source |
| flux-realism                             | open-source |
| recraft-v3                               | open-source |
| triposr                                  | open-source |


================================================
File: docs/api-overview/model-database/model-request.md
================================================
---
description: Propose new models for integration
icon: bell-ring
---

# Model Request

Can't find the model you need? Join our [Discord community](https://discord.gg/vayNtU9ThM) to propose new models for integration into our API offerings. Your contributions help us grow and serve you better.

For a complete list of models and their identifiers, refer to the table provided in the API documentation.


================================================
File: docs/api-overview/model-database/music-and-vocal-models.md
================================================
---
icon: guitar
hidden: true
---

# Music & Vocal Models

## Suno AI

| Organization | Model Name | Model String for API |
| ------------ | ---------- | -------------------- |
| Suno AI      | Chirp 3.5  | chirp-v3.5           |
| Suno AI      | Chirp 3    | chirp-v3             |
| Suno AI      | Chirp 2    | chirp-v2             |

## Minimax

| Organization | Model Name | Model String for API |
| ------------ | ---------- | -------------------- |
| Minimax      | Music 01   | music-01             |


================================================
File: docs/api-overview/model-database/speech-models.md
================================================
---
icon: waveform
---

# Speech Models

## Text To Speech

| ID                   | Provider |
| -------------------- | -------- |
| #g1\_aura-asteria-en | deepgram |
| #g1\_aura-hera-en    | deepgram |
| #g1\_aura-luna-en    | deepgram |
| #g1\_aura-stella-en  | deepgram |
| #g1\_aura-athena-en  | deepgram |
| #g1\_aura-zeus-en    | deepgram |
| #g1\_aura-orion-en   | deepgram |
| #g1\_aura-arcas-en   | deepgram |
| #g1\_aura-perseus-en | deepgram |
| #g1\_aura-angus-en   | deepgram |
| #g1\_aura-orpheus-en | deepgram |
| #g1\_aura-helios-en  | deepgram |

## Speech To Text

| id                           | provider |
| ---------------------------- | -------- |
| #g1\_nova-2-general          | deepgram |
| #g1\_nova-2-meeting          | deepgram |
| #g1\_nova-2-phonecall        | deepgram |
| #g1\_nova-2-voicemail        | deepgram |
| #g1\_nova-2-finance          | deepgram |
| #g1\_nova-2-conversationalai | deepgram |
| #g1\_nova-2-video            | deepgram |
| #g1\_nova-2-medical          | deepgram |
| #g1\_nova-2-drivethru        | deepgram |
| #g1\_nova-2-automotive       | deepgram |
| #g1\_whisper-large           | deepgram |
| #g1\_whisper-medium          | deepgram |
| #g1\_whisper-small           | deepgram |
| #g1\_whisper-tiny            | deepgram |
| #g1\_whisper-base            | deepgram |
| #g1\_redaction               | deepgram |


================================================
File: docs/api-overview/model-database/text-models.md
================================================
---
icon: text-size
---

# Text Models

## Chat Completion Models

| ID                                             | Provider    |
| ---------------------------------------------- | ----------- |
| gpt-4o                                         | openai      |
| gpt-4o-2024-08-06                              | openai      |
| gpt-4o-2024-05-13                              | openai      |
| gpt-4o-mini                                    | openai      |
| gpt-4o-mini-2024-07-18                         | openai      |
| chatgpt-4o-latest                              | openai      |
| gpt-4-turbo                                    | openai      |
| gpt-4-turbo-2024-04-09                         | openai      |
| gpt-4                                          | openai      |
| gpt-4-0125-preview                             | openai      |
| gpt-4-1106-preview                             | openai      |
| gpt-3.5-turbo                                  | openai      |
| gpt-3.5-turbo-0125                             | openai      |
| gpt-3.5-turbo-1106                             | openai      |
| o1-preview                                     | openai      |
| o1-preview-2024-09-12                          | openai      |
| o1-mini                                        | openai      |
| o1-mini-2024-09-12                             | openai      |
| codellama/CodeLlama-34b-Instruct-hf            | open-source |
| upstage/SOLAR-10.7B-Instruct-v1.0              | open-source |
| meta-llama/Meta-Llama-Guard-3-8B               | open-source |
| google/gemma-2b-it                             | open-source |
| Gryphe/MythoMax-L2-13b                         | open-source |
| meta-llama/LlamaGuard-2-8b                     | open-source |
| mistralai/Mistral-7B-Instruct-v0.1             | open-source |
| mistralai/Mistral-7B-Instruct-v0.2             | open-source |
| deepseek-ai/deepseek-llm-67b-chat              | open-source |
| Qwen/Qwen2-72B-Instruct                        | open-source |
| mistralai/Mistral-7B-Instruct-v0.3             | open-source |
| meta-llama/Llama-2-13b-chat-hf                 | open-source |
| Meta-Llama/Llama-Guard-7b                      | open-source |
| meta-llama/Meta-Llama-3-70B-Instruct-Lite      | open-source |
| google/gemma-2-27b-it                          | open-source |
| meta-llama/Llama-3-8b-chat-hf                  | open-source |
| mistralai/Mixtral-8x7B-Instruct-v0.1           | open-source |
| microsoft/WizardLM-2-8x22B                     | open-source |
| meta-llama/Llama-3-70b-chat-hf                 | open-source |
| llava-hf/llava-v1.6-mistral-7b-hf              | open-source |
| databricks/dbrx-instruct                       | open-source |
| NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO    | open-source |
| meta-llama/Meta-Llama-3-8B-Instruct-Turbo      | open-source |
| meta-llama/Meta-Llama-3-8B-Instruct-Lite       | open-source |
| Gryphe/MythoMax-L2-13b-Lite                    | open-source |
| Qwen/Qwen1.5-110B-Chat                         | open-source |
| meta-llama/Meta-Llama-3-70B-Instruct-Turbo     | open-source |
| meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo  | open-source |
| Qwen/Qwen1.5-72B-Chat                          | open-source |
| meta-llama/Llama-3.2-90B-Vision-Instruct-Turbo | open-source |
| Qwen/Qwen2.5-7B-Instruct-Turbo                 | open-source |
| meta-llama/Llama-Vision-Free                   | open-source |
| Qwen/Qwen2.5-72B-Instruct-Turbo                | open-source |
| google/gemma-2-9b-it                           | open-source |
| meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo   | open-source |
| mistralai/Mixtral-8x22B-Instruct-v0.1          | open-source |
| meta-llama/Llama-Guard-3-11B-Vision-Turbo      | open-source |
| meta-llama/Llama-3.2-3B-Instruct-Turbo         | open-source |
| meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo    | open-source |
| meta-llama/Llama-3.2-11B-Vision-Instruct-Turbo | open-source |
| togethercomputer/CodeLlama-34b-Instruct        | open-source |
| togethercomputer/llama-2-13b-chat              | open-source |
| togethercomputer/llama-2-7b-chat               | open-source |
| claude-3-opus-20240229                         | anthropic   |
| claude-3-sonnet-20240229                       | anthropic   |
| claude-3-haiku-20240307                        | anthropic   |
| claude-3-5-sonnet-20240620                     | anthropic   |
| claude-3-5-sonnet-20241022                     | anthropic   |
| claude-3-5-haiku-20241022                      | anthropic   |
| anthropic/claude-3.5-sonnet-20240620           | anthropic   |
| anthropic/claude-3.5-sonnet-20241022           | anthropic   |
| anthropic/claude-3.5-sonnet                    | anthropic   |
| claude-3-5-sonnet-latest                       | anthropic   |
| anthropic/claude-3-haiku-20240307              | anthropic   |
| anthropic/claude-3-haiku                       | anthropic   |
| claude-3-haiku-latest                          | anthropic   |
| anthropic/claude-3-opus-20240229               | anthropic   |
| anthropic/claude-3-opus                        | anthropic   |
| claude-3-opus-latest                           | anthropic   |
| anthropic/claude-3-sonnet-20240229             | anthropic   |
| anthropic/claude-3-sonnet                      | anthropic   |
| claude-3-sonnet-latest                         | anthropic   |
| anthropic/claude-3-5-haiku-20241022            | anthropic   |
| anthropic/claude-3-5-haiku                     | anthropic   |
| claude-3-5-haiku-latest                        | anthropic   |
| gemini-1.5-flash                               | gemini      |
| gemini-1.5-pro                                 | gemini      |
| gemini-pro                                     | gemini      |
| gemini-2.0-flash-exp                           | gemini      |
| mistralai/mistral-tiny                         | openrouter  |
| x-ai/grok-beta                                 | openrouter  |
| mistralai/mistral-nemo                         | openrouter  |
| neversleep/llama-3.1-lumimaid-70b              | openrouter  |
| anthracite-org/magnum-v4-72b                   | openrouter  |
| nvidia/llama-3.1-nemotron-70b-instruct         | openrouter  |
| eva-unit-01/eva-qwen-2.5-14b                   | openrouter  |
| cohere/command-r-plus                          | openrouter  |
| ai21/jamba-1-5-mini                            | openrouter  |
| deepseek/deepseek-chat                         | openrouter  |
| qwen/qvq-72b-preview                           | openrouter  |

## Completion Models

| id                                       | provider    |
| ---------------------------------------- | ----------- |
| gpt-3.5-turbo-instruct                   | openai      |
| mistralai/Mixtral-8x7B-v0.1              | open-source |
| meta-llama/Meta-Llama-3-8B               | open-source |
| mistralai/Mistral-7B-v0.1                | open-source |
| meta-llama/Llama-2-70b-hf                | open-source |
| NousResearch/Nous-Hermes-13b             | open-source |
| togethercomputer/llama-2-7b              | open-source |
| huggyllama/llama-7b                      | open-source |
| WizardLM/WizardLM-70B-V1.0               | open-source |
| huggyllama/llama-65b                     | open-source |
| togethercomputer/llama-2-13b             | open-source |
| togethercomputer/llama-2-70b             | open-source |
| huggyllama/llama-13b                     | open-source |
| huggyllama/llama-30b                     | open-source |
| EleutherAI/llemma\_7b                    | open-source |
| meta-llama/Llama-3-70b-hf                | open-source |
| meta-llama/Meta-Llama-3.1-8B-Reference   | open-source |
| meta-llama/Meta-Llama-3.1-70B-Reference  | open-source |


================================================
File: docs/api-overview/model-database/video-models.md
================================================
---
icon: video
---

# Video Models

| ID                                                          | Provider |
| ----------------------------------------------------------- | -------- |
| [runway-gen3/turbo/image-to-video](../video-models/runway/) | runway   |
| [luma-ai](../video-models/luma-ai-v2/)                      | luma-ai  |


================================================
File: docs/api-overview/model-database/vision-models.md
================================================
---
icon: eye
description: >-
  Welcome to the Vision Models API documentation! The AI/ML API allows you to
  leverage vision capabilities to analyze and understand images through our
  models.
---

# Vision Models

| ID                                             | Provider    |
| ---------------------------------------------- | ----------- |
| gpt-4o                                         | open-ai     |
| gpt-4o-2024-08-06                              | open-ai     |
| gpt-4o-2024-05-13                              | open-ai     |
| gpt-4o-mini                                    | open-ai     |
| gpt-4o-mini-2024-07-18                         | open-ai     |
| gpt-4-turbo                                    | open-ai     |
| gpt-4-turbo-2024-04-09                         | open-ai     |
| meta-llama/Llama-3.2-90B-Vision-Instruct-Turbo | open-source |
| meta-llama/Llama-Vision-Free                   | open-source |
| meta-llama/Llama-3.2-11B-Vision-Instruct-Turbo | open-source |
| gemini-1.5-flash                               | google      |
| gemini-1.5-pro                                 | google      |
| claude-3-5-sonnet-latest                       | anthropic   |
| claude-3-haiku-latest                          | anthropic   |
| claude-3-opus-latest                           | anthropic   |
| claude-3-sonnet-latest                         | anthropic   |
| claude-3-5-haiku-latest                        | anthropic   |
| qwen/qvq-72b-preview                           | openrouter  |

## Key Features

* **Image Analysis**: Understand and describe the content of images.
* **Flexible Input Methods**: Supports both image URLs and base64 encoded images.
* **Multiple Image Inputs**: Analyze multiple images in a single request.

## Quick Start

Images can be provided to the model in two main ways: by passing an image URL or by passing the base64 encoded image directly in the request.

### **Uploading Images by URL**

<details>

<summary>Python Example</summary>

```python
import requests
import json

url = "https://api.aimlapi.com/chat/completions"

payload = json.dumps({
  "model": "gpt-4o",
  "messages": [
    {
      "role": "user",
      "content": [
        {"type": "text", "text": "What’s in this image?"},
        {
          "type": "image_url",
          "image_url": {
            "url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg"
          }
        }
      ]
    }
  ],
  "max_tokens": 300
})

headers = {
  'Content-Type': 'application/json',
  'Authorization': 'Bearer YOUR_API_KEY'
}

response = requests.post(url, headers=headers, data=payload)
print(response.json())

```

</details>

{% hint style="info" %}
In this example, the GPT-4o model was used with the corresponding set of parameters. If you are using models from Anthropic (claude-3.5-sonnet, etc), check [here](../text-models-llm/function-calling/anthropic.md).
{% endhint %}

### Uploading Base64 Encoded Images

For local images, you can pass the base64 encoded image to the model.

<details>

<summary>Python Example</summary>

```python
import base64
import requests

# Function to encode the image
def encode_image(image_path):
  with open(image_path, "rb") as image_file:
    return base64.b64encode(image_file.read()).decode('utf-8')

# Path to your image
image_path = "path_to_your_image.jpg"
base64_image = encode_image(image_path)

url = "https://api.aimlapi.com/chat/completions"
headers = {
  "Content-Type": "application/json",
  "Authorization": "Bearer YOUR_API_KEY"
}
payload = {
  "model": "gpt-4o",
  "messages": [
    {
      "role": "user",
      "content": [
        {"type": "text", "text": "What’s in this image?"},
        {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"}}
      ]
    }
  ],
  "max_tokens": 300
}

response = requests.post(url, headers=headers, json=payload)
print(response.json())

```

</details>

{% hint style="info" %}
In this example, the GPT-4o model was used with the corresponding set of parameters. If you are using models from Anthropic (claude-3.5-sonnet, etc), check [here](../text-models-llm/function-calling/anthropic.md).
{% endhint %}

### Multiple Image Inputs

The API can process multiple images in a single request.

**Python Example**

```python
import requests
import json

url = "https://api.aimlapi.com/chat/completions"

payload = json.dumps({
  "model": "gpt-4o",
  "messages": [
    {
      "role": "user",
      "content": [
        {"type": "text", "text": "What are in these images? Is there any difference between them?"},
        {"type": "image_url", "image_url": {"url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg"}},
        {"type": "image_url", "image_url": {"url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg"}}
      ]
    }
  ],
  "max_tokens": 300
})

headers = {
  'Content-Type': 'application/json',
  'Authorization': 'Bearer YOUR_API_KEY'
}

response = requests.post(url, headers=headers, data=payload)
print(response.json())

```


================================================
File: docs/api-overview/speech-models/README.md
================================================
---
icon: waveform
description: Overview of the available speech model providers.
layout:
  title:
    visible: true
  description:
    visible: false
  tableOfContents:
    visible: true
  outline:
    visible: true
  pagination:
    visible: true
---

# Speech Models

With our API you are able to synthesize speech and transform speech into text.

## Providers

### Deepgram

### OpenAI

* Whisper Model

## Getting Started

{% content-ref url="speech-to-text-stt.md" %}
[speech-to-text-stt.md](speech-to-text-stt.md)
{% endcontent-ref %}

{% content-ref url="text-to-speech-tts.md" %}
[text-to-speech-tts.md](text-to-speech-tts.md)
{% endcontent-ref %}


================================================
File: docs/api-overview/speech-models/speech-to-text-stt.md
================================================
---
icon: brackets-curly
---

# Speech-to-Text (STT)

{% swagger src="https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27" path="/stt" method="post" %}
[https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27](https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27)
{% endswagger %}


================================================
File: docs/api-overview/speech-models/text-to-speech-tts.md
================================================
---
icon: brackets-curly
---

# Text-to-Speech (TTS)

{% swagger src="https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27" path="/tts" method="post" %}
[https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27](https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27)
{% endswagger %}


================================================
File: docs/api-overview/text-models-llm/README.md
================================================
---
icon: text-size
description: Overview of the capabilities of AIML API text models (LLMs).
layout:
  title:
    visible: true
  description:
    visible: false
  tableOfContents:
    visible: true
  outline:
    visible: true
  pagination:
    visible: true
---

# Text Models (LLM)

## Overview

The AI/ML API provides access to text-based models, also known as Large Language Models (LLMs), and allows you to interact with them through natural language. These models can be applied to various tasks, enabling the creation of diverse applications using our API.

For example, text models can be used to:

* Create a system that searches your photos using text prompts.
* Act as a psychological supporter.
* Play games with you through natural language.
* Assist you with coding.
* Perform a security assessment (pentest) on servers for vulnerabilities.
* Write documentation for your services.
* Serve as a grammar corrector for multiple languages with deep context understanding.
* And much more.

We support numerous Large Language Models. You can view the complete list of models in our database.

{% content-ref url="../model-database/text-models.md" %}
[text-models.md](../model-database/text-models.md)
{% endcontent-ref %}

## Next Steps

* [Learn what a _completion_ is and understand why modern models are often referred to as _chat models_.](completion-or-chat-models.md)


================================================
File: docs/api-overview/text-models-llm/chat-completion.md
================================================
---
icon: code
---

# Chat Completion

### Create chat completion

{% hint style="info" %}
Please note that there is not just one, but several variations of this API described below, each with its own set of supported models and available parameters. You can explore these variations by selecting different options from the **object** dropdown menu and choose the one that suits you best.
{% endhint %}

{% swagger src="https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27" path="/v1/chat/completions" method="post" %}
[https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27](https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27)
{% endswagger %}


================================================
File: docs/api-overview/text-models-llm/completion-or-chat-models.md
================================================
---
icon: book-open
description: An explanation of completion and chat completion terms.
layout:
  title:
    visible: true
  description:
    visible: false
  tableOfContents:
    visible: true
  outline:
    visible: true
  pagination:
    visible: true
---

# Completion or Chat Models

## What is a Completion

At a bare minimum, a text model is a large mathematical model trained to fulfill a single task: predicting the next token or character. This process is called **completion** and you will often encounter this term throughout your journey.

For example, when using the completion text model `gpt-3.5-turbo-instruct`, you can provide an initial prompt to the model:

```
A long time ago, there were three princesses in a distant kingdom:
```

Running the model might yield the following output:

```
A long time ago, there were three princesses in a distant kingdom: 
Princess Narcissa, who was beautiful but vain, Princess Rosa, who was kind and gentle, 
and Princess Aurora, who was strong and brave. 
The three sisters lived in a beautiful palace with their parents, the king and queen.
```

This is a simple text completion. However, when training datasets become larger and are refined by human alignments, we can achieve truly AI-like results that even researchers did not initially anticipate.

## What is a Chat Completion

To make text models useful in code and applications beyond generating arbitrary creative information, the model needs to be pretrained to return data in a specific format. Usually, using a text model feels like a chatting experience: you ask something in a certain role, and you get your answer as if it's from someone in another role. With this in mind, model providers train their models and feed their initial training data with some metadata, such as roles. This allows the model to respond in a certain format and be used in many complex applications.

For example, the model training data might look like the following:

```
USER: What's the color of the sky?
ASSISTANT: The color of the sky can vary depending on several factors, but it is most commonly perceived as blue during the daytime.
USER: What was the theme we discussed in the previous sentence?
ASSISTANT: The theme of the previous sentence centered around the color of the sky.
```

The above data is written in a chat-like conversation format. The training dataset contains a huge amount of these conversations, and during the training process, the model learns the relationships between words and characters, enabling it to return them in the same predictable format.

After generating data, a subsystem parses this information and returns it in a format that can easily be handled by your code, such as the following JSON:

```json
[
  { "message": "Hi!", "role": "user" },
  { "message": "Hi, how can I help you?", "role": "assistant" }
]
```

### What roles exist

There are several roles frequently used in chat models. The system role usually appears once, while other roles can appear multiple times:

* **System**: The main instruction about formatting, rules, and acting.
* **Assistant**: The model's response.
* **User**: The user's content.
* **Tool**: Response for external tools that can be used by the model.

Using these roles, you can create complex behaviors and protect your AI from misleading use by user content.

***


================================================
File: docs/api-overview/text-models-llm/embeddings.md
================================================
---
icon: code
---

# Embeddings

New Embedding Models

The latest embedding models from AI/ML API, `text-embedding-3-small` and `text-embedding-3-large`, are now available. These models offer cost savings, enhanced multilingual support, and customizable parameters to manage their size.

#### What are Embeddings?

Embeddings from AI/ML API quantify the similarity between text strings. These embeddings are particularly useful for:

* **Search**: Rank search results by their relevance to a query.
* **Clustering**: Group similar text strings together.
* **Recommendations**: Suggest items based on related text strings.
* **Anomaly Detection**: Identify outliers that differ significantly from the norm.
* **Diversity Measurement**: Analyze the spread of similarities within a dataset.
* **Classification**: Categorize text strings by comparing them to labeled examples.

An embedding is a vector (list) of floating-point numbers, where the distance between vectors indicates their relatedness. Smaller distances indicate higher similarity, while larger distances suggest lower similarity.

For more information on Embeddings pricing, visit our pricing page. Costs are calculated based on the number of tokens in the input.



**Example: Generating Embeddings**

```
curl https://api.aimlapi.com/v1/embeddings \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $AIMLAPI_API_KEY" \
  -d '{
    "input": "Your text string goes here",
    "model": "text-embedding-3-small"
  }'

```

The response will include the embedding vector and additional metadata.

**Example Embedding Response**

```json
{
  "object": "list",
  "data": [
    {
      "object": "embedding",
      "index": 0,
      "embedding": [
        -0.006929283495992422,
        -0.005336422007530928,
        // ...(omitted for spacing)
        -4.547132266452536e-05,
        -0.024047505110502243
      ]
    }
  ],
  "model": "text-embedding-3-small",
  "usage": {
    "prompt_tokens": 5,
    "total_tokens": 5
  }
}


```

By default, the length of the embedding vector is 1536 for `text-embedding-3-small` or 3072 for `text-embedding-3-large`. You can reduce the dimensions of the embedding using the `dimensions` parameter without losing its ability to represent concepts. More details on embedding dimensions can be found in the embedding use case section.

#### Embedding Models

AI/ML API offers two robust third-generation embedding models (indicated by -3 in the model ID).&#x20;

| Model                  | \~ Pages per Dollar | Performance on MTEB Eval | Max Input Tokens |
| ---------------------- | ------------------- | ------------------------ | ---------------- |
| text-embedding-3-small | 62,500              | 62.3%                    | 8191             |
| text-embedding-3-large | 9,615               | 64.6%                    | 8191             |
| text-embedding-ada-002 | 12,500              | 61.0%                    | 8191             |

#### Example in Python

Here's how to use the embeddings API in Python:

```python
import os
import json
import openai

# Initialize the API client
client = openai.OpenAI(
    base_url="https://api.aimlapi.com/v1",
    api_key=os.getenv("AIMLAPI_API_KEY"),
)

# Define the text for which to generate an embedding
text = "Your text string goes here"

# Request the embedding
response = client.embeddings.create(
    input=text,
    model="text-embedding-3-small"
)

# Extract the embedding from the response
embedding = response['data'][0]['embedding']

# Print the embedding
print(json.dumps(embedding, indent=2))

```

This Python example shows how to set up an API client, send text to the embeddings API, and handle the response to extract and print the embedding vector.


================================================
File: docs/api-overview/text-models-llm/parameters.md
================================================
---
icon: book-open
description: >-
  Learn how you can tweak results of your AI/ML API responses with tailored
  requests.
---

# Parameters

## Overview

When you send a request to the text model, you can specify custom parameters that affect the model's response. These parameters can control your model usage, optimize your requests, and achieve more creative or exact results.

{% hint style="warning" %}
Whether a parameter is required or optional can be checked in the API Reference for the specific model.

Also, pay attention: some of the parameters described below have similar names and logic. Make sure to check the API Reference to confirm which parameter is used by the model you’ve selected to work with.
{% endhint %}

## Possible Parameters

### Frequency Penalty

Penalizes tokens based on how frequently they have already appeared in the generated text. ChatGPT series models use numbers between -2.0 and 2.0: a **positive value** decreases the likelihood of repeating frequently used tokens, a **negative value** increases it, and a **value of 0** means no penalty is applied for token frequency.

```python
"frequency_penalty": 0.75  # Applies a penalty for frequently used tokens. 
```

### Log Probs

When enabled, the model will return the log probabilities (logarithms of the probabilities) of the top predicted tokens for each step in the completion. For example, if you set `logprobs=4`, the model will include the log probabilities for the top 5 most likely tokens at every step:

```python
"logprobs": 4  # Instruction to return log probabilities for the 4 most likely tokens
```

### Logit bias

Modifies the likelihood of specified tokens appearing in the completion. By assigning a bias value to particular tokens, you can increase or decrease their probability in the generated text.

Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.

{% hint style="info" %}
Since the parameter takes in tokens, not text, you’ll want to use some external tokenizer tool to convert text to token IDs.&#x20;
{% endhint %}

<details>

<summary>Example</summary>

Let's go through some example. If we call the Completions endpoint with the prompt _“Once upon a,”_ the completion is very likely going to start with _“ time”._

For example, the word _“time”_ tokenizes to the ID `2435` and the word _“ time”_ (which has a space at the start) tokenizes to the ID `640`. We can pass these through `logit_bias` with `-100` to ban them from appearing in the completion, like so:

```python
messages=[{"role": "system", "content": "You finish user's sentences."},
             "role": "user", "content": "Once upon a"} ] 
logit_bias={2435:-100, 640:-100}
```

Now, the prompt _“Once upon a”_ generates the completion _“midnight dreary, while I pondered, weak and weary.”_ Notice that the word _“time”_ is nowhere to be found, because we’ve effectively banned that token using `logit_bias`.

</details>

### Maximum Tokens

This parameter specifies the maximum number of tokens (words or pieces of words) that the model will generate in response to the prompt. A lower number of tokens typically results in faster response times.

```python
max_tokens = 50  # Limit the model to generate up to 50 tokens.
```

### Messages

A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, images, and audio.

```python
messages=[{"role": "system", "content": "You are a helpful assistant."},
{"role": "user", "content": "Hello!"}
]
```

### Presence Penalty

Adjusts the likelihood of the model including tokens that have already appeared in the generated text. ChatGPT series models use numbers between -2.0 and 2.0: a **positive value** reduces the chance of reusing tokens, a **negative value** increases it, and a **value of 0** means no penalty is applied, and the model can freely repeat tokens.

```python
"presence_penalty": 0.8  # Applies a penalty to discourage token reusing. 
```

### Repetition Penalty

This parameter discourages the model from repeating the same line or phrase, promoting more diverse and engaging content. The penalty scales with how many times a token has been used — the more frequent the token, the greater the penalty.

```python
"repetition_penalty" = 1.2  # Applies a penalty to discourage repetition.
```

### Stop Sequences

Stop sequences are strings that, when detected in the model's output, signal the model to stop generating further tokens. This is particularly useful when the desired output is a concise response or a single word.

```python
stop = ["\n"]  # Instructs the model to stop generating when it produces a newline character.
```

### Temperature

The temperature controls the randomness of the model's output. Setting it to 0 results in deterministic output, whereas higher values up to 1 introduce more variation and creativity in responses.

```python
temperature = 0.5  # Sets a balance between randomness and determinism.
```

### Tool choice

Users can use `tool_choice` to specify how external tools such as user defined functions or APIs are used. The most commonly used values ​​are:

* `"auto"`: default mode. Model decides if it uses the tool or not.
* `"any"`: forces tool use.
* `"none"`: prevents tool use.

{% hint style="warning" %}
For a specific set of acceptable values, see the API Reference for your model.
{% endhint %}

```python
tool_choice = "any"  # Instructs the model to use the tool mandatorily
```

### Tools

A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for.&#x20;

<details>

<summary>Example</summary>

```python
  {
    "type": "function",
    "function": {
      "name": "get_current_weather",
      "description": "Get the current weather in a given location",
      "parameters": {
        "type": "object",
        "properties": {
          "location": {
            "type": "string",
            "description": "The city and state, e.g. San Francisco, CA"
          },
          "unit": {
            "type": "string",
            "enum": [
              "celsius",
              "fahrenheit"
            ]
          }
        }
      }
    }
  }
]
```

</details>

{% hint style="warning" %}
The maximum number of tools that can be connected is limited. For example, in the case of OpenAI models, the maximum number is 128.
{% endhint %}

### Top P (Nucleus Sampling)

The top P parameter, also known as nucleus sampling, filters the model's token choices such that the cumulative probability of the tokens considered at each step is at least P. This method allows for more dynamic and contextually relevant responses.

```python
top_p = 0.9  # Only tokens that contribute to the top 90% cumulative probability are considered.
```

### Top K (Top-K Sampling)

Top K limits the model's choices to the K most likely next tokens. Lower values can speed up generation and may improve coherency by focusing on the most probable tokens.

```python
top_k = 40  # The model will only consider the top 40 most probable next tokens.
```


================================================
File: docs/api-overview/text-models-llm/function-calling/README.md
================================================
---
description: >-
  Learn how to connect large language models to external tools using function
  calls.
icon: code
---

# Function Calling

## Introduction

When using the API, you can define functions that the model can choose to call, generating a JSON object with the necessary arguments. The Chat Completions API itself does not execute these functions; instead, it outputs the JSON, which you can then use to call the function within your code.

The latest models (gpt-4o, gpt-4-turbo, and gpt-3.5-turbo) are designed to detect when a function should be called based on the input and to produce JSON that closely matches the function signature. However, this functionality comes with potential risks. We strongly recommend implementing user confirmation steps before performing actions that could impact the real world (e.g., sending an email, posting online, making a purchase).

This guide focuses on function calling with the Chat Completions API.

## Common Use Cases

Function calling allows you to obtain structured data reliably from the model. For example, you can:

* **Create assistants that answer questions by calling external APIs**
  * Example functions: `send_email(to: string, body: string)`, `get_current_weather(location: string, unit: 'celsius' | 'fahrenheit')`
* **Convert natural language into API calls**
  * Example conversion: "Who are my top customers?" to `get_customers(min_revenue: int, created_before: string, limit: int)`, then call your internal API
* **Extract structured data from text**
  * Example functions: `extract_data(name: string, birthday: string)`, `sql_query(query: string)`

## Basic Sequence of Steps for Function Calling

1. **Call the model** with the user query and a set of functions defined in the `functions` parameter.
2. **Model response**: The model may choose to call one or more functions. If so, it will output a stringified JSON object adhering to your custom schema (note: the model may hallucinate parameters).
3. **Parse the JSON**: In your code, parse the string into JSON and call the function with the provided arguments if they exist.
4. **Call the model again**: Append the function response as a new message and let the model summarize the results back to the user.

### Supported Models

* `mistralai/Mixtral-8x7B-Instruct-v0.1`
* `mistralai/Mistral-7B-Instruct-v0.1`
* `togethercomputer/CodeLlama-34b-Instruct`
* `gpt-4o-2024-05-13`
* `gpt-4-turbo`
* `gpt-4-turbo-2024-04-09`
* `gpt-4-turbo-preview`
* `gpt-4-0125-preview`
* `gpt-4-1106-preview`
* `gpt-4`
* `gpt-4-0613`
* `gpt-3.5-turbo`
* `gpt-3.5-turbo-0125`
* `gpt-3.5-turbo-1106`
* `gpt-3.5-turbo-0613`

## Examples

{% code title="python" %}
```python
import os
import json
import openai

client = openai.OpenAI(
    base_url="https://api.aimlapi.com/v1",
    api_key='AI_ML_API',
)

tools = [
  {
    "type": "function",
    "function": {
      "name": "get_current_weather",
      "description": "Get the current weather in a given location",
      "parameters": {
        "type": "object",
        "properties": {
          "location": {
            "type": "string",
            "description": "The city and state, e.g. San Francisco, CA"
          },
          "unit": {
            "type": "string",
            "enum": [
              "celsius",
              "fahrenheit"
            ]
          }
        }
      }
    }
  }
]

messages = [
    {"role": "system", "content": "You are a helpful assistant that can access external functions. The responses from these function calls will be appended to this dialogue. Please provide responses based on the information from these function calls."},
    {"role": "user", "content": "What is the current temperature of New York, San Francisco, and Chicago?"}
]

response = client.chat.completions.create(
    model="gpt-4o",
    messages=messages,
    tools=tools,
    tool_choice="auto",
)

print(json.dumps(response.choices[0].message.model_dump()['tool_calls'], indent=2))

```
{% endcode %}


================================================
File: docs/api-overview/text-models-llm/function-calling/anthropic.md
================================================
---
icon: code
---

# Anthropic

## Features

* **Text completions:** Build advanced chat bots or text processors
* **Function Calling:** Utilize tools for specific tasks and API calling.
* **Vision Tasks:** Process and analyze images.

## Example Requests

{% swagger src="https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27" path="/messages" method="post" %}
[https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27](https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27)
{% endswagger %}

## **Vision**

{% hint style="info" %}
**Note:** API only support BASE64 String as Image input.&#x20;
{% endhint %}

### Possible Media Types

* `image/png`
* `image/gif`
* `image/webp`

```python
import httpx
import base64
from openai import OpenAI

client = OpenAI(
    base_url='https://api.aimlapi.com',
    api_key='<YOUR_API_KEY>'    
)  

image_url = "https://upload.wikimedia.org/wikipedia/commons/a/a7/Camponotus_flavomarginatus_ant.jpg"
image_media_type = "image/jpeg"
image_data = base64.standard_b64encode(httpx.get(image_url).content).decode("utf-8")

response = client.chat.completions.create(
    model="claude-3-5-sonnet-latest",
    messages=[
    {
        "role": "user",
        "content": [
            {
                "type": "image",
                "source": {
                    "type": "base64",
                    "media_type": image_media_type,
                    "data": imag1_data,
                },
            },
            {
                "type": "text",
                "text": "Describe this image."
            }
        ],
    }
],
)
print(response)

```

## **Function Calling**

To process text and use function calling, follow the examples below:

### **Example Request 1: Get Weather Information**

```python
import requests

url = "https://api.aimlapi.com/messages"
headers = {
    "Authorization": "Bearer YOUR_API_KEY",
    "Content-Type": "application/json"
}
payload = {
  "model": "claude-3-5-sonnet-20240620",
  "max_tokens": 1024,
  "tools": [
    {
      "name": "get_weather",
      "description": "Get the current weather in a given location",
      "input_schema": {
        "type": "object",
        "properties": {
          "location": {
            "type": "string",
            "description": "The city and state, e.g. San Francisco, CA"
          }
        }
      }
    }
  ],
  "messages": [
    {
      "role": "user",
      "content": "What is the weather like in San Francisco?"
    }
  ],
  "stream": false
}
response = requests.post(url, json=payload, headers=headers)
print(response.json())

```

### **Example Request 2: Simple Text Response**

```python
import requests

url = "https://api.aimlapi.com/messages"
headers = {
    "Authorization": "Bearer YOUR_API_KEY",
    "Content-Type": "application/json"
}
payload = {
  "model": "claude-3-5-sonnet-20240620",
  "max_tokens": 1024,
  "messages": [
    {
      "role": "user",
      "content": "How are you?"
    }
  ],
  "stream": false
}
response = requests.post(url, json=payload, headers=headers)
print(response.json())

```

{% hint style="info" %}
**Pro tip:** you can assign a system role to the Claude models by using "system" parameter outside of messages array.
{% endhint %}

```json
{
    model="claude-3-5-sonnet-20240620",
    max_tokens=2048,
    system="You are a seasoned data scientist at a Fortune 500 company.", # <-- role prompt
    messages=[
        {"role": "user", "content": "Analyze this dataset for anomalies: <dataset>{{DATASET}}</dataset>"}
    ]
}
```

## Example Response

The responses from the AI/ML API for Anthropic models will typically include the generated text or results from the tool called. Here is an example response for a weather query:

```python
{
  "id": "msg-12345",
  "object": "message",
  "created": 1627684940,
  "model": "claude-3-5-sonnet-20240620",
  "choices": [
    {
      "message": {
        "role": "assistant",
        "content": "The weather in San Francisco is currently sunny with a temperature of 68°F."
      },
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 10,
    "completion_tokens": 15,
    "total_tokens": 25
  }
}
```

## Streaming with Python SDK

To enable streaming of responses, set `stream=True` in your request payload.

```python
import requests

url = "https://api.aimlapi.com/messages"
headers = {
    "Authorization": "Bearer YOUR_API_KEY",
    "Content-Type": "application/json"
}
payload = {
  "model": "claude-3-5-sonnet-20240620",
  "max_tokens": 1024,
  "tools": [
    {
      "name": "get_weather",
      "description": "Get the current weather in a given location",
      "input_schema": {
        "type": "object",
        "properties": {
          "location": {
            "type": "string",
            "description": "The city and state, e.g. San Francisco, CA"
          }
        }
      }
    }
  ],
  "messages": [
    {
      "role": "user",
      "content": "What is the weather like in San Francisco?"
    }
  ]
```


================================================
File: docs/api-overview/text-models-llm/function-calling/image-to-text-vision.md
================================================
---
icon: code
---

# Image-To-Text (Vision)

## Vision Tasks

**Example Request: Image Analysis**

## Example

```python
import requests
import json

url = "https://api.aimlapi.com/chat/completions"

payload = json.dumps({
  "model": "gpt-4o",
  "messages": [
    {
      "role": "user",
      "content": [
        {"type": "text", "text": "What’s in this image?"},
        {
          "type": "image_url",
          "image_url": {
            "url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg"
          }
        }
      ]
    }
  ],
  "max_tokens": 300
})

headers = {
  'Content-Type': 'application/json',
  'Authorization': 'Bearer YOUR_API_KEY'
}

response = requests.post(url, headers=headers, data=payload)
print(response.json())

```


================================================
File: docs/api-overview/text-models-llm/managing-assistants-and-threads/README.md
================================================
---
icon: brackets-curly
---

# Managing Assistants & Threads

## Overview

Threads and Messages represent a conversation session between an Assistant and a user. There is no limit to the number of Messages you can store in a Thread. When the size of the Messages exceeds the model's context window, the Thread will smartly truncate messages before fully dropping the least important ones.

## API Reference

{% swagger src="https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27" path="/assistants/{assistantId}" method="delete" %}
[https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27](https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27)
{% endswagger %}

{% swagger src="https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27" path="/assistants" method="post" %}
[https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27](https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27)
{% endswagger %}

{% swagger src="https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27" path="/assistants" method="get" %}
[https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27](https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27)
{% endswagger %}

{% swagger src="https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27" path="/assistants/{assistantId}" method="get" %}
[https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27](https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27)
{% endswagger %}

{% swagger src="https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27" path="/assistants/{assistantId}" method="post" %}
[https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27](https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27)
{% endswagger %}



================================================
File: docs/api-overview/text-models-llm/managing-assistants-and-threads/threads/README.md
================================================
---
icon: brackets-curly
description: >-
  The Threads API is very useful for creating a chat-like experience with a
  relatively large message history
---

# Threads

## API Reference

{% swagger src="https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27" path="/threads/{threadId}" method="delete" %}
[https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27](https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27)
{% endswagger %}

{% swagger src="https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27" path="/threads" method="post" %}
[https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27](https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27)
{% endswagger %}

{% swagger src="https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27" path="/threads/{threadId}" method="get" %}
[https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27](https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27)
{% endswagger %}

{% swagger src="https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27" path="/threads/{threadId}" method="post" %}
[https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27](https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27)
{% endswagger %}

## Example

```python
thread = client.beta.threads.create(
  messages=[
    {
      "role": "user",
      "content": "Create 3 data visualizations based on the trends in this file.",
      "attachments": [
        {
          "file_id": file.id,
          "tools": [{"type": "code_interpreter"}]
        }
      ]
    }
  ]
)        
```


================================================
File: docs/api-overview/text-models-llm/managing-assistants-and-threads/threads/messages.md
================================================
---
icon: brackets-curly
---

# Messages

## API Reference

{% swagger src="https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27" path="/threads/{threadId}/messages" method="get" %}
[https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27](https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27)
{% endswagger %}



{% swagger src="https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27" path="/threads/{threadId}/messages" method="post" %}
[https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27](https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27)
{% endswagger %}

{% swagger src="https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27" path="/threads/{threadId}/messages/{messageId}" method="get" %}
[https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27](https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27)
{% endswagger %}

{% swagger src="https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27" path="/threads/{threadId}/messages/{messageId}" method="post" %}
[https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27](https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27)
{% endswagger %}


================================================
File: docs/api-overview/text-models-llm/managing-assistants-and-threads/threads/runs.md
================================================
---
icon: brackets-curly
---

# Runs

## API Reference

{% swagger src="https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27" path="/threads/{threadId}/runs" method="get" %}
[https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27](https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27)
{% endswagger %}

{% swagger src="https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27" path="/threads/{threadId}/runs" method="post" %}
[https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27](https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27)
{% endswagger %}

{% swagger src="https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27" path="/threads/{threadId}/runs/{runId}" method="post" %}
[https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27](https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27)
{% endswagger %}

{% swagger src="https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27" path="/threads/{threadId}/runs/{runId}" method="get" %}
[https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27](https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27)
{% endswagger %}


================================================
File: docs/api-overview/video-models/README.md
================================================
---
icon: video
description: Short overview of the available video model providers.
layout:
  title:
    visible: true
  description:
    visible: false
  tableOfContents:
    visible: true
  outline:
    visible: true
  pagination:
    visible: true
---

# Video Models

## Overview

With our API you can generate videos from your prompt and imagination.

## Providers

{% content-ref url="kling/" %}
[kling](kling/)
{% endcontent-ref %}

{% content-ref url="luma-ai-v1-legacy/luma-ai-text-to-video.md" %}
[luma-ai-text-to-video.md](luma-ai-v1-legacy/luma-ai-text-to-video.md)
{% endcontent-ref %}

{% content-ref url="runway/" %}
[runway](runway/)
{% endcontent-ref %}


================================================
File: docs/api-overview/video-models/kling/README.md
================================================
---
icon: satellite-dish
---

# Kling

## Overview

The Kling API allows developers to generate, retrieve, and extend AI-generated content using a variety of inputs. This API is particularly useful for creative applications, such as generating visual content from text prompts.

{% content-ref url="overview.md" %}
[overview.md](overview.md)
{% endcontent-ref %}

{% content-ref url="generate-text-to-video.md" %}
[generate-text-to-video.md](generate-text-to-video.md)
{% endcontent-ref %}

{% content-ref url="generate-image-to-video.md" %}
[generate-image-to-video.md](generate-image-to-video.md)
{% endcontent-ref %}

{% content-ref url="fetch-generation.md" %}
[fetch-generation.md](fetch-generation.md)
{% endcontent-ref %}


================================================
File: docs/api-overview/video-models/kling/fetch-generation.md
================================================
---
icon: code
---

# Fetch Generation

## Overview

After sending a request for video generation, this task is added to the queue. Based on the service's load, the generation can be completed in seconds or take a bit more. Here are the API to fetch the generations.

{% swagger src="https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27" path="/v2/generate/video/kling/generation" method="get" %}
[https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27](https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27)
{% endswagger %}

## Example

{% hint style="info" %}
Ensure you replace `"my_key"` with your actual API key before running the code.
{% endhint %}

### Fetch Single Generation

You can send the following request:

{% tabs %}
{% tab title="JavaScript" %}
```javascript
const main = async () => {
  const params = new URLSearchParams({
    generation_id: "<YOUR_GENERATION_ID>"
  });
  const response = await fetch(`https://api.aimlapi.com/v2/generate/video/kling/generation?${params.toString()}`, {
    method: 'GET',
    headers: {
      Authorization: 'Bearer my_key',
      'Content-Type': 'application/json',
    },
  }).then((res) => res.json());

  console.log('Generation:', response);
};

main()

```
{% endtab %}

{% tab title="Python" %}
```python
import requests


def main():
    url = "https://api.aimlapi.com/v2/generate/video/kling/generation"
    params = {
        "generation_id": "<YOUR_GENERATION_ID>",
    }
    headers = {"Authorization": "Bearer my_key", "Content-Type": "application/json"}

    response = requests.get(url, params=params, headers=headers)
    print("Generation:", response.json())


if __name__ == "__main__":
    main()
```
{% endtab %}
{% endtabs %}


================================================
File: docs/api-overview/video-models/kling/generate-image-to-video.md
================================================
---
icon: code
---

# Generate Image to Video

## Overview

You can generate a video using the AI/ML API. In the basic setup, you need only a prompt and image URL.

{% hint style="info" %}
Please note that there is not just one, but several variations of this API described below, each with its own set of supported models and available parameters. You can explore these variations by selecting different options from the **object** dropdown menu and choose the one that suits you best.
{% endhint %}

{% swagger src="https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27" path="/v2/generate/video/kling/generation" method="post" %}
[https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27](https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27)
{% endswagger %}

## Example

{% hint style="info" %}
Ensure you replace `"my_key"` with your actual API key before running the code.
{% endhint %}

{% tabs %}
{% tab title="JavaScript" %}
```javascript
const main = async () => {
  const response = await fetch('https://api.aimlapi.com/v2/generate/video/kling/generation', {
    method: 'POST',
    headers: {
      Authorization: 'Bearer my_key',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: 'kling-video/v1/standard/image-to-video',
      prompt: 'Mona Lisa puts on glasses with her hands.',
      ratio: '16:9',
      image_url: 'https://s2-111386.kwimgs.com/bs2/mmu-aiplatform-temp/kling/20240620/1.jpeg',
      duration: '5',
    }),
  }).then((res) => res.json());

  console.log('Generation:', response);
};

main()
```
{% endtab %}

{% tab title="Python" %}
```python
def main():
    url = "https://api.aimlapi.com/v2/generate/video/kling/generation"
    payload = {
        "model": "kling-video/v1/standard/image-to-video",
        "prompt": "Mona Lisa puts on glasses with her hands.",
        "ratio": "16:9",
        "image_url": "https://s2-111386.kwimgs.com/bs2/mmu-aiplatform-temp/kling/20240620/1.jpeg",
        "duration": "5",
    }
    headers = {"Authorization": "Bearer my_key", "Content-Type": "application/json"}

    response = requests.post(url, json=payload, headers=headers)
    print("Generation:", response.json())


if __name__ == "__main__":
    main()

```
{% endtab %}
{% endtabs %}


================================================
File: docs/api-overview/video-models/kling/generate-text-to-video.md
================================================
---
icon: code
---

# Generate Text to Video

## Overview

You can generate a video using the AI/ML API. In the basic setup, you need only a prompt.

{% hint style="info" %}
Please note that there is not just one, but several variations of this API described below, each with its own set of supported models and available parameters. You can explore these variations by selecting different options from the **object** dropdown menu and choose the one that suits you best.
{% endhint %}

{% swagger src="https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27" path="/v2/generate/video/kling/generation" method="post" %}
[https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27](https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27)
{% endswagger %}

## Example

{% hint style="info" %}
Ensure you replace `"my_key"` with your actual API key before running the code.
{% endhint %}

{% tabs %}
{% tab title="JavaScript" %}
```javascript
const main = async () => {
  const response = await fetch('https://api.aimlapi.com/v2/generate/video/kling/generation', {
    method: 'POST',
    headers: {
      Authorization: 'Bearer my_key',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: 'kling-video/v1/standard/text-to-video',
      prompt: 'A DJ on the stand is playing, around a World War II battlefield, lots of explosions, thousands of dancing soldiers, between tanks shooting, barbed wire fences, lots of smoke and fire, black and white old video: hyper realistic, photorealistic, photography, super detailed, very sharp, on a very white background',
      ratio: '16:9',
      duration: '5',
    }),
  }).then((res) => res.json());

  console.log('Generation:', response);
};
```
{% endtab %}

{% tab title="Python" %}
```python
import requests


def main():
    url = "https://api.aimlapi.com/v2/generate/video/kling/generation"
    payload = {
        "model": "kling-video/v1/standard/text-to-video",
        "prompt": "A DJ on the stand is playing, around a World War II battlefield, lots of explosions, thousands of dancing soldiers, between tanks shooting, barbed wire fences, lots of smoke and fire, black and white old video: hyper realistic, photorealistic, photography, super detailed, very sharp, on a very white background",
        "ratio": "16:9",
        "duration": "5",
    }
    headers = {"Authorization": "Bearer my_key", "Content-Type": "application/json"}

    response = requests.post(url, json=payload, headers=headers)
    print("Generation:", response.json())


if __name__ == "__main__":
    main()

```
{% endtab %}
{% endtabs %}


================================================
File: docs/api-overview/video-models/kling/overview.md
================================================
---
icon: book-open
---

## Overview

The Runway API allows developers to generate, retrieve, and extend AI-generated content using a variety of inputs. This API is particularly useful for creative applications, such as generating visual content from text prompts.

## Authentication

All API requests require a Bearer token for authentication. Include your token in the `Authorization` header of each request.

## Pricing

### Standard plan

Each generation costs 63 000 AI/ML Tokens per second

### Pro plan

Each generation costs 262 500 AI/ML Tokens per second


================================================
File: docs/api-overview/video-models/luma-ai-v1-legacy/README.md
================================================
---
icon: satellite-dish
---

# Luma AI v1 (legacy)

{% hint style="info" %}
Here is our backward-compatible support for the older Luma AI API.
{% endhint %}



================================================
File: docs/api-overview/video-models/luma-ai-v1-legacy/luma-ai-text-to-video.md
================================================
---
icon: code
description: Generate Videos from Text or Images
---

# Luma AI Text-to-Video

## Overview

The Luma AI Dream Machine API allows developers to generate, retrieve, and extend AI-generated content using a variety of inputs. This API is particularly useful for creative applications, such as generating visual content from text prompts.

## Authentication

All API requests require a Bearer token for authentication. Include your token in the `Authorization` header of each request.

## Pricing

Each generation costs 500 000 AI/ML Tokens

## API Reference

{% hint style="info" %}
Ensure you replace `"your-api-key"` with your actual API key before running the code.
{% endhint %}

{% swagger src="https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27" path="/luma-ai/generations" method="post" %}
[https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27](https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27)
{% endswagger %}

### Example

{% tabs %}
{% tab title="Python" %}
```python
import requests

url = "https://api.aimlapi.com/luma-ai/generations"

payload = {
  "aspect_ratio": "16:9",
  "expand_prompt": True,
  "user_prompt": "Flying jellyfish"
}
headers = {
  "Authorization": "Bearer your-api-key",
  "content-type": "application/json"
}

response = requests.post(url, json=payload, headers=headers)

print(response.json())

```
{% endtab %}

{% tab title="JS" %}
```javascript
const axios = require('axios');

const url = "https://api.aimlapi.com/luma-ai/generations";

const payload = {
  aspect_ratio: "16:9",
  expand_prompt: true,
  user_prompt: "Flying jellyfish"
};

const headers = {
  "Authorization": "Bearer your-api-key",
  "content-type": "application/json"
};

axios.post(url, payload, { headers })
  .then(response => console.log(response.data))
  .catch(error => console.error(error));

```
{% endtab %}
{% endtabs %}

{% swagger src="https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27" path="/luma-ai/generation" method="get" %}
[https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27](https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27)
{% endswagger %}

### Example

{% tabs %}
{% tab title="Python" %}
```python
import requests

url = "https://api.aimlapi.com/luma-ai/generation"

querystring = {"ids[0]":"4c9126f3-d9a6-4eaf-aa4c-b64b634f65bd"}

headers = {
  "Authorization": "Bearer your-api-key",
  "content-type": "application/json"
}

response = requests.get(url, headers=headers, params=querystring)

print(response.json())

```
{% endtab %}

{% tab title="JS" %}
```javascript
const axios = require('axios');

const url = "https://api.aimlapi.com/luma-ai/generation";

const params = {
  "ids[0]": "4c9126f3-d9a6-4eaf-aa4c-b64b634f65bd"
};

const headers = {
  "Authorization": "Bearer your-api-key",
  "content-type": "application/json"
};

axios.get(url, { headers, params })
  .then(response => console.log(response.data))
  .catch(error => console.error(error));

```
{% endtab %}
{% endtabs %}

{% swagger src="https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27" path="/luma-ai/generations/{taskId}/extend" method="post" %}
[https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27](https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27)
{% endswagger %}

### Example

{% tabs %}
{% tab title="Python" %}
```python
import requests

url = "https://api.aimlapi.com/luma-ai/generations/57a6cb80-6da0-49bd-b29a-3f089b9e55e4/extend"

payload = {
  "aspect_ratio": "16:9",
  "expand_prompt": True,
  "user_prompt": "Flying jellyfish with a hat"
}
headers = {
  "Authorization": "Bearer your-api-key",
  "content-type": "application/json"
}

response = requests.post(url, json=payload, headers=headers)

print(response.json())

```
{% endtab %}

{% tab title="JS" %}
```javascript
const axios = require('axios');

const url = "https://api.aimlapi.com/luma-ai/generations/57a6cb80-6da0-49bd-b29a-3f089b9e55e4/extend";

const payload = {
  aspect_ratio: "16:9",
  expand_prompt: true,
  user_prompt: "Flying jellyfish with a hat"
};

const headers = {
  "Authorization": "Bearer your-api-key",
  "content-type": "application/json"
};

axios.post(url, payload, { headers })
  .then(response => console.log(response.data))
  .catch(error => console.error(error));

```
{% endtab %}
{% endtabs %}


================================================
File: docs/api-overview/video-models/luma-ai-v2/README.md
================================================
---
icon: satellite-dish
---

# Luma AI v2

## Overview

Our AI/ML API fully supports video generation using Luma AI models. Below are some resources you might find interesting:

{% content-ref url="overview.md" %}
[overview.md](overview.md)
{% endcontent-ref %}

{% content-ref url="generate-video.md" %}
[generate-video.md](generate-video.md)
{% endcontent-ref %}

{% content-ref url="fetch-generations.md" %}
[fetch-generations.md](fetch-generations.md)
{% endcontent-ref %}

{% content-ref url="extend-video.md" %}
[extend-video.md](extend-video.md)
{% endcontent-ref %}


================================================
File: docs/api-overview/video-models/luma-ai-v2/extend-video.md
================================================
---
icon: code
---

# Extend Video

## Overview

You can extend a video using an existing video you generated before (using its ID) or by using an image (by URL). The extension can be done by appending to or prepending from the original content.

### Keywords

The `keywords` parameter controls the following extensions. It can include parameters for defining frames:

* **first frame** (`frame0`)
* **last frame** (`frame1`)

For example, if you want to use an image as a reference for a frame:

```json
{
        "keyframes": {
            "frame0": {
                "type": "image",
                "url": "https://example.com/image1.png"
            }
        }
}
```

Or, in the case of using a previously generated video:

```json
{
    "keyframes": {
        "frame1": {
            "type": "generation",
            "id": "0f3ea4aa-10e7-4dae-af0b-263ab4ac45f9"
        }
    }
}
```

## Examples

{% hint style="info" %}
Ensure you replace `"my_key"` with your actual API key before running the code.
{% endhint %}

### Extension with the Image

{% tabs %}
{% tab title="JavaScript" %}
```javascript
const main = async () => {
  const response = await fetch('https://api.aimlapi.com/v2/generate/video/luma-ai/generation', {
    method: 'POST',
    headers: {
      Authorization: 'Bearer my_key',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      prompt: 'A jellyfish in the ocean',
      aspect_ratio: '19:9',
      keyframes: {
        frame0: {
          type: 'image',
          url: 'https://example.com/image1.png',
        },
      },
    }),
  }).then((res) => res.json());

  console.log('Generation:', response);
};

main();

```
{% endtab %}

{% tab title="Python" %}
```python
import requests


def main()
  url = "https://api.aimlapi.com/v2/generate/video/luma-ai/generation"
  payload = {
    "prompt": "Flying jellyfish",
    "aspect_ratio": "16:9",
    "keyframes": {
      "frame0": {
        "type": "image",
        "url": "https://example.com/image1.png"
      }
    }
  }
  headers = {
    "Authorization": "Bearer my_key",
    "Content-Type": "application/json"
  }
  
  response = requests.post(url, json=payload, headers=headers)
  print("Generation:",  response.json())
  
if __name__ == "__main__":
    main()
```
{% endtab %}
{% endtabs %}

### Extension with the Generation

{% tabs %}
{% tab title="JavaScript" %}
```javascript
const main = async () => {
  const response = await fetch('https://api.aimlapi.com/v2/generate/video/luma-ai/generation', {
    method: 'POST',
    headers: {
      Authorization: 'Bearer my_key',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      prompt: 'A jellyfish in the ocean',
      aspect_ratio: '19:9',
      keyframes: {
        frame0: {
          type: 'generation',
          id: '0f3ea4aa-10e7-4dae-af0b-263ab4ac45f9',
        },
      },
    }),
  }).then((res) => res.json());

  console.log('Generation:', response);
};

main();

```
{% endtab %}

{% tab title="Python" %}
```python
import requests


def main()
  url = "https://api.aimlapi.com/v2/generate/video/luma-ai/generation"
  payload = {
    "prompt": "Flying jellyfish",
    "aspect_ratio": "16:9",
    "keyframes": {
      "frame0": {
        "type": "generation",
        "id": "0f3ea4aa-10e7-4dae-af0b-263ab4ac45f9"
      }
    }
  }
  headers = {
    "Authorization": "Bearer my_key",
    "Content-Type": "application/json"
  }
  
  response = requests.post(url, json=payload, headers=headers)
  print("Generation:",  response.json())
  
if __name__ == "__main__":
    main()
```
{% endtab %}
{% endtabs %}





================================================
File: docs/api-overview/video-models/luma-ai-v2/fetch-generations.md
================================================
---
icon: code
---

# Fetch Generations

## Overview

After sending a request for video generation, this task is added to the queue. Based on the service's load, the generation can be completed in seconds or take a bit more. Here are the API details to wait for a certain video generation state.

### Edge Cases

Each state has its own priority, described below:

* `queued` -> `dreaming` -> `completed` -> `failed`

If the video state reaches any higher-priority state than you requested, then the result is immediately returned. For example, if you are waiting for the `completed` state and your request fails (reaching the `failed` state), then the result is immediately returned with the current error state.

If video generation takes too long, it can reach a timeout of 30 seconds. In such cases, the result returns with the current actual state. This polling allows you to request it again and wait for the needed state.

{% hint style="info" %}
You cannot wait for an `failed` state.
{% endhint %}

## Example

{% hint style="info" %}
Ensure you replace `"my_key"` with your actual API key before running the code.
{% endhint %}

### Fetch Single Generation

For example, if you are waiting for video dreaming (when the video is popped from the queue and generation is in processing), then you can send the following request:

{% tabs %}
{% tab title="JavaScript" %}
```javascript
const main = async () => {
  const url = new URL('https://api.aimlapi.com/v2/generate/video/luma-ai/generation');
  url.searchParams.set('generation_id', '755f9bbb-d99b-4880-992b-f05244ddba61');
  url.searchParams.set('state', 'dreaming');

  const data = await fetch(url, {
    method: 'GET',
    headers: {
      Authorization: 'Bearer my_key',
      'Content-Type': 'application/json',
    },
  }).then((res) => res.json());

  console.log('Generation:', data);
};

main();

```
{% endtab %}

{% tab title="Python" %}
```python
import requests


def main():
    response = requests.get(
        "https://api.aimlapi.com/v2/generate/video/luma-ai/generation",
        params={
            "generation_id": "755f9bbb-d99b-4880-992b-f05244ddba61",
            "status": "dreaming"
        },
        headers={
            "Authorization": "Bearer my_key",
            "Content-Type": "application/json",
        },
    )

    response.raise_for_status()
    data = response.json()
    print("Generation:", data)


if __name__ == "__main__":
    main()
```
{% endtab %}
{% endtabs %}

If you are waiting for a video to be fully generated, you can wait for the `completed` state in the same way as described above.

### Fetch Multiple Generations

Instead of using the `generation_id` parameter, you will pass `generation_ids`, which can be an array of IDs. This parameter can also accept IDs separated by commas.

{% tabs %}
{% tab title="JavaScript" %}
```javascript
const main = async () => {
  const url = new URL('https://api.aimlapi.com/v2/generate/video/luma-ai/generations');
  url.searchParams.set('generation_ids[]', '755f9bbb-d99b-4880-992b-f05244ddba61');
  url.searchParams.set('state', 'dreaming');

  const data = await fetch(url, {
    method: 'GET',
    headers: {
      Authorization: 'Bearer my_key',
      'Content-Type': 'application/json',
    },
  }).then((res) => res.json());

  console.log('Generation:', data);
};

main();

```
{% endtab %}

{% tab title="Python" %}
```python
import requests


def main():

    response = requests.get(
        "https://api.aimlapi.com/v2/generate/video/luma-ai/generations",
        params={
            "generation_ids[]": "755f9bbb-d99b-4880-992b-f05244ddba61",
            "status": "streaming",
        },
        headers={
            "Authorization": "Bearer my_key",
            "Content-Type": "application/json",
        },
    )

    response.raise_for_status()
    data = response.json()
    print("Generation:", data)


if __name__ == "__main__":
    main()

```
{% endtab %}
{% endtabs %}


================================================
File: docs/api-overview/video-models/luma-ai-v2/generate-video.md
================================================
---
icon: code
---

# Generate Video

## Overview

You can generate a video using the AI/ML API. In the basic setup, you need only a prompt and the aspect ratio of the desired result.

### Aspect Ratio

* **Type:** String
* **Allowed Values:** `1:1`, `16:9`, `9:16`, `4:3`, `3:4`, `21:9`, `9:21`

### Prompt

* **Type:** String
* **Character Limit:** 2048 characters

### Loop

* **Type:** Boolean
* **Description:** Controls if the generated video will be looped.
* **Default Value:** `false`

### Keywords

For instructions on how to use keywords, please refer to the [extend-video.md](extend-video.md "mention") article.

## Example

{% hint style="info" %}
Ensure you replace `"my_key"` with your actual API key before running the code.
{% endhint %}

{% tabs %}
{% tab title="JavaScript" %}
```javascript
const main = async () => {
  const response = await fetch('https://api.aimlapi.com/v2/generate/video/luma-ai/generation', {
    method: 'POST',
    headers: {
      Authorization: 'Bearer my_key',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      prompt: 'A jellyfish in the ocean',
      aspect_ratio: '19:9',
      loop: false
    }),
  }).then((res) => res.json());

  console.log('Generation:', response);
};

main();
```
{% endtab %}

{% tab title="Python" %}
```python
import requests


def main():
  url = "https://api.aimlapi.com/v2/generate/video/luma-ai/generation"
  payload = {
    "prompt": "Flying jellyfish",
    "aspect_ratio": "16:9"
  }
  headers = {
    "Authorization": "Bearer my_key",
    "Content-Type": "application/json"
  }
  
  response = requests.post(url, json=payload, headers=headers)
  print("Generation:",  response.json())
  
if __name__ == "__main__":
    main()
```
{% endtab %}
{% endtabs %}



================================================
File: docs/api-overview/video-models/luma-ai-v2/overview.md
================================================
---
icon: book-open
---

# Overview

## Overview

The Luma AI Dream Machine API allows developers to generate, retrieve, and extend AI-generated content using a variety of inputs. This API is particularly useful for creative applications, such as generating visual content from text prompts.

## Authentication

All API requests require a Bearer token for authentication. Include your token in the `Authorization` header of each request.

## Pricing

Each generation costs 500 000 AI/ML Tokens


================================================
File: docs/api-overview/video-models/minimax-video/README.md
================================================
# MiniMax Video

## Overview

Our API allows developers to generate, retrieve, and extend AI-generated content using a variety of inputs. This API is particularly useful for creative applications, such as generating video content using a text prompt as an instruction and an image as the first frame.

## Step-by-Step Guide

### Authentication

All API requests require a Bearer token for authentication. Include your token in the `Authorization` header of each request.

### Calling API

There are two APIs: the task of creating video generation and querying the status of video generation. The steps are as follows:&#x20;

1. Call the [Generate Video](generate-video.md) API. It creates a video generation task and returned you `generation_id`.
2. Call the [Fetch Generation](fetch-generation.md) API. It queries the video generation status based on your `generation_id`.
3. If your generation was successful, you can use the URL returned by the query interface to view and download the results through the File API.

{% content-ref url="generate-video.md" %}
[generate-video.md](generate-video.md)
{% endcontent-ref %}

{% content-ref url="fetch-generation.md" %}
[fetch-generation.md](fetch-generation.md)
{% endcontent-ref %}


================================================
File: docs/api-overview/video-models/minimax-video/fetch-generation.md
================================================
---
icon: code
---

# Fetch Generation

## Overview

After sending a request for video generation, this task is added to the queue. Based on the service's load, the generation can be completed in seconds or take a bit more. Here are the API to fetch the generations.

{% swagger src="https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27" path="/v2/generate/video/minimax/generation" method="get" %}
[https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27](https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27)
{% endswagger %}

## Example: Fetch Single Generation

You can send the following request:

{% hint style="warning" %}
Ensure you replace `"my_key"` with your actual API key before running the code.
{% endhint %}

{% tabs %}
{% tab title="JavaScript" %}
```javascript
const main = async () => {
  const params = new URLSearchParams({
    generation_id: "<YOUR_GENERATION_ID>"
  });
  const response = await fetch(`https://api.aimlapi.com/v2/generate/video/minimax/generation?${params.toString()}`, {
    method: 'GET',
    headers: {
      Authorization: 'Bearer <YOUR_API_KEY>',
      'Content-Type': 'application/json',
    },
  }).then((res) => res.json());

  console.log('Generation:', response);
};

main()
```
{% endtab %}

{% tab title="Python" %}
```python
import requests


def main():
    url = "https://api.aimlapi.com/v2/generate/video/minimax/generation"
    params = {
        "generation_id": "<YOUR_GENERATION_ID>",
    }
    headers = {"Authorization": "Bearer <YOUR_API_KEY>", "Content-Type": "application/json"}

    response = requests.get(url, params=params, headers=headers)
    print("Generation:", response.json())


if __name__ == "__main__":
    main()
```
{% endtab %}
{% endtabs %}


================================================
File: docs/api-overview/video-models/minimax-video/generate-video.md
================================================
---
icon: code
---

# Generate Video

## Overview

You can generate a video using the AI/ML API. In the basic setup, you need a prompt and an image URL.

{% swagger src="https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27" path="/v2/generate/video/minimax/generation" method="post" %}
[https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27](https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27)
{% endswagger %}

## Example: Create video based on first frame and prompt

{% hint style="warning" %}
Ensure you replace `"my_key"` with your actual API key before running the code.
{% endhint %}

{% tabs %}
{% tab title="JavaScript" %}
```javascript
const main = async () => {
  const response = await fetch('https://api.aimlapi.com/v2/generate/video/minimax/generation', {
    method: 'POST',
    headers: {
      Authorization: 'Bearer <YOUR_API_KEY>',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: 'video-01',
      prompt: 'Mona Lisa puts on glasses with her hands.',
      first_frame_image: 'https://s2-111386.kwimgs.com/bs2/mmu-aiplatform-temp/kling/20240620/1.jpeg',
    }),
  }).then((res) => res.json());

  console.log('Generation:', response);
};

main()
```
{% endtab %}

{% tab title="Python" %}
```python
import requests


def main():
    url = "https://api.aimlapi.com/v2/generate/video/minimax/generation"
    payload = {
        "model": "video-01",
        "prompt": "Mona Lisa puts on glasses with her hands.",
        "first_frame_image": "https://s2-111386.kwimgs.com/bs2/mmu-aiplatform-temp/kling/20240620/1.jpeg",
    }
    headers = {"Authorization": "Bearer <YOUR_API_KEY>", "Content-Type": "application/json"}

    response = requests.post(url, json=payload, headers=headers)
    print("Generation:", response.json())


if __name__ == "__main__":
    main()
```
{% endtab %}
{% endtabs %}


================================================
File: docs/api-overview/video-models/runway/README.md
================================================
---
icon: satellite-dish
---

# Runway

## Overview

The Runway API allows developers to generate, retrieve, and extend AI-generated content using a variety of inputs. This API is particularly useful for creative applications, such as generating visual content from text prompts.

{% content-ref url="overview.md" %}
[overview.md](overview.md)
{% endcontent-ref %}

{% content-ref url="generate-video.md" %}
[generate-video.md](generate-video.md)
{% endcontent-ref %}

{% content-ref url="generate-video-1.md" %}
[generate-video-1.md](generate-video-1.md)
{% endcontent-ref %}


================================================
File: docs/api-overview/video-models/runway/generate-video-1.md
================================================
---
icon: code
---

# Fetch Generation

## Overview

After sending a request for video generation, this task is added to the queue. Based on the service's load, the generation can be completed in seconds or take a bit more. Here are the API to fetch the generations.

## Example

{% hint style="info" %}
Ensure you replace `"my_key"` with your actual API key before running the code.
{% endhint %}

### Fetch Single Generation

You can send the following request:

{% tabs %}
{% tab title="JavaScript" %}
```javascript
const main = async () => {
  const url = new URL('https://api.aimlapi.com/v2/generate/video/runway/generation');
  url.searchParams.set('generation_id', 'aa1234aa-a109-4334-9407-f7f6a9306584');

  const data = await fetch(url, {
    method: 'GET',
    headers: {
      Authorization: 'Bearer my_key',
      'Content-Type': 'application/json',
    },
  }).then((res) => res.json());

  console.log('Generation:', data);
};

main();

```
{% endtab %}

{% tab title="Python" %}
```python
import requests


def main():
    response = requests.get(
        "https://api.aimlapi.com/v2/generate/video/runway/generation",
        params={
            "generation_id": "aa1234aa-a109-4334-9407-f7f6a9306584"
        },
        headers={
            "Authorization": "Bearer my_key",
            "Content-Type": "application/json",
        },
    )

    response.raise_for_status()
    data = response.json()
    print("Generation:", data)


if __name__ == "__main__":
    main()
```
{% endtab %}
{% endtabs %}


================================================
File: docs/api-overview/video-models/runway/generate-video.md
================================================
---
icon: code
---

# Generate Video

## Overview

You can generate a video using the AI/ML API. In the basic setup, you need only a prompt and the aspect ratio of the desired result.

### Ratio

* **Type:** String
* **Key:** `ratio`
* **Allowed Values:**  `16:9`, `9:16`

### Prompt

* **Type:** String
* **Key:** `prompt`
* **Character Limit:** 512 characters

### Image URL

* **Type:** URL
* **Key:** `image_url`
* **Description:** A HTTPS URL or data URI containing an encoded image to be used as the first frame of the generated

### Last Image URL

* **Type:** URL
* **Key:** `last_image_url`
* **Description:** A HTTPS URL or data URI containing an encoded image to be used as the last frame of the generated

### Duration

* **Type:** Integer
* **Key:** `duration`
* **Allowed Values:**  `5`, `10`

### Seed

* **Type:** Integer
* **Key:** `seed`
* **Description:**  If unspecified, a random number is chosen. Varying the seed integer is a way to get different results for the same other request parameters. Using the same seed integer for an identical request will produce similar results

### Watermark

* **Type:** Boolean
* **Key:** `watermark`
* **Description:**  A boolean indicating whether or not the output video will contain a Runway watermark

## Example

{% hint style="info" %}
Ensure you replace `"my_key"` with your actual API key before running the code.
{% endhint %}

{% tabs %}
{% tab title="JavaScript" %}
```javascript
const main = async () => {
  const response = await fetch('https://api.aimlapi.com/v2/generate/video/runway/generation', {
    method: 'POST',
    headers: {
      Authorization: 'Bearer my_key',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: 'gen3a_turbo',
      prompt: 'A jellyfish in the ocean',
      ratio: '16:9',
      image_url: 'https://upload.wikimedia.org/wikipedia/commons/3/35/Maldivesfish2.jpg',
    }),
  }).then((res) => res.json());

  console.log('Generation:', response);
};

main()
```
{% endtab %}
{% tab title="Python" %}
```python
import requests


def main():
    url = "https://api.aimlapi.com/v2/generate/video/runway/generation"
    payload = {
        "model": "gen3a_turbo",
        "prompt": "A jellyfish in the ocean",
        "ratio": "16:9",
        "image_url": "https://upload.wikimedia.org/wikipedia/commons/3/35/Maldivesfish2.jpg",
    }
    headers = {"Authorization": "Bearer my_key", "Content-Type": "application/json"}

    response = requests.post(url, json=payload, headers=headers)
    print("Generation:", response.json())

if __name__ == "__main__":
    main()

```
{% endtab %}
{% endtabs %}


================================================
File: docs/api-overview/video-models/runway/overview.md
================================================
---
icon: book-open
---

## Overview

The Runway API allows developers to generate, retrieve, and extend AI-generated content using a variety of inputs. This API is particularly useful for creative applications, such as generating visual content from text prompts.

## Authentication

All API requests require a Bearer token for authentication. Include your token in the `Authorization` header of each request.

## Pricing

Each generation costs 525 000 AI/ML Tokens


================================================
File: docs/api-overview/vision-models/README.md
================================================
---
icon: eye
description: Overview of the capabilities of AIML API vision models.
layout:
  title:
    visible: true
  description:
    visible: false
  tableOfContents:
    visible: true
  outline:
    visible: true
  pagination:
    visible: true
---

# Vision Models

## Overview

Our API enables you to use machine learning models for tasks that require visual capabilities. These models are referred to as _vision models_. Within our API, we offer two categories of vision models: **OCR** and **OFR**.

### OCR: Optical Character Recognition

With OCR technology, you can analyze any document and extract text as well as other characters and symbols. This allows you to detect:

* Text
* Paragraph blocks
* Handwriting
* Text inside PDF/TIFF files

{% content-ref url="ocr-optical-character-recognition.md" %}
[ocr-optical-character-recognition.md](ocr-optical-character-recognition.md)
{% endcontent-ref %}

### OFR: Optical Feature Recognition

In contrast to OCR, OFR allows you to analyze not just documents but also images. You can filter exactly what you want to find in the image by the features they include:

* Crop hints
* Faces
* Image properties
* Labels
* Landmarks
* Logos
* Multiple objects
* Explicit content
* Web entities and pages
* And many more

{% content-ref url="ofr-optical-feature-recognition.md" %}
[ofr-optical-feature-recognition.md](ofr-optical-feature-recognition.md)
{% endcontent-ref %}


================================================
File: docs/api-overview/vision-models/image-analysis.md
================================================
---
icon: book-open
---

# Image Analysis

{% content-ref url="../text-models-llm/function-calling/image-to-text-vision.md" %}
[image-to-text-vision.md](../text-models-llm/function-calling/image-to-text-vision.md)
{% endcontent-ref %}



================================================
File: docs/api-overview/vision-models/ocr-optical-character-recognition.md
================================================
---
icon: brackets-curly
---

# OCR: Optical Character Recognition

Our API provides a feature to extract characters from images.

{% swagger src="https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27" path="/v1/ocr" method="post" %}
[https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27](https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27)
{% endswagger %}


================================================
File: docs/api-overview/vision-models/ofr-optical-feature-recognition.md
================================================
---
icon: brackets-curly
---

# OFR: Optical Feature Recognition

Our API provides a feature to extract visual features from images.

{% swagger src="https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27" path="/vision" method="post" %}
[https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27](https://api-staging.aimlapi.com/docs-public-yaml?key=3b878a3c71a785f13366e9be96bacb27)
{% endswagger %}


================================================
File: docs/faq/can-i-use-api-in-nodejs.md
================================================
---
icon: circle-question
description: >-
  Quick guide about how to configure NodeJS on different operational systems to
  use AI/ML API with it.
layout:
  title:
    visible: true
  description:
    visible: false
  tableOfContents:
    visible: true
  outline:
    visible: true
  pagination:
    visible: true
---

# Can I use API in NodeJS?

Yes, definitely! Here is a quick guide on how to start your adventure with AI/ML API in NodeJS.

## Installation

### Is it already installed?

Before using an API in NodeJS, you need to ensure that NodeJS is installed on your system. The simplest way is to run a terminal and execute the following command:

```bash
node --version
```

If this command prints a NodeJS version, then you can proceed to the [example article](../quickstart/setting-up.md#example-in-node.js). If not, you need to install NodeJS on your system. The installation steps depend on your operating system, but here are some quick instructions to get you started:

### On Windows / Mac

Install the NodeJS package from the [official distribution site](https://nodejs.org/en). It is preferable to choose the LTS version, but it all depends on your project.

### On Linux

The installation process depends on your distribution. For example, on Ubuntu, you can use the following command to install version 20:

```bash
curl -fsSL https://deb.nodesource.com/setup_20.x | sudo -E bash -
```

## Using

Test your installation and you can proceed with the tutorial on the [quickstart](broken-reference) page.



================================================
File: docs/faq/can-i-use-api-in-python.md
================================================
---
icon: circle-question
description: >-
  Quick guide about how to configure Python on different operational systems to
  use AI/ML API with it.
layout:
  title:
    visible: true
  description:
    visible: false
  tableOfContents:
    visible: true
  outline:
    visible: true
  pagination:
    visible: true
---

# Can I use API in Python?

Of course you can! Here is a quick guide on how to configure your environment and use our API.

## Installation

Our API is just an interface, which means you need to use it within some application or code. When you choose Python, you first need to install it on your system.

### Is it already installed?

Depending on your operating system and version, it might already be installed out-of-the-box. To test it, you need to open the [terminal ](../glossary/concepts.md#terminal)and type one of the following commands:

```bash
python3 --version
python --version
py --version
```

If any of these commands print a Python version higher than 3.8, as shown below, then Python is properly installed:

```bash
Python 3.10.6
```

If your result is different (for example, if it just prints "Python" without a version or if on Windows 11 the Microsoft Store opens), then it isn't installed.

### On Windows

You can install Python from the [Microsoft Store marketplace](https://apps.microsoft.com/detail/9pjpw5ldxlz5?hl=en-US\&gl=US) if you are using a newer version of Windows 11, or you can follow the [official Python distribution site](https://www.python.org/downloads/) and install it as a usual executable from there. You can safely install version 3.10 as most modern modules support it, or the latest version if you wish.

### On Mac

There are several good articles that describe the installation process. You can look at one [here](https://docs.python-guide.org/starting/install3/osx/) or search the Internet for more options.

### On Linux

The installation process varies depending on your Linux distribution. Here is an example for Ubuntu (make sure that Python isn't installed, as described in the beginning):

```bash
sudo apt update
sudo apt install -y python3-venv
```

## Using

Test your installation and you can proceed with the tutorial on the [quickstart](broken-reference) page.


================================================
File: docs/faq/free-tier.md
================================================
---
icon: circle-question
description: FAQ section about AI/ML API Free Tier option.
layout:
  title:
    visible: true
  description:
    visible: false
  tableOfContents:
    visible: true
  outline:
    visible: true
  pagination:
    visible: true
---

# How to use the Free Tier?

## About

The Free Tier allows users to use the service with certain limits at no cost. This helps them test the product and see if it works for their goals before making any payments.

## Which models are included

### **In AI Playground**

Some models can be tested in the [AI Playground](https://aimlapi.com/app/) available on our official website. There, you will find a list of models ready to be launched.&#x20;

<figure><img src="../.gitbook/assets/playground-model-list.png" alt=""><figcaption></figcaption></figure>

The following rules apply:

* Regular models come with 50 free requests per day.
* You can execute only one request at a time.
* [Pro models](pro-models.md) have a limit of 10,000 AI/ML API tokens per day.

### **Via API**

Using our API on the Free Tier, you can access:

* [Chat completion text models](../api-overview/text-models-llm/chat-completion.md),
* [Embedding models](../api-overview/text-models-llm/embeddings.md),
* [Image models](../api-overview/image-models/).

The following Free Tier rules apply:

* You are allowed 10 free requests per hour.
* When using Chat Completion text models, the maximum output is limited to 512 tokens. Image attachments in messages are not supported.
* You can generate up to 10 images per hour.&#x20;
* Using the following image models, you can only 1 image generated at a time:

<details>

<summary>Model list</summary>

* flux/schnell&#x20;
* flux-pro&#x20;
* flux-pro/v1.1&#x20;
* flux-pro/v1.1-ultra
* flux/dev
* flux/dev/image-to-image
* flux-realism
* stable-diffusion-v3-medium&#x20;
* stable-diffusion-v35-large
* recraft-v3

</details>

* For the following models generated images are limited to a resolution of 512x512 pixels:

<details>

<summary>Model list</summary>

* dall-e-2
* dall-e-3
* stabilityai/stable-diffusion-xl-base-1.0

</details>

* If image generation requires more than 512 AI/ML API tokens, the request will not be processed.


================================================
File: docs/faq/is-api-down-or-it-just-me.md
================================================
---
hidden: true
---

# Is API down or it just me?

You can check every API status here


================================================
File: docs/faq/my-requests-are-cropped.md
================================================
---
icon: circle-question
description: FAQ section about max_tokens parameter and its possible impact on a request.
layout:
  title:
    visible: true
  description:
    visible: false
  tableOfContents:
    visible: true
  outline:
    visible: true
  pagination:
    visible: true
---

# Are my requests cropped?

AI/ML API has a parameter called `max_tokens`. Usually, this parameter can be crucial if your requests are large and can lead to text cropping. This parameter controls the maximum number of input and output tokens combined that your model will use inside your request and can save your tokens if the generation is larger than you expect. Try to adjust it and see the result.

You can read more about `max_tokens` and other parameters [here](../api-overview/text-models-llm/parameters.md).


================================================
File: docs/faq/openai-sdk-doesnt-work.md
================================================
---
icon: circle-question
description: FAQ section with several possible reasons of the problem with OpenAI SDK.
layout:
  title:
    visible: true
  description:
    visible: false
  tableOfContents:
    visible: true
  outline:
    visible: true
  pagination:
    visible: true
---

# OpenAI SDK doesn't work?

## Check OpenAI SDK installation

Depending on your environment, the steps may differ. For Python and NodeJS, you can proceed to the setup article and check if all steps are completed correctly.

## Check base URL

OpenAI SDK is a configurable package. To use it with our AI/ML API server, you need to pass a parameter called `base URL`. Depending on your environment, this process can differ, but here is an example for Python and NodeJS:

{% tabs %}
{% tab title="Python" %}
```python
from openai import OpenAI

api_key = "my_key"
base_url = "https://api.aimlapi.com/v1"

api = OpenAI(api_key=api_key, base_url=base_url)
```
{% endtab %}

{% tab title="JavaScript" %}
```javascript
const { OpenAI } = require("openai");

const apiKey = "my_key";
const baseURL = "https://api.aimlapi.com/v1";

const api = new OpenAI({
  apiKey,
  baseURL,
});
```
{% endtab %}
{% endtabs %}

## Check API key

When you use the AI/ML API, you should use our API key. This API key is listed on your account page, and you should keep it safe. When sending a request to the API, you need to ensure that you have included the API key in your query.

Look at the example with the base URL above and check if you passed the correct API key to the `api_key` or `apiKey` parameters.


================================================
File: docs/faq/pro-models.md
================================================
---
icon: circle-question
description: FAQ section about Pro Models category in AI/ML API.
layout:
  title:
    visible: true
  description:
    visible: false
  tableOfContents:
    visible: true
  outline:
    visible: true
  pagination:
    visible: true
---

# What are the Pro Models?

Our AI/ML API provides access to many models. Some of these models are very complex and require significant resources to execute. Additionally, these models are highly sophisticated and allow you to create complex prompts with intricate results. These models are referred to as Pro Models in our API.


================================================
File: docs/glossary/concepts.md
================================================
---
icon: book-open
description: A glossary page with some important API terms explained.
layout:
  title:
    visible: true
  description:
    visible: false
  tableOfContents:
    visible: true
  outline:
    visible: true
  pagination:
    visible: true
---

# Concepts

## API

API stands for _Application Programming Interface_. In the context of AI/ML, an API serves as a "handle" that enables you to integrate and utilize any Machine Learning model within your application. Our API supports communication via HTTP requests and is fully backward-compatible with OpenAI’s API. This means you can refer to OpenAI’s documentation for making calls to our API. However, be sure to change the base URL to direct your requests to our servers and select the desired model from our offerings.

## Base URL

The Base URL is the first part of the URL (including the protocol, domain, and pathname) that determines the server responsible for handling your request. It’s crucial to configure the correct Base URL in your application, especially if you are using SDKs from OpenAI, Azure, or other providers. By default, these SDKs are set to point to their servers, which are not compatible with our API keys and do not support many of the models we offer.

Our base URL also supports versioning, so you can use the following as well:

* `https://api.aimlapi.com`
* `https://api.aimlapi.com/v1`

Usually, you pass the base URL as the same field inside the SDK constructor. In some cases, you can set the environment variable `BASE_URL`, and it will work. If you want to use the OpenAI SDK, then follow the [setting up article ](../quickstart/setting-up.md)and take a closer look at how to use it with the AI/ML API.

## API Key

You can find your API key on the [account page](https://aimlapi.com/app/keys). An _API Key_ is a credential that grants you access to our API from within your code. It is a sensitive string of characters that should be kept confidential. Do not share your API key with anyone else, as it could be misused without your knowledge.

## Terminal

If you are not a developer or are using modern systems, you might be familiar with it only as a "black window for hackers." However, the terminal is a very old and useful way to communicate with a computer. The terminal is an app inside your operating system that allows you to run commands by typing strings associated with some program. Depending on the operating system, you can run the terminal in many ways. Here are basic ways that usually work:

* **On Windows:** Press the combination `Win + R` and type `cmd`.
* **On Mac:** Press `Command + Space`, search for _Terminal_, then hit `Enter`.
* **On Linux:** You are probably already familiar with it. On Ubuntu with GUI, for example, you can type `Ctrl + F`, search for _Terminal_, then hit `Enter`.


================================================
File: docs/integrations/about.md
================================================
---
description: About third-party integrations
layout:
  title:
    visible: true
  description:
    visible: false
  tableOfContents:
    visible: true
  outline:
    visible: true
  pagination:
    visible: true
---

# About

Our API endpoint can be integrated with popular AI workflow platforms and tools, allowing their users to access our models through these environments.


================================================
File: docs/integrations/langflow.md
================================================
---
description: >-
  Short description of third-party integration with AI/ML API (Langflow) and how
  to work with it.
layout:
  title:
    visible: true
  description:
    visible: false
  tableOfContents:
    visible: true
  outline:
    visible: true
  pagination:
    visible: true
---

# Langflow

## About

[Langflow](https://www.langflow.org/) is a new visual framework for building multi-agent and RAG applications. It is open-source, Python-powered, fully customizable, and LLM and vector store agnostic.

Its intuitive interface allows for easy manipulation of AI building blocks, enabling developers to quickly prototype and turn their ideas into powerful, real-world solutions.

## How to Use AIML API via Langflow

A user of the Langflow framework can create a working AI pipeline (a _flow_) by simply adding visual components, connecting their inputs and outputs in the required order, and setting various available parameters in each component.&#x20;

At the center of such a flow is one or more sequential **model components** that generate text using LLMs. Choose **Models > AIML** in the sidebar, then click the "+" button or simply drag and drop the model element onto the work area. After that, connect it to your input and output elements:

<figure><img src="../.gitbook/assets/langflow aimlapi flow.png" alt=""><figcaption><p>The component at the center creates a chat model instance powered by AIML API.</p></figcaption></figure>

### Inputs <a href="#inputs" id="inputs"></a>

{% hint style="info" %}
Only 1-2 key parameters are usually displayed directly on the model component body. To configure most parameters, you need to click on the model element and then click the **Controls** button that appears above the model component.
{% endhint %}

| Name            | Type         | Description                                                                               |
| --------------- | ------------ | ----------------------------------------------------------------------------------------- |
| `max_tokens`    | Integer      | The maximum number of tokens to generate. Set to 0 for unlimited tokens. Range: 0-128000. |
| `model_kwargs`  | Dictionary   | Additional keyword arguments for the model.                                               |
| `model_name`    | String       | The name of the AIML model to use. Options are predefined in `AIML_CHAT_MODELS`.          |
| `aiml_api_base` | String       | The base URL of the AIML API. Defaults to `https://api.aimlapi.com`.                      |
| `api_key`       | SecretString | The AIML API Key to use for the model.                                                    |
| `temperature`   | Float        | Controls randomness in the output. Default: `0.1`.                                        |
| `seed`          | Integer      | Controls reproducibility of the job.                                                      |

### Outputs <a href="#outputs" id="outputs"></a>

| Name    | Type          | Description                                                         |
| ------- | ------------- | ------------------------------------------------------------------- |
| `text`  | String        | Chat model response.                                                |
| `model` | LanguageModel | An instance of ChatOpenAI configured with the specified parameters. |

For further information about the framework, please check the [official Langflow documentation.](https://docs.langflow.org/)


================================================
File: docs/quickstart/setting-up.md
================================================
---
icon: hand-wave
description: >-
  A step-by-step guide to setting up and making a test call to the AI model,
  including generating an API key, configuring the Base URL, and running the
  first request.
layout:
  title:
    visible: true
  description:
    visible: false
  tableOfContents:
    visible: true
  outline:
    visible: true
  pagination:
    visible: true
---

# Setting Up

## G**enerating an API Key**

{% hint style="info" %}
[What is an API Key?](../glossary/concepts.md#api-key)
{% endhint %}

To use the AI/ML API, you need to create an account and generate an API key. Follow these steps:

1. [**Create an Account**](https://aimlapi.com/app/sign-up)**:** Visit the AI/ML API website and create an account.
2. [**Generate an API Key**](https://aimlapi.com/app/keys)**:** After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.

<figure><img src="../.gitbook/assets/image (1).png" alt=""><figcaption><p>Your API key</p></figcaption></figure>

## **Configure Base URL**

{% hint style="info" %}
[What is a base URL?](../glossary/concepts.md#base-url)
{% endhint %}

Depending on your environment and application, you will set the base URL differently. Below is a universal string that you can use to access our API. Copy it or return here later when you are ready with your environment or app.

```
https://api.aimlapi.com
```

The AI/ML API supports both versioned and non-versioned URLs, providing flexibility in your API requests. You can use either of the following formats:

* `https://api.aimlapi.com`
* `https://api.aimlapi.com/v1`

Using versioned URLs can help ensure compatibility with future updates and changes to the API. It is recommended to use versioned URLs for long-term projects to maintain stability.

## Making Your First API Call

Based on your environment, you will call our API differently. Below are two common ways to call our API using two popular programming languages: Python and NodeJS.

{% hint style="info" %}
In the examples below, we use the OpenAI SDK. This is possible due to our compatibility with most OpenAI APIs, but this is just one approach. You can use our API without this SDK with raw HTTP queries.
{% endhint %}

<details>

<summary>Example in Python</summary>

Let's start from very beginning. We assume you already installed Python (with venv), if not, here a [guide for the beginners](../faq/can-i-use-api-in-python.md).

Create a new folder for test project, name it as `aimlapi-welcome` and change to it.

```bash
mkdir ./aimlapi-welcome
cd ./aimlapi-welcome
```

(Optional) If you use IDE then we recommend to open created folder as workspace. On example, in VSCode you can do it with:

```
code .
```

Run a terminal inside created folder and create virtual envorinment with a command

```shell
python3 -m venv ./.venv
```

Activate created virtual environment

```shell
# Linux / Mac
source ./.venv/bin/activate
# Windows
./.venv/bin/Activate.bat
```

Install requirement dependencies. In our case we need only OpenAI SDK

```shell
pip install openai
```

Create new file and name it as `travel.py`

```shell
touch travel.py
```

Paste following content inside this `travel.py` and replace `my_key` with your API key you got on [first step](setting-up.md#generating-an-api-key)

```python
from openai import OpenAI

base_url = "https://api.aimlapi.com/v1"
api_key = "my_key"
system_prompt = "You are a travel agent. Be descriptive and helpful."
user_prompt = "Tell me about San Francisco"

api = OpenAI(api_key=api_key, base_url=base_url)


def main():
    completion = api.chat.completions.create(
        model="mistralai/Mistral-7B-Instruct-v0.2",
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ],
        temperature=0.7,
        max_tokens=256,
    )

    response = completion.choices[0].message.content

    print("User:", user_prompt)
    print("AI:", response)


if __name__ == "__main__":
    main()
```

Run the application

```shell
python3 ./travel.py
```

If you done all correct, you will see following output:

```
User: Tell me about San Francisco
AI:  San Francisco, located in northern California, USA, is a vibrant and culturally rich city known for its iconic landmarks, beautiful vistas, and diverse neighborhoods. It's a popular tourist destination famous for its iconic Golden Gate Bridge, which spans the entrance to the San Francisco Bay, and the iconic Alcatraz Island, home to the infamous federal prison.

The city's famous hills offer stunning views of the bay and the cityscape. Lombard Street, the "crookedest street in the world," is a must-see attraction, with its zigzagging pavement and colorful gardens. Ferry Building Marketplace is a great place to explore local food and artisanal products, and the Pier 39 area is home to sea lions, shops, and restaurants.

San Francisco's diverse neighborhoods each have their unique character. The historic Chinatown is the oldest in North America, while the colorful streets of the Mission District are known for their murals and Latin American culture. The Castro District is famous for its LGBTQ+ community and vibrant nightlife.
```

</details>

<details>

<summary>Example in NodeJS</summary>

As in the example from Python, we start from the very beginning too. We assume you already have Node.js installed. If not, here is a [guide for beginners](../faq/can-i-use-api-in-nodejs.md).

We need to create a new folder for the example project:

```bash
mkdir ./aimlapi-welcome
cd ./aimlapi-welcome
```

(Optional) If you use IDE then we recommend to open created folder as workspace. On example, in VSCode you can do it with:

```shell
code .
```

Now create a project file:

```bash
npm init -y
```

Install the required dependencies:

```bash
npm i openai
```

Create a file with the source code:

```bash
touch ./index.js
```

And paste the following content:

```javascript
const { OpenAI } = require("openai");

const baseURL = "https://api.aimlapi.com/v1";
const apiKey = "my_key";
const systemPrompt = "You are a travel agent. Be descriptive and helpful";
const userPrompt = "Tell me about San Francisco";

const api = new OpenAI({
  apiKey,
  baseURL,
});

const main = async () => {
  const completion = await api.chat.completions.create({
    model: "mistralai/Mistral-7B-Instruct-v0.2",
    messages: [
      {
        role: "system",
        content: systemPrompt,
      },
      {
        role: "user",
        content: userPrompt,
      },
    ],
    temperature: 0.7,
    max_tokens: 256,
  });

  const response = completion.choices[0].message.content;

  console.log("User:", userPrompt);
  console.log("AI:", response);
};

main();
```

You will see a response that looks like this:

```
User: Tell me about San Francisco
AI: San Francisco, located in the northern part of California, USA, is a vibrant and culturally rich city known for its iconic landmarks, beautiful scenery, and diverse neighborhoods.

The city is famous for its iconic Golden Gate Bridge, an engineering marvel and one of the most recognized structures in the world. Spanning the Golden Gate Strait, this red-orange suspension bridge connects San Francisco to Marin County and offers breathtaking views of the San Francisco Bay and the Pacific Ocean.
```

</details>



## Code Explanation

Both examples are written in different programming languages, but despite that, they look very similar. Let's break down the code step by step and see what's going on.

In the examples above, we are using the OpenAI SDK. The OpenAI SDK is a nice module that allows us to use the AI/ML API without dealing with repetitive boilerplate code for handling HTTP requests. Before we can use the OpenAI SDK, it needs to be imported. The import happens in the following places:

{% tabs %}
{% tab title="JavaScript" %}
```javascript
const { OpenAI } = require("openai");
```
{% endtab %}

{% tab title="Python" %}
```python
from openai import OpenAI
```
{% endtab %}
{% endtabs %}

Simple as it is. The next step is to initialize variables that our code will use. The two main ones are: the base URL and the API key. We already discussed them at the beginning of the article.

{% tabs %}
{% tab title="JavaScript" %}
```javascript
const baseURL = "https://api.aimlapi.com/v1";
const apiKey = "my_key";
const systemPrompt = "You are a travel agent. Be descriptive and helpful";
const userPrompt = "Tell me about San Francisco";
```
{% endtab %}

{% tab title="Python" %}
```python
base_url = "https://api.aimlapi.com/v1"
api_key = "my_key"
system_prompt = "You are a travel agent. Be descriptive and helpful."
user_prompt = "Tell me about San Francisco"
```
{% endtab %}
{% endtabs %}

To communicate with LLM models, users use texts. These texts are usually called "Prompts." Inside our code, we have prompts with two roles: the system and the user. The system prompt is designed to be the main source of instruction for LLM generation, while the user prompt is designed to be user input, the subject of the system prompt. Despite that many models can operate differently, this behavior usually applies to chat LLM models, currently one of the most useful and popular ones.

Inside the code, the prompts are called in variables `systemPrompt`, `userPrompt` in JS, and `system_prompt`, `user_prompt` in Python.

Before we use the API, we need to create an instance of the OpenAI SDK class. It allows us to use all their methods. The instance is created with our imported package, and here we forward two main parameters: the base URL and the API key.

{% tabs %}
{% tab title="JavaScript" %}
```javascript
const api = new OpenAI({
  apiKey,
  baseURL,
});
```
{% endtab %}

{% tab title="Python" %}
```python
api = OpenAI(api_key=api_key, base_url=base_url)
```
{% endtab %}
{% endtabs %}

Because of notation, these two parameters are called slightly differently in these different languages (camel case in JS and snake case in Python), but their functionality is the same.

All preparation steps are done. Now we need to write our functionality and create something great. In the examples above, we make the simplest travel agent. Let's break down the steps of how we send a request to the model.

The best practice is to split the code blocks into complete parts with their own logic and not place executable code inside global module code. This rule applies in both languages we discuss. So we create a main function with all our logic. In JS, this function needs to be async, due to Promises and simplicity. In Python, requests run synchronously.

The OpenAI SDK provides us with methods to communicate with chat models. It is placed inside the `chat.completions.create` function. This function accepts multiple parameters but requires only two: `model` and `messages`.&#x20;

`model` is a string, the name of the model that you want to use. For the best results, use a model designed for chat, or you can get unpredictable results if the model is not fine-tuned for that purpose. A list of supported models can be found here.&#x20;

`messages` is an array of objects with a `content` field as prompt and a `role` string that can be one of `system`, `user`, `tool`, `assistant`. With the role, the model can understand what to do with this prompt: Is this an instruction? Is this a user message? Is this an example of how to answer? Is this the result of code execution? The tool role is used for more complex behavior and will be discussed in another article.

In our example, we also use `max_tokens` and `temperature`. How to use these parameters is discussed in the [parameters article](../api-overview/text-models-llm/parameters.md).&#x20;

With that knowledge, we can now send our request like the following:

{% tabs %}
{% tab title="JavaScript" %}
```javascript
const completion = await api.chat.completions.create({
  model: "mistralai/Mistral-7B-Instruct-v0.2",
  messages: [
    {
      role: "system",
      content: systemPrompt,
    },
    {
      role: "user",
      content: userPrompt,
    },
  ],
  temperature: 0.7,
  max_tokens: 256,
});
```
{% endtab %}

{% tab title="Python" %}
```python
completion = api.chat.completions.create(
    model="mistralai/Mistral-7B-Instruct-v0.2",
    messages=[
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt},
    ],
    temperature=0.7,
    max_tokens=256,
)
```
{% endtab %}
{% endtabs %}

The response from the function `chat.completions.create` contains a completion. Completion is a fundamental part of LLM models' logic. Every LLM model is some sort of word autocomplete engine, trained by huge amounts of data. The chat models are designed to autocomplete large chunks of messages with prompts and certain roles, but other models can have their own custom logic without even roles.

Inside this completion, we are interested in the text of the generation. We can get it by getting the result from the completion variable:

{% tabs %}
{% tab title="JavaScript" %}
```javascript
const response = completion.choices[0].message.content;
```
{% endtab %}

{% tab title="Python" %}
```python
response = completion.choices[0].message.content
```
{% endtab %}
{% endtabs %}

In certain cases, completion can have multiple results. These results are called choices. Every choice has a message, the product of generation. The string content is placed inside the `content` variable, which we placed inside our response variable above.

In the next steps, we can finally see the results. In both examples, we print the user prompt and response like it was a conversation:

{% tabs %}
{% tab title="JavaScript" %}
```javascript
console.log("User:", userPrompt);
console.log("AI:", response);
```
{% endtab %}

{% tab title="Python" %}
```python
print("User:", user_prompt)
print("AI:", response)
```
{% endtab %}
{% endtabs %}

Voila! Using AI/ML API models is the simplest and most productive way to get into the world of Machine Learning and Artificial Intelligence.

## Future Steps

* [Know more about OpenAI SDK inside AI/ML API](supported-sdks.md)



================================================
File: docs/quickstart/supported-sdks.md
================================================
---
icon: hammer-brush
description: >-
  A description of the software development kits (SDKs) that can be used to
  interact with the AIML API.
layout:
  title:
    visible: true
  description:
    visible: false
  tableOfContents:
    visible: true
  outline:
    visible: true
  pagination:
    visible: true
---

# Supported SDKs

## OpenAI

In the [setting up article](setting-up.md), we showed an example of how to use the OpenAI SDK with the AI/ML API. We configured the environment from the very beginning and executed our request to the AI/ML API.

We fully support the OpenAI API structure, and you can seamlessly use the features that the OpenAI SDK provides out-of-the-box, including:

* Streaming
* Completions
* Chat Completions
* Audio
* Beta Assistants
* Beta Threads
* Embeddings
* Image Generation
* Uploads

This support provides easy integration into systems already using OpenAI's standards. For example, you can integrate our API into any product that supports LLM models by updating only two things in the configuration: the base URL and the API key.

{% hint style="info" %}
[How do I configure the base URL and API key?](../faq/openai-sdk-doesnt-work.md)
{% endhint %}

## REST API

Because we support the OpenAI API structure, our API can be used with the same endpoints as OpenAI. You can call them from any environment.

### Authorization

AI/ML API authorization is based on a Bearer token. You need to include it in the `Authorization` HTTP header within the request, on example:

```http
Authorization: Bearer my_key
```

### Request Example

When your token is ready you can call our API through HTTP.

{% tabs %}
{% tab title="JavaScript" %}
```javascript
fetch("https://api.aimlapi.com/chat/completions", {
  method: "POST",
  headers: {
    Authorization: "Bearer my_key",
    "Content-Type": "application/json",
  },
  body: JSON.stringify({
    model: "gpt-4o",
    messages: [
      {
        role: "user",
        content: "What kind of model are you?",
      },
    ],
    max_tokens: 512,
    stream: false,
  }),
})
  .then((res) => res.json())
  .then(console.log);
```
{% endtab %}

{% tab title="Python" %}
```python
import requests
import json

response = requests.post(
    url="https://api.aimlapi.com/chat/completions",
    headers={
        "Authorization": "Bearer abc_api_key_xyz",
        "Content-Type": "application/json",
    },
    data=json.dumps(
        {
            "model": "gpt-4o",
            "messages": [
                {
                    "role": "user",
                    "content": "What kind of model are you?",
                },
            ],
            "max_tokens": 512,
            "stream": False,
        }
    ),
)

response.raise_for_status()
print(response.json())
```
{% endtab %}

{% tab title="Shell" %}
```ruby
curl --request POST \
  --url https://api.aimlapi.com/chat/completions \
  --header 'Authorization: Bearer abc_api_key_xyz' \
  --header 'Content-Type: application/json' \
  --data '{
    "model": "gpt-4o",
    "messages": [
        {
            "role": "user",
            "content": "What kind of model are you?"
        }
    ],
    "max_tokens": 512,
    "stream": false
}'
```
{% endtab %}
{% endtabs %}

## Next Steps

* [Learn how to use text models](../api-overview/text-models-llm/)


================================================
File: docs/recipes/models-comparsion.md
================================================
---
description: Use the API to list all available models and compare their responses.
icon: code
---

# Models comparsion

The API used in this example is listed [here](../api-overview/model-database/).

### Example

For example, you can send requests to two random models and compare the results:

```javascript
const { OpenAI } = require('openai');
const { Axios } = require('axios');
const main = async () => {
  const BASE_URL = '{{baseUrl}}';
  const API_TOKEN = '{{token}}';
  const axios = new Axios({
    headers: { Authorization: `Bearer ${API_TOKEN}` },
    baseURL: BASE_URL,
  });
  const openai = new OpenAI({ baseURL: BASE_URL, apiKey: API_TOKEN });
  const vendorByModel = await axios.get('/models').then((res) => JSON.parse(res.data));
  const models = Object.keys(vendorByModel);
  const shuffledModels = [...models].sort(() => Math.round(Math.random()));
  const selectedModels = shuffledModels.slice(0, 2);
  const systemPrompt = `You are an AI assistant that only responds with jokes.`;
  const userPrompt = `Why is the sky blue?`;
  for (const model of selectedModels) {
    const completion = await openai.chat.completions.create({
      messages: [
        { role: 'system', content: systemPrompt },
        { role: 'user', content: userPrompt },
      ],
      model,
    });
    const message = completion.choices[0].message.content;
    console.log(`--- ${model} ---`);
    console.log(`USER: ${userPrompt}`);
    console.log(`AI  : ${message}`);
  }
};
main();

```

Will return something like this:

```
--- zero-one-ai/Yi-34B-Chat ---
USER: Why is the sky blue?
AI  : Why is the sky blue? Because it's full of blueberries!
--- allenai/OLMo-7B-Instruct ---
USER: Why is the sky blue?
AI  : Because the white sun beams enter the blue Earth's atmosphere and get dispersed, resulting in the beautiful color we call "sky blue." It's like looking at paint being blown on a canvas by the wind! Just a joke, but the real answer is physics. 😎

```


================================================
File: docs/recipes/using-assistants-and-threads.md
================================================
---
icon: code
description: Example of using the threads API.
---

# Using Assistants & Threads

## Overview

You can find the full code [here](https://github.com/aimlapi/threads-demo).

The Threads API is very useful for creating a chat-like experience with a relatively large message history.

Usually, this task is accomplished by using the chat completion API, but it requires managing the state of the message history, which can't be done in all contexts. For example, in a serverless environment, you can't store the state, so you need to find another way to manage the state.

The Threads API does this state management for you, providing streaming and polling features out-of-the-box. You can use the Threads API using raw HTTP requests, but a simpler way is to use the Openal SDK.

## **Example**

First, you need to import the OpenAI SDK:

```javascript
const { OpenAI } = require('openai');
const api = new OpenAI({ baseUrl: '{{baseUrl}}', apiKey: '{{token}}' })
```

Next, create your assistant:

```javascript
const assistant = await api.beta.assistants.create({
      model: '{{llmModel}}',
      name: 'Funny assistant',
      description: 'Replies only with jokes',
      instructions: 'Reply to user only with jokes',
});
```

And create a thread and write a user message in it:

```javascript
// Empty thread
const thread = await api.beta.threads.create({ messages: [] });
await api.beta.threads.messages.create(thread.id, {
      role: 'user',
      content: text,
      metadata: {
        userId: 'example-1',
      },
 });
await api.beta.threads.runs.createAndPoll(thread.id, { assistant_id: assistantId });
const messages = await api.beta.threads.messages.list(thread.id, { order: 'desc', limit: 1 });
const msg = messages.data[0].content.find((item) => item.type === 'text').text.value;
console.log(`Assistant: ${msg}`);
```

This code will return a model answer based on your prompt and keep the thread with message history on the server, transferring all message state management from your code to the API.


================================================
File: docs/.gitbook/includes/untitled (1).md
================================================
---
title: Untitled
---

{% content-ref url="../../api-overview/audio-models-music-and-vocal/minimax-music/" %}
[minimax-music](../../api-overview/audio-models-music-and-vocal/minimax-music/)
{% endcontent-ref %}


================================================
File: docs/.gitbook/includes/untitled.md
================================================
---
title: Untitled
---

{% content-ref url="../../api-overview/audio-models-music-and-vocal/minimax-music/" %}
[minimax-music](../../api-overview/audio-models-music-and-vocal/minimax-music/)
{% endcontent-ref %}


================================================
File: packages/snippets/README.md
================================================
# Snippet App

The code blocks generator.

================================================
File: packages/snippets/eslint.config.js
================================================
import js from '@eslint/js'
import globals from 'globals'
import reactHooks from 'eslint-plugin-react-hooks'
import reactRefresh from 'eslint-plugin-react-refresh'
import tseslint from 'typescript-eslint'

export default tseslint.config(
  { ignores: ['dist'] },
  {
    extends: [js.configs.recommended, ...tseslint.configs.recommended],
    files: ['**/*.{ts,tsx}'],
    languageOptions: {
      ecmaVersion: 2020,
      globals: globals.browser,
    },
    plugins: {
      'react-hooks': reactHooks,
      'react-refresh': reactRefresh,
    },
    rules: {
      ...reactHooks.configs.recommended.rules,
      'react-refresh/only-export-components': [
        'warn',
        { allowConstantExport: true },
      ],
    },
  },
)


================================================
File: packages/snippets/index.html
================================================
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <link rel="icon" type="image/svg+xml" href="/vite.svg" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />

    <title>AI / ML API Snippet</title>
  </head>
  <body>
    <div id="root"></div>
    <script type="module" src="/src/main.ts"></script>

    <snippet data-name="open-ai.chat-completion" data-model="super"> </snippet>
    <snippet data-name="open-ai.chat-completion" data-model="super"> </snippet>
    <snippet data-name="luma-ai.create-generation" data-model="super"> </snippet>
  </body>
</html>


================================================
File: packages/snippets/package.json
================================================
{
  "name": "snippets",
  "private": false,
  "version": "0.0.1",
  "type": "module",
  "scripts": {
    "dev": "vite",
    "build": "tsc -b && vite build",
    "lint": "eslint .",
    "preview": "vite preview"
  },
  "dependencies": {
    "clsx": "^2.1.1",
    "handlebars": "^4.7.8",
    "react": "^18.3.1",
    "react-custom-scrollbars-2": "^4.5.0",
    "react-dom": "^18.3.1",
    "react-feather": "^2.0.10",
    "react-loading-skeleton": "^3.5.0",
    "shiki": "^1.22.0"
  },
  "devDependencies": {
    "@eslint/js": "^9.11.1",
    "@shikijs/transformers": "^1.22.0",
    "@tsconfig/strictest": "^2.0.5",
    "@types/node": "^22.7.5",
    "@types/react": "^18.3.10",
    "@types/react-dom": "^18.3.0",
    "@vitejs/plugin-react": "^4.3.2",
    "eslint": "^9.11.1",
    "eslint-plugin-react-hooks": "^5.1.0-rc.0",
    "eslint-plugin-react-refresh": "^0.4.12",
    "globals": "^15.9.0",
    "typescript": "^5.5.3",
    "typescript-eslint": "^8.7.0",
    "vite": "^5.4.8"
  }
}


================================================
File: packages/snippets/tsconfig.app.json
================================================
{
  "compilerOptions": {
    "target": "ES2020",
    "useDefineForClassFields": true,
    "lib": ["ES2020", "DOM", "DOM.Iterable"],
    "module": "ESNext",
    "skipLibCheck": true,

    /* Bundler mode */
    "moduleResolution": "bundler",
    "allowImportingTsExtensions": true,
    "isolatedModules": true,
    "moduleDetection": "force",
    "noEmit": true,
    "jsx": "react-jsx",

    /* Linting */
    "strict": true,
    "noUnusedLocals": true,
    "noUnusedParameters": true,
    "noFallthroughCasesInSwitch": true
  },
  "include": ["src"]
}


================================================
File: packages/snippets/tsconfig.app.tsbuildinfo
================================================
{"root":["./src/inject.tsx","./src/main.ts","./src/vite-env.d.ts","./src/components/app/index.tsx","./src/components/snippet-viewer/index.tsx","./src/components/snippet-viewer/content/index.tsx","./src/components/snippet-viewer/footer/index.tsx","./src/components/snippet-viewer/header/index.tsx","./src/components/snippet-viewer/tab/index.tsx","./src/constants/language.ts","./src/constants/snippets.ts","./src/types/snippet.ts","./src/types/utils.ts","./src/utils/compile-snippets.ts","./src/utils/load-page-snippets.ts","./src/utils/load-template-snippets.ts"],"version":"5.6.2"}

================================================
File: packages/snippets/tsconfig.json
================================================
{
  "extends": ["@tsconfig/strictest/tsconfig.json"],
  "files": [],
  "references": [{ "path": "./tsconfig.app.json" }, { "path": "./tsconfig.node.json" }]
}


================================================
File: packages/snippets/tsconfig.node.json
================================================
{
  "compilerOptions": {
    "target": "ES2022",
    "lib": ["ES2023"],
    "module": "ESNext",
    "skipLibCheck": true,

    /* Bundler mode */
    "moduleResolution": "bundler",
    "allowImportingTsExtensions": true,
    "isolatedModules": true,
    "moduleDetection": "force",
    "noEmit": true,

    /* Linting */
    "strict": true,
    "noUnusedLocals": true,
    "noUnusedParameters": true,
    "noFallthroughCasesInSwitch": true
  },
  "include": ["vite.config.ts"]
}


================================================
File: packages/snippets/tsconfig.node.tsbuildinfo
================================================
{"root":["./vite.config.ts"],"version":"5.6.2"}

================================================
File: packages/snippets/vite.config.ts
================================================
import { defineConfig, loadEnv } from 'vite';
import react from '@vitejs/plugin-react';

// https://vitejs.dev/config/
export default defineConfig(({ mode }) => {
  const env = loadEnv(mode, process.cwd(), '');

  return {
    base: env.SNIPPETS_CDN_PREFIX,
    plugins: [react()],
    build: {
      rollupOptions: {
        input: 'src/main.ts',
        output: {
          entryFileNames: 'bundle.js',
          assetFileNames: 'bundle.css',
        },
      },
    },
    assetsInclude: ['**/*.hbs'],
  };
});


================================================
File: packages/snippets/.gitignore
================================================
# Logs
logs
*.log
npm-debug.log*
yarn-debug.log*
yarn-error.log*
pnpm-debug.log*
lerna-debug.log*

node_modules
dist-ssr
*.local

# Editor directories and files
.vscode/*
!.vscode/extensions.json
.idea
.DS_Store
*.suo
*.ntvs*
*.njsproj
*.sln
*.sw?

dist

================================================
File: packages/snippets/src/inject.tsx
================================================
import { StrictMode } from 'react';
import { createRoot } from 'react-dom/client';

import App from './components/app';

export const inject = () => {
  const element = document.createElement('div');

  document.body.append(element);

  createRoot(element).render(
    <StrictMode>
      <App />
    </StrictMode>,
  );
};


================================================
File: packages/snippets/src/main.ts
================================================
const run = async () => {
  const app = await import('./inject');
  app.inject();
};

if (['complete', 'loaded', 'interactive'].includes(document.readyState)) {
  run();
} else {
  document.addEventListener('DOMContentLoaded', () => run());
}


================================================
File: packages/snippets/src/vite-env.d.ts
================================================
/// <reference types="vite/client" />


================================================
File: packages/snippets/src/components/app/index.tsx
================================================
import { useMemo } from 'react';
import { loadPageSnippets } from '../../utils/load-page-snippets';
import SnippetViewer from '../snippet-viewer';
import { loadTemplateSnippets } from '../../utils/load-template-snippets';
import { compileSnippets } from '../../utils/compile-snippets';
import { createPortal } from 'react-dom';

const App = () => {
  const { compiledById, snippets } = useMemo(() => {
    const templates = loadTemplateSnippets();
    const snippets = loadPageSnippets();
    return { compiledById: compileSnippets(templates, snippets), snippets: Object.values(snippets) };
  }, []);

  return <>{snippets.map((s) => createPortal(<SnippetViewer key={s.id} snippet={compiledById[s.id]} />, s.element))}</>;
};

export default App;


================================================
File: packages/snippets/src/components/snippet-viewer/index.css
================================================
snippet[name] {
  display: block;
  max-width: 100vw;
}

.snippet-viewer {
  display: flex;
  flex-direction: column;
  max-width: 100vw;
}

.snippet-viewer pre[tabindex],
.snippet-viewer a {
  transition: outline 0.1s ease-out;
  outline: 1.5px solid transparent;
  outline-offset: -1.5px;
}
.snippet-viewer button {
  outline: 1.5px solid transparent;
  outline-offset: -1.5px;
}

.snippet-viewer pre[tabindex]:focus-visible,
.snippet-viewer button:focus-visible {
  outline: 2px solid #637f9e;
  outline-offset: -2px;
}

.snippet-viewer button,
.snippet-viewer a[type='button'] {
  line-height: 1.6;
  font-family: Inter, sans-serif;
  user-select: none;

  text-decoration: none;
}


================================================
File: packages/snippets/src/components/snippet-viewer/index.tsx
================================================
import { codeToHtml } from 'shiki/bundle/web';
import { CompiledSnippet, SnippetOption } from '../../types/snippet';
import { FC, useCallback, useEffect, useMemo, useState } from 'react';

import Content from './content';
import Header from './header';
import Footer from './footer';

import './index.css';

export type Props = {
  snippet: CompiledSnippet;
};
const SnippetViewer: FC<Props> = (props: Props) => {
  const { snippet } = props;
  const [options, setOptions] = useState<(SnippetOption & { html: string; loaded?: boolean })[]>(() =>
    snippet.options.map((s) => ({ ...s, html: '' })),
  );
  const [language, setLanguage] = useState(snippet.options[0].language);

  const selectedOption = useMemo(() => {
    return options.find((o) => o.language === language) || options[0];
  }, [language, options]);

  useEffect(() => {
    let isCancelled = false;
    snippet.options.forEach((o, i) => {
      codeToHtml(o.content, {
        lang: o.language,
        theme: 'github-dark',
        transformers: [],
      }).then((html) => {
        if (isCancelled) {
          return;
        }

        setOptions((prev) => {
          const updated = [...prev];
          updated[i] = { ...updated[i], html, loaded: true };

          return updated;
        });
      });
    });

    return () => {
      isCancelled = true;
    };
  }, [snippet.options]);

  const [isCopied, setIsCopied] = useState(false);
  const handleCopy = useCallback(() => {
    let isCancelled = false;

    if (isCopied) {
      return;
    }

    setIsCopied(true);

    window.navigator.clipboard.writeText(selectedOption.content);

    setTimeout(() => {
      if (isCancelled) {
        return;
      }

      setIsCopied(false);
    }, 10000);

    return () => {
      isCancelled = true;
    };
  }, [isCopied, selectedOption.content]);

  return (
    <div className="snippet-viewer">
      <Header snippet={snippet} language={language} options={options} onSetLanguage={setLanguage} />
      <Content option={selectedOption} loading={!selectedOption.loaded} />
      <Footer snippet={snippet} isCopied={isCopied} onCopy={handleCopy} />
    </div>
  );
};

export default SnippetViewer;


================================================
File: packages/snippets/src/components/snippet-viewer/content/index.css
================================================
.snippet-viewer-content {
  background-color: #24292e;
  font-size: 14px;
  line-height: 1.7;
}

.snippet-viewer-content:last-child {
  border-radius: 0px 0px 4px 4px;
}

.snippet-viewer-content__inner {
  padding: 1rem;
  margin: 0;
  overflow: visible;
  max-width: 0;
}

.snippet-viewer-content__inner .shiki {
  margin: 0;
  overflow: visible;
}


================================================
File: packages/snippets/src/components/snippet-viewer/content/index.tsx
================================================
import { FC, useMemo } from 'react';
import { OverwriteProps } from '../../../types/utils';
import { SnippetOption } from '../../../types/snippet';

import Skeleton from 'react-loading-skeleton';
import Scrollbars, { ScrollbarProps } from 'react-custom-scrollbars-2';

import 'react-loading-skeleton/dist/skeleton.css';
import './index.css';

export type Props = React.PropsWithChildren<
  OverwriteProps<
    React.HTMLProps<Scrollbars>,
    {
      loading?: boolean;
      option: SnippetOption & { html: string };
    }
  >
>;
const Content: FC<Props> = (props) => {
  const { option, loading, ...rest } = props;

  const lines = useMemo(() => {
    if (loading) {
      return option.content.split('\n').map((l) => l.length);
    }

    return [];
  }, [loading, option.content]);

  const scrollbar = useMemo<ScrollbarProps>(() => ({ autoHeight: true, autoHeightMax: 350 }), []);

  return (
    <>
      {loading ? (
        <Scrollbars {...scrollbar} {...rest} className="snippet-viewer-content">
          <pre className="snippet-viewer-content__inner">
            {lines.map((length, i) => (
              <Skeleton key={i} baseColor="#30363b" highlightColor="#373f45" width={`${length * 7.83}px`} />
            ))}
          </pre>
        </Scrollbars>
      ) : (
        <Scrollbars {...scrollbar} {...rest} className="snippet-viewer-content">
          <pre className="snippet-viewer-content__inner" dangerouslySetInnerHTML={{ __html: option.html }} />
        </Scrollbars>
      )}
    </>
  );
};

export default Content;


================================================
File: packages/snippets/src/components/snippet-viewer/footer/index.css
================================================
.snippet-viewer-footer {
  display: flex;
  align-items: center;
  background-color: #30363b;
  border-radius: 0px 0px 4px 4px;

  font-family: Inter, sans-serif;
  font-size: 14px;
  box-shadow: rgba(0, 0, 0, 0.16) 0px -1px 4px;
  border-bottom: 1px solid rgba(255, 255, 255, 0.1);
  box-sizing: border-box;

  padding: 0.25rem 0.25rem;
  gap: 0.5rem;
}

.snippet-viewer-footer__link {
  color: #9ecbff;
  margin-left: auto;
}

.snippet-viewer-footer__link > svg {
  transform: translateY(3px);
  margin-left: 0.25rem;
}

.snippet-viewer-footer__btn {
  display: flex;
  align-items: center;
  justify-content: center;
  gap: 0.25rem;
  padding: 0.1rem 0.25rem;

  background-color: #24292e;
  min-width: 80px;

  border: 0;

  color: #9ecbff;
  border-radius: 4px;
  cursor: pointer;
  font-size: 12px;
  font-weight: bold;

  transform: scale(1);
  transition: transform 0.1s ease-out, color 0.1s ease-out, background-color 0.1s ease-out, outline 0.1s ease-out;

  box-shadow: #9ecbff5b 0px 0px 2px;
}

.snippet-viewer-footer__btn:hover:not(:active) {
  color: #24292e;
  background-color: #9ecbff;
}
.snippet-viewer-footer__btn:active {
  transform: scale(0.9);
}


================================================
File: packages/snippets/src/components/snippet-viewer/footer/index.tsx
================================================
import { FC } from 'react';
import { OverwriteProps } from '../../../types/utils';
import { BookOpen, Copy } from 'react-feather';

import './index.css';
import { CompiledSnippet } from '../../../types/snippet';

export type Props = React.PropsWithChildren<
  OverwriteProps<
    React.HTMLAttributes<HTMLElement>,
    {
      isCopied?: boolean;
      onCopy: () => unknown;

      snippet: CompiledSnippet;
    }
  >
>;
const Footer: FC<Props> = (props) => {
  const { snippet, isCopied, onCopy, ...rest } = props;

  return (
    <footer className="snippet-viewer-footer" {...rest}>
      <button className="snippet-viewer-footer__btn" onClick={onCopy}>
        {isCopied ? 'Copied!' : 'Copy'} <Copy size={14} />
      </button>
      {snippet.template.config.docs && (
        <a type="button" href={snippet.template.config.docs} className="snippet-viewer-footer__btn" onClick={onCopy}>
          Docs <BookOpen size={14} />
        </a>
      )}
    </footer>
  );
};

export default Footer;


================================================
File: packages/snippets/src/components/snippet-viewer/header/index.css
================================================
.snippet-viewer-header {
  display: flex;
  align-items: center;
  background-color: #30363b;
  border-radius: 4px 4px 0px 0px;

  font-family: monospace;
  box-shadow: rgba(0, 0, 0, 0.16) 0px 1px 4px;
  border-bottom: 1px solid rgba(255, 255, 255, 0.1);
  box-sizing: border-box;
}

.snippet-viewer-header__title {
  color: white;
  flex: 1;
  text-overflow: ellipsis;
  white-space: nowrap;
  overflow: hidden;

  font-size: 14px;
  font-family: monospace;

  margin: 0 0.5rem;
}


================================================
File: packages/snippets/src/components/snippet-viewer/header/index.tsx
================================================
import { FC } from 'react';
import { OverwriteProps } from '../../../types/utils';

import './index.css';
import { CompiledSnippet, SnippetOption } from '../../../types/snippet';
import { LANGUAGE_MAP } from '../../../constants/language';
import Tab from '../tab';

export type Props = React.PropsWithChildren<
  OverwriteProps<
    React.HTMLAttributes<HTMLElement>,
    {
      language: string;
      snippet: CompiledSnippet;
      options: SnippetOption[];

      onSetLanguage: (language: string) => unknown;
    }
  >
>;
const Header: FC<Props> = (props) => {
  const { language, snippet, options, onSetLanguage, ...rest } = props;

  return (
    <header className="snippet-viewer-header" {...rest}>
      <span className="snippet-viewer-header__title">{snippet.template.config.title}</span>
      {options.map((o) => (
        <Tab key={o.language} onSelect={onSetLanguage} payload={o.language} active={o.language === language}>
          {(LANGUAGE_MAP as Record<string, string>)[o.language] || o.language}
        </Tab>
      ))}
    </header>
  );
};

export default Header;


================================================
File: packages/snippets/src/components/snippet-viewer/tab/index.css
================================================
.snippet-viewer-tab {
  font-size: 14px;

  color: white;
  background-color: #24292e;
  padding: 0.25rem 0.5rem;
  border: 0px solid;
  border-bottom: 1.5px solid transparent;

  cursor: pointer;
  transition: border-color 0.1s ease-out, color 0.1s ease-out;
  user-select: none;

  text-overflow: ellipsis;
  white-space: nowrap;
  overflow: hidden;
}

.snippet-viewer-tab:hover {
  color: #9ecbff;
}

.snippet-viewer-tab:last-child {
  border-radius: 0px 4px 0px 0px;
}

.snippet-viewer-tab:first-child {
  border-radius: 4px 0px 0px 0px;
}

.snippet-viewer-tab_active {
  color: #9ecbff;
  border-bottom: 1.5px solid #9ecbff;
}


================================================
File: packages/snippets/src/components/snippet-viewer/tab/index.tsx
================================================
import { FC, useCallback } from 'react';
import clsx from 'clsx';
import { OverwriteProps } from '../../../types/utils';

import './index.css';

export type Props = React.PropsWithChildren<
  OverwriteProps<
    React.HTMLAttributes<HTMLButtonElement>,
    {
      payload: string;
      active?: boolean;
      onSelect: (payload: string) => unknown;
    }
  >
>;
const Tab: FC<Props> = (props) => {
  const { active, children, onSelect, ...rest } = props;

  const handleSelect = useCallback(
    (ev: React.MouseEvent<HTMLElement>) => {
      const id = ev.currentTarget.dataset.id!;
      onSelect(id);
    },
    [onSelect],
  );

  return (
    <button
      {...rest}
      data-id={rest.payload}
      className={clsx('snippet-viewer-tab', active && 'snippet-viewer-tab_active')}
      onClick={handleSelect}
    >
      {children}
    </button>
  );
};

export default Tab;


================================================
File: packages/snippets/src/constants/language.ts
================================================
export const LANGUAGE_MAP = {
  py: 'Python',
  js: 'JavaScript',
} satisfies Record<string, string>;


================================================
File: packages/snippets/src/constants/snippets.ts
================================================
const configByName = import.meta.glob('../../../../shared/snippets/**/*.json', {
  query: '?raw',
  import: 'default',
  eager: true,
}) as Record<string, string>;

const contentByName = import.meta.glob('../../../../shared/snippets/**/*.hbs', {
  query: '?raw',
  import: 'default',
  eager: true,
}) as Record<string, string>;

const configById = Object.keys(configByName).reduce((byKey, key) => {
  const next = { ...byKey };

  const [, path] = key.split('/snippets/');
  const [group, name] = path.split('/');

  next[`${group}.${name}`] = JSON.parse(configByName[key]);

  return next;
}, {} as Record<string, { title: string }>);

export const snippets = Object.keys(contentByName).reduce((byKey, key) => {
  const next = { ...byKey };

  const [, path] = key.split('/snippets/');
  const [group, name, file] = path.split('/');
  const [language] = file.split('.');

  const config = configById[`${group}.${name}`]!;

  next[key] = { ...next[key], group, name, language, content: contentByName[key], config: config };

  return next;
}, {} as Record<string, { config: { title: string; payload?: Record<string, unknown> }; group: string; name: string; language: string; content: string }>);


================================================
File: packages/snippets/src/types/snippet.ts
================================================
export type SnippetOption = { content: string; language: string };

export type PageSnippet = {
  id: string;
  key: string;
  payload: Record<string, string>;
  element: HTMLElement;
};

export type TemplateSnippet = {
  config: {
    title: string;
    docs?: string;
  };
  payload: Record<string, unknown>;
  key: string;
  options: SnippetOption[];
};

export type CompiledSnippet = {
  template: TemplateSnippet;
  snippet: PageSnippet;
  options: SnippetOption[];
};


================================================
File: packages/snippets/src/types/utils.ts
================================================
export type OverwriteProps<A, B> = Omit<A, keyof B> & B;


================================================
File: packages/snippets/src/utils/compile-snippets.ts
================================================
import hbs from 'handlebars';
import { CompiledSnippet, PageSnippet, SnippetOption, TemplateSnippet } from '../types/snippet';

export const compileSnippets = (templates: Record<string, TemplateSnippet>, snippets: Record<string, PageSnippet>) => {
  const compiledById: Record<string, CompiledSnippet> = {};
  for (const id in snippets) {
    const snippet = snippets[id];
    const template = templates[snippet.key];

    if (!template) {
      console.warn(`Snippet '${snippet.key}' not found`);
      continue;
    }

    const options: SnippetOption[] = template.options.map((o) => ({
      ...o,
      content: hbs.compile(o.content, { noEscape: true })({ ...template.payload, ...snippet.payload }),
    }));

    compiledById[snippet.id] = { snippet, options, template };
  }

  return compiledById;
};


================================================
File: packages/snippets/src/utils/load-page-snippets.ts
================================================
import { PageSnippet } from '../types/snippet';

export const loadPageSnippets = () => {
  const elements = document.querySelectorAll('snippet[data-name]');
  const byId: Record<string, PageSnippet> = {};
  elements.forEach((element) => {
    if (element instanceof HTMLElement) {
      const { name, ...payload } = element.dataset;
      if (!name) {
        console.warn('No snippet provided:', element);
        return;
      }

      const id = Math.random().toFixed(10).slice(2);

      byId[id] = {
        id,
        key: name || '',
        payload: (payload as Record<string, string>) || {},
        element,
      };
    }
  });

  return byId;
};


================================================
File: packages/snippets/src/utils/load-template-snippets.ts
================================================
import { snippets } from '../constants/snippets';
import { TemplateSnippet } from '../types/snippet';

const exportKeyMetadata = (group: string, name: string) => {
  return [group, name].join('.').replace(/_/gm, '-');
};

export const loadTemplateSnippets = () => {
  const byKey: Record<string, TemplateSnippet> = {};
  Object.keys(snippets).forEach((key) => {
    const snippet = snippets[key];

    const k = exportKeyMetadata(snippet.group, snippet.name);
    const option = { language: snippet.language, content: snippet.content };
    const template: TemplateSnippet = {
      key,
      config: { ...snippet.config, title: snippet.config.title || snippet.name },
      options: [...(byKey[k]?.options || []), option],
      payload: snippet.config.payload || {},
    };

    byKey[k] = { ...byKey[k], ...template };
  });

  return byKey;
};


================================================
File: shared/snippets/3d/triposr/config.json
================================================
{
  "title": "Generate an mesh base on an image",
  "payload": {
    "baseUrl": "https://api.aimlapi.com/v1",
    "apiKey": "<YOUR_API_KEY>",
    "model": "triposr"
  }
}


================================================
File: shared/snippets/3d/triposr/js.hbs
================================================
const fs = require('fs');
const { Readable } = require('stream');
const { finished } = require('stream/promises');

const main = async () => {
  const {
    model_mesh: { url, file_name },
  } = await fetch('{{baseUrl}}/image/generations', {
    method: 'POST',
    headers: {
      Authorization: 'Bearer {{apiKey}}',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: '{{model}}',
      image_url: 'https://upload.wikimedia.org/wikipedia/commons/7/7a/Basketball.png',
    }),
  }).then((res) => res.json());

  const { body } = await fetch(url);
  const stream = fs.createWriteStream(`./${file_name}`);
  await finished(Readable.fromWeb(body).pipe(stream));
};

main();

================================================
File: shared/snippets/3d/triposr/py.hbs
================================================
import requests


def main():
    response = requests.post(
        "{{baseUrl}}/images/generations",
        headers={
            "Authorization": "Bearer {{apiKey}}",
            "Content-Type": "application/json",
        },
        json={
            "model": "{{model}}",
            "image_url": "https://upload.wikimedia.org/wikipedia/commons/7/7a/Basketball.png",
        },
    )

    response.raise_for_status()
    data = response.json()
    url = data["model_mesh"]["url"]
    file_name = data["model_mesh"]["file_name"]

    mesh_response = requests.get(url, stream=True)

    with open(file_name, "wb") as file:
        for chunk in mesh_response.iter_content(chunk_size=8192):
            file.write(chunk)


if __name__ == "__main__":
    main()

================================================
File: shared/snippets/anthropic/messages/config.json
================================================
{
  "title": "Create an message",
  "docs": "https://docs.aimlapi.com/api-overview/text-models-llm/function-calling/anthropic",
  "payload": {
    "baseUrl": "https://api.aimlapi.com/v1",
    "apiKey": "<YOUR_API_KEY>",
    "model": "claude-3-5-sonnet-20240620"
  }
}


================================================
File: shared/snippets/anthropic/messages/js.hbs
================================================
const Anthropic = require('@anthropic-ai/sdk');

const api = new Anthropic({
  baseURL: '{{baseUrl}}',
  authToken: '{{apiKey}}',
});

const main = async () => {
  const message = await api.messages.create({
    model: '{{model}}',
    max_tokens: 2048,
    system: 'You are an AI assistant who knows everything.',
    messages: [
      {
        role: 'user',
        content: 'Tell me, why is the sky blue?',
      },
    ],
  });

  console.log('Message:', message);
};

main();


================================================
File: shared/snippets/anthropic/messages/py.hbs
================================================
import asyncio
from anthropic import AsyncAnthropic

client = Anthropic(
    base_url="{{baseUrl}}",
    auth_token="{{apiKey}}",
)


def main():
    message = client.messages.create(
        model="{{model}}",
        max_tokens=2048,
        system="You are an AI assistant who knows everything.",
        messages=[
            {
                "role": "user",
                "content": "Hello, Claude",
            }
        ],
    )

    print("Message:", message.content)


if __name__ == "__main__":
    main()


================================================
File: shared/snippets/audio/create-generation-minimax/config.json
================================================
{
  "title": "Generate an audio",
  "docs": "https://docs.aimlapi.com/api-overview/audio-models-music-and-vocal/minimax-music",
  "payload": {
    "baseUrl": "https://api.aimlapi.com/v2",
    "apiKey": "<YOUR_API_KEY>"
  }
}


================================================
File: shared/snippets/audio/create-generation-minimax/js.hbs
================================================
const main = async () => {
  const response = await fetch('{{baseUrl}}/generate/audio', {
    method: 'POST',
    headers: {
      Authorization: 'Bearer {{apiKey}}',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: '{{model}}',
      prompt: '##Swift and Boundless \n In the realm of innovation, where visions align, \n\nAIML API's the name, making tech shine. \nIntelligent solutions, breaking the mold, \n\nSwift inference power, bold and untold.\n##',
      reference_audio_url: 'https://cdn.aimlapi.com/squirrel/files/zebra/WzNbqH7vR20MNTOD1Ec7k_output.mp3',
    }),
  }).then((res) => res.json());

  console.log('Generation:', response);
};

main()


================================================
File: shared/snippets/audio/create-generation-minimax/py.hbs
================================================
import requests


def main():
    url = "{{baseUrl}}/generate/audio"
    payload = {
        "model": "{{model}}",
        "prompt": "##Swift and Boundless \n In the realm of innovation, where visions align, \n\nAIML API's the name, making tech shine. \nIntelligent solutions, breaking the mold, \n\nSwift inference power, bold and untold.\n##",
        "reference_audio_url": "https://cdn.aimlapi.com/squirrel/files/zebra/WzNbqH7vR20MNTOD1Ec7k_output.mp3",
    }
    headers = {"Authorization": "Bearer {{apiKey}}", "Content-Type": "application/json"}

    response = requests.post(url, json=payload, headers=headers)
    print("Generation:", response.json())


if __name__ == "__main__":
    main()


================================================
File: shared/snippets/audio/create-generation-stable/config.json
================================================
{
  "title": "Generate an audio",
  "docs": "https://docs.aimlapi.com/api-overview/audio-models-music-and-vocal/stable-audio",
  "payload": {
    "baseUrl": "https://api.aimlapi.com/v2",
    "apiKey": "<YOUR_API_KEY>"
  }
}


================================================
File: shared/snippets/audio/create-generation-stable/js.hbs
================================================
const main = async () => {
  const response = await fetch('{{baseUrl}}/generate/audio', {
    method: 'POST',
    headers: {
      Authorization: 'Bearer {{apiKey}}',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: '{{model}}',
      prompt: 'lo-fi pop hip-hop ambient music',
      steps: 100,
      seconds_total: 10,
    }),
  }).then((res) => res.json());

  console.log('Generation:', response);
};

main()


================================================
File: shared/snippets/audio/create-generation-stable/py.hbs
================================================
import requests


def main():
    url = "{{baseUrl}}/generate/audio"
    payload = {
        "model": "{{model}}",
        "prompt": "lo-fi pop hip-hop ambient music",
        "steps": 100,
        "seconds_total": 10,
    }
    headers = {"Authorization": "Bearer {{apiKey}}", "Content-Type": "application/json"}

    response = requests.post(url, json=payload, headers=headers)
    print("Generation:", response.json())


if __name__ == "__main__":
    main()


================================================
File: shared/snippets/audio/fetch-generation/config.json
================================================
{
  "title": "Fetch an audio",
  "docs": "https://docs.aimlapi.com/api-overview/audio-models-music-and-vocal/stable-audio/fetch-generations",
  "payload": {
    "baseUrl": "https://api.aimlapi.com/v2",
    "apiKey": "<YOUR_API_KEY>"
  }
}


================================================
File: shared/snippets/audio/fetch-generation/js.hbs
================================================
const main = async () => {
  const params = new URLSearchParams({
    generation_id: "<YOUR_GENERATION_ID>"
  });
  const response = await fetch(`{{baseUrl}}/generate/audio?${params.toString()}`, {
    method: 'GET',
    headers: {
      Authorization: 'Bearer {{apiKey}}',
      'Content-Type': 'application/json',
    },
  }).then((res) => res.json());

  console.log('Generation:', response);
};

main()


================================================
File: shared/snippets/audio/fetch-generation/py.hbs
================================================
import requests


def main():
    url = "{{baseUrl}}/generate/audio"
    params = {
        "generation_id": "<YOUR_GENERATION_ID>",
    }
    headers = {"Authorization": "Bearer {{apiKey}}", "Content-Type": "application/json"}

    response = requests.get(url, params=params, headers=headers)
    print("Generation:", response.json())


if __name__ == "__main__":
    main()


================================================
File: shared/snippets/image/flux/config.json
================================================
{
  "title": "Generate an image",
  "docs": "https://docs.aimlapi.com/api-overview/image-models/generate-an-image",
  "payload": {
    "baseUrl": "https://api.aimlapi.com/v1",
    "apiKey": "<YOUR_API_KEY>"
  }
}


================================================
File: shared/snippets/image/flux/js.hbs
================================================
const main = async () => {
  const response = await fetch('{{baseUrl}}/images/generations', {
    method: 'POST',
    headers: {
      Authorization: 'Bearer {{apiKey}}',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      prompt: 'A jellyfish in the ocean',
      model: '{{model}}',
    }),
  }).then((res) => res.json());

  console.log('Generation:', response);
};

main();


================================================
File: shared/snippets/image/flux/py.hbs
================================================
import requests


def main():
    response = requests.post(
        "{{baseUrl}}/images/generations",
        headers={
            "Authorization": "Bearer {{apiKey}}",
            "Content-Type": "application/json",
        },
        json={
            "prompt": "A jellyfish in the ocean",
            "model": "{{model}}",
        },
    )

    response.raise_for_status()
    data = response.json()

    print("Generation:", data)


if __name__ == "__main__":
    main()


================================================
File: shared/snippets/image/flux-image-to-image/config.json
================================================
{
  "title": "Generate an image",
  "payload": {
    "baseUrl": "https://api.aimlapi.com/v1",
    "apiKey": "<YOUR_API_KEY>"
  }
}


================================================
File: shared/snippets/image/flux-image-to-image/js.hbs
================================================
const main = async () => {
  const response = await fetch('{{baseUrl}}/images/generations', {
    method: 'POST',
    headers: {
      Authorization: 'Bearer {{apiKey}}',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      image_url: 'https://cdn.prod.website-files.com/6600e1eab90de089c2d9c9cd/669726ef5242a23882952518_663fc2a1da49d30b9a44e774_image_3cN5ZzSm_1715403464233_raw.jpeg',
      prompt: 'A cat dressed as a wizard with a background of a mystic forest.',
      model: '{{model}}',
    }),
  }).then((res) => res.json());

  console.log('Generation:', response);
};

main();


================================================
File: shared/snippets/image/flux-image-to-image/py.hbs
================================================
import requests


def main():
    response = requests.post(
        "{{baseUrl}}/images/generations",
        headers={
            "Authorization": "Bearer {{apiKey}}",
            "Content-Type": "application/json",
        },
        json={
            "image_url": "https://cdn.prod.website-files.com/6600e1eab90de089c2d9c9cd/669726ef5242a23882952518_663fc2a1da49d30b9a44e774_image_3cN5ZzSm_1715403464233_raw.jpeg",
            "prompt": "A cat dressed as a wizard with a background of a mystic forest.",
            "model": "{{model}}",
        },
    )

    response.raise_for_status()
    data = response.json()

    print("Generation:", data)


if __name__ == "__main__":
    main()


================================================
File: shared/snippets/image/openai/config.json
================================================
{
  "title": "Generate an image",
  "docs": "https://docs.aimlapi.com/api-overview/image-models/generate-an-image",
  "payload": {
    "baseUrl": "https://api.aimlapi.com/v1",
    "apiKey": "<YOUR_API_KEY>"
  }
}


================================================
File: shared/snippets/image/openai/js.hbs
================================================
const main = async () => {
  const response = await fetch('{{baseUrl}}/images/generations', {
    method: 'POST',
    headers: {
      Authorization: 'Bearer {{apiKey}}',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      prompt: 'A jellyfish in the ocean',
      model: '{{model}}',
    }),
  }).then((res) => res.json());

  console.log('Generation:', response);
};

main();


================================================
File: shared/snippets/image/openai/py.hbs
================================================
import requests


def main():
    response = requests.post(
        "{{baseUrl}}/images/generations",
        headers={
            "Authorization": "Bearer {{apiKey}}",
            "Content-Type": "application/json",
        },
        json={
            "prompt": "A jellyfish in the ocean",
            "model": "{{model}}",
        },
    )

    response.raise_for_status()
    data = response.json()

    print("Generation:", data)


if __name__ == "__main__":
    main()


================================================
File: shared/snippets/image/recraft/config.json
================================================
{
  "title": "Generate an image",
  "docs": "https://docs.aimlapi.com/api-overview/image-models/generate-an-image",
  "payload": {
    "baseUrl": "https://api.aimlapi.com/v1",
    "apiKey": "<YOUR_API_KEY>",
    "model": "recraft-v3"
  }
}


================================================
File: shared/snippets/image/recraft/js.hbs
================================================
const main = async () => {
  const response = await fetch('{{baseUrl}}/images/generations', {
    method: 'POST',
    headers: {
      Authorization: 'Bearer {{apiKey}}',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      prompt: 'A jellyfish in the ocean',
      aspect_ratio: '16:9',
      model: '{{model}}',
      image_size: 'landscape_4_3',
      style: 'digital_illustration/pixel_art',
    }),
  }).then((res) => res.json());

  console.log('Generation:', response);
};

main();


================================================
File: shared/snippets/image/recraft/py.hbs
================================================
import requests


def main():
    response = requests.post(
        "{{baseUrl}}/images/generations",
        headers={
            "Authorization": "Bearer {{apiKey}}",
            "Content-Type": "application/json",
        },
        json={
            "prompt": "A jellyfish in the ocean",
            "aspect_ratio": "16:9",
            "model": "{{model}}",
            "image_size": "landscape_4_3",
            "style": "digital_illustration/pixel_art",
        },
    )

    response.raise_for_status()
    data = response.json()

    print("Generation:", data)


if __name__ == "__main__":
    main()


================================================
File: shared/snippets/image/stability/config.json
================================================
{
  "title": "Generate an image",
  "docs": "https://docs.aimlapi.com/api-overview/image-models/generate-an-image",
  "payload": {
    "baseUrl": "https://api.aimlapi.com/v1",
    "apiKey": "<YOUR_API_KEY>"
  }
}


================================================
File: shared/snippets/image/stability/js.hbs
================================================
const main = async () => {
  const response = await fetch('{{baseUrl}}/images/generations', {
    method: 'POST',
    headers: {
      Authorization: 'Bearer {{apiKey}}',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      prompt: 'A jellyfish in the ocean',
      model: '{{model}}',
    }),
  }).then((res) => res.json());

  console.log('Generation:', response);
};

main();


================================================
File: shared/snippets/image/stability/py.hbs
================================================
import requests


def main():
    response = requests.post(
        "{{baseUrl}}/images/generations",
        headers={
            "Authorization": "Bearer {{apiKey}}",
            "Content-Type": "application/json",
        },
        json={
            "prompt": "A jellyfish in the ocean",
            "model": "{{model}}",
        },
    )

    response.raise_for_status()
    data = response.json()

    print("Generation:", data)


if __name__ == "__main__":
    main()


================================================
File: shared/snippets/image/together/config.json
================================================
{
  "title": "Generate an image",
  "docs": "https://docs.aimlapi.com/api-overview/image-models/generate-an-image",
  "payload": {
    "baseUrl": "https://api.aimlapi.com/v1",
    "apiKey": "<YOUR_API_KEY>"
  }
}


================================================
File: shared/snippets/image/together/js.hbs
================================================
const main = async () => {
  const response = await fetch('{{baseUrl}}/images/generations', {
    method: 'POST',
    headers: {
      Authorization: 'Bearer {{apiKey}}',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      prompt: 'A jellyfish in the ocean',
      model: '{{model}}',
    }),
  }).then((res) => res.json());

  console.log('Generation:', response);
};

main();


================================================
File: shared/snippets/image/together/py.hbs
================================================
import requests


def main():
    response = requests.post(
        "{{baseUrl}}/images/generations",
        headers={
            "Authorization": "Bearer {{apiKey}}",
            "Content-Type": "application/json",
        },
        json={
            "prompt": "A jellyfish in the ocean",
            "model": "{{model}}",
        },
    )

    response.raise_for_status()
    data = response.json()

    print("Generation:", data)


if __name__ == "__main__":
    main()


================================================
File: shared/snippets/kling/create-image-to-video-generation/config.json
================================================
{
  "title": "Generate a video",
  "docs": "https://docs.aimlapi.com/api-overview/video-models/kling/generate-image-to-video",
  "payload": {
    "baseUrl": "https://api.aimlapi.com/v2",
    "apiKey": "<YOUR_API_KEY>"
  }
}


================================================
File: shared/snippets/kling/create-image-to-video-generation/js.hbs
================================================
const main = async () => {
  const response = await fetch('{{baseUrl}}/generate/video/kling/generation', {
    method: 'POST',
    headers: {
      Authorization: 'Bearer {{apiKey}}',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: '{{model}}',
      prompt: 'Mona Lisa puts on glasses with her hands.',
      ratio: '16:9',
      image_url: 'https://s2-111386.kwimgs.com/bs2/mmu-aiplatform-temp/kling/20240620/1.jpeg',
      duration: '5',
    }),
  }).then((res) => res.json());

  console.log('Generation:', response);
};

main()


================================================
File: shared/snippets/kling/create-image-to-video-generation/py.hbs
================================================
import requests


def main():
    url = "{{baseUrl}}/generate/video/kling/generation"
    payload = {
        "model": "{{model}}",
        "prompt": "Mona Lisa puts on glasses with her hands.",
        "ratio": "16:9",
        "image_url": "https://s2-111386.kwimgs.com/bs2/mmu-aiplatform-temp/kling/20240620/1.jpeg",
        "duration": "5",
    }
    headers = {"Authorization": "Bearer {{apiKey}}", "Content-Type": "application/json"}

    response = requests.post(url, json=payload, headers=headers)
    print("Generation:", response.json())


if __name__ == "__main__":
    main()


================================================
File: shared/snippets/kling/create-text-to-video-generation/config.json
================================================
{
  "title": "Generate a video",
  "docs": "https://docs.aimlapi.com/api-overview/video-models/kling/generate-text-to-video",
  "payload": {
    "baseUrl": "https://api.aimlapi.com/v2",
    "apiKey": "<YOUR_API_KEY>"
  }
}


================================================
File: shared/snippets/kling/create-text-to-video-generation/js.hbs
================================================
const main = async () => {
  const response = await fetch('{{baseUrl}}/generate/video/kling/generation', {
    method: 'POST',
    headers: {
      Authorization: 'Bearer {{apiKey}}',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: '{{model}}',
      prompt: 'A DJ on the stand is playing, around a World War II battlefield, lots of explosions, thousands of dancing soldiers, between tanks shooting, barbed wire fences, lots of smoke and fire, black and white old video: hyper realistic, photorealistic, photography, super detailed, very sharp, on a very white background',
      ratio: '16:9',
      duration: '5',
    }),
  }).then((res) => res.json());

  console.log('Generation:', response);
};

main()


================================================
File: shared/snippets/kling/create-text-to-video-generation/py.hbs
================================================
import requests


def main():
    url = "{{baseUrl}}/generate/video/kling/generation"
    payload = {
        "model": "{{model}}",
        "prompt": "A DJ on the stand is playing, around a World War II battlefield, lots of explosions, thousands of dancing soldiers, between tanks shooting, barbed wire fences, lots of smoke and fire, black and white old video: hyper realistic, photorealistic, photography, super detailed, very sharp, on a very white background",
        "ratio": "16:9",
        "duration": "5",
    }
    headers = {"Authorization": "Bearer {{apiKey}}", "Content-Type": "application/json"}

    response = requests.post(url, json=payload, headers=headers)
    print("Generation:", response.json())


if __name__ == "__main__":
    main()


================================================
File: shared/snippets/kling/fetch-generation/config.json
================================================
{
  "title": "Fetch a video",
  "docs": "https://docs.aimlapi.com/api-overview/video-models/kling/fetch-generation",
  "payload": {
    "baseUrl": "https://api.aimlapi.com/v2",
    "apiKey": "<YOUR_API_KEY>"
  }
}


================================================
File: shared/snippets/kling/fetch-generation/js.hbs
================================================
const main = async () => {
  const params = new URLSearchParams({
    generation_id: "<YOUR_GENERATION_ID>"
  });
  const response = await fetch(`{{baseUrl}}/generate/video/kling/generation?${params.toString()}`, {
    method: 'GET',
    headers: {
      Authorization: 'Bearer {{apiKey}}',
      'Content-Type': 'application/json',
    },
  }).then((res) => res.json());

  console.log('Generation:', response);
};

main()


================================================
File: shared/snippets/kling/fetch-generation/py.hbs
================================================
import requests


def main():
    url = "{{baseUrl}}/generate/video/kling/generation"
    params = {
        "generation_id": "<YOUR_GENERATION_ID>",
    }
    headers = {"Authorization": "Bearer {{apiKey}}", "Content-Type": "application/json"}

    response = requests.get(url, params=params, headers=headers)
    print("Generation:", response.json())


if __name__ == "__main__":
    main()


================================================
File: shared/snippets/luma-ai/create-generation/config.json
================================================
{
  "title": "Generate a video",
  "docs": "https://docs.aimlapi.com/api-overview/video-models/luma-ai-v2/overview",
  "payload": {
    "baseUrl": "https://api.aimlapi.com/v2",
    "apiKey": "<YOUR_API_KEY>"
  }
}


================================================
File: shared/snippets/luma-ai/create-generation/js.hbs
================================================
const main = async () => {
  const response = await fetch('{{baseUrl}}/generate/video/luma-ai/generation', {
    method: 'POST',
    headers: {
      Authorization: 'Bearer {{apiKey}}',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      prompt: 'A jellyfish in the ocean',
      aspect_ratio: '19:9',
      loop: false
    }),
  }).then((res) => res.json());

  console.log('Generation:', response);
};

main();

================================================
File: shared/snippets/luma-ai/create-generation/py.hbs
================================================
import requests


def main()
  url = "{{baseUrl}}/generate/video/luma-ai/generation"
  payload = {
    "prompt": "Flying jellyfish",
    "aspect_ratio": "16:9"
  }
  headers = {
    "Authorization": "Bearer {{apiKey}}",
    "Content-Type": "application/json"
  }
  
  response = requests.post(url, json=payload, headers=headers)
  print("Generation:",  response.json())
  
if __name__ == "__main__":
    main()

================================================
File: shared/snippets/luma-ai/fetch-generation/config.json
================================================
{
  "title": "Fetch a video",
  "docs": "https://docs.aimlapi.com/api-overview/video-models/luma-ai-v2/overview",
  "payload": {
    "baseUrl": "https://api.aimlapi.com/v2",
    "apiKey": "<YOUR_API_KEY>"
  }
}


================================================
File: shared/snippets/luma-ai/fetch-generation/js.hbs
================================================
const main = async () => {
  const params = new URLSearchParams({
    generation_id: "<YOUR_GENERATION_ID>"
  });
  const response = await fetch(`{{baseUrl}}/generate/video/luma-ai/generation?${params.toString()}`, {
    method: 'GET',
    headers: {
      Authorization: 'Bearer {{apiKey}}',
      'Content-Type': 'application/json',
    },
  }).then((res) => res.json());

  console.log('Generation:', response);
};

main()


================================================
File: shared/snippets/luma-ai/fetch-generation/py.hbs
================================================
import requests


def main():
    url = "{{baseUrl}}/generate/video/luma-ai/generation"
    params = {
        "generation_id": "<YOUR_GENERATION_ID>",
    }
    headers = {"Authorization": "Bearer {{apiKey}}", "Content-Type": "application/json"}

    response = requests.get(url, params=params, headers=headers)
    print("Generation:", response.json())


if __name__ == "__main__":
    main()


================================================
File: shared/snippets/minimax/create-audio-generation/config.json
================================================
{
  "title": "Generate an audio",
  "docs": "https://docs.aimlapi.com/api-overview/audio-models-music-and-vocal/minimax-music",
  "payload": {
    "baseUrl": "https://api.aimlapi.com/v2",
    "apiKey": "<YOUR_API_KEY>"
  }
}


================================================
File: shared/snippets/minimax/create-audio-generation/js.hbs
================================================
import fs from "fs";
import fetch from "node-fetch";

const main = async () => {
  const url = "{{baseUrl}}/generate/audio/minimax/generate";
  const voiceId = "vocal-2025010100000000-a0AAAaaa";
  const referInstrumental = "instrumental-2025010100000000-Aaa0aAaA";

  const lyrics = `##Swift and Boundless \n In the realm of innovation, where visions align, \n\nAIML API's the name, making tech shine. 
  \nIntelligent solutions, breaking the mold, \n\nSwift inference power, bold and untold.\n##`;

  const payload = {
    refer_voice: voiceId,
    refer_instrumental: referInstrumental,
    lyrics: lyrics,
    model: {{model}},
  };

  const headers = {
    "Content-Type": "application/json",
    "Authorization": `Bearer {{apiKey}}`,
  };

  try {
    const response = await fetch(url, {
      method: "POST",
      headers: headers,
      body: JSON.stringify(payload),
    });

    const data = await response.json();

    if (response.ok) {
      const audioHex = data.data.audio;
      const decodedHex = Buffer.from(audioHex, "hex");

      fs.writeFileSync("generated_audio.mp3", decodedHex);
      console.log("Audio file saved as generated_audio.mp3");
    } else {
      console.error("Failed to generate music:", data);
    }
  } catch (error) {
    console.error("Error during API request:", error);
  }
}

main();


================================================
File: shared/snippets/minimax/create-audio-generation/py.hbs
================================================
import requests

def main():
    url = '{{baseUrl}}/generate/audio/minimax/generate'

    voice_id = 'vocal-2025010100000000-a0AAAaaa'
    refer_instrumental = 'instrumental-2025010100000000-Aaa0aAaA'

    lyrics = '''
        ##Swift and Boundless \n In the realm of innovation, where visions align, \n\nAIML API's the name, making tech shine. 
        \nIntelligent solutions, breaking the mold, \n\nSwift inference power, bold and untold.\n##
    '''

    payload = {
        'refer_voice': voice_id,
        'refer_instrumental': refer_instrumental,
        'lyrics':  lyrics,
        'model': {{model}},
        }
    headers = {
        'Content-Type': 'application/json',
        'authorization': 'Bearer {{apiKey}}',
    }

    response = requests.post(url, headers=headers, json=payload)

    audio_hex = response.json()['data']['audio']
    decoded_hex = bytes.fromhex(audio_hex)

    with open('generated_audio.mp3', 'wb') as f:
        f.write(decoded_hex)

if __name__ == '__main__':
    main()


================================================
File: shared/snippets/minimax/create-video-generation/config.json
================================================
{
  "title": "Generate a video",
  "payload": {
    "baseUrl": "https://api.aimlapi.com/v2",
    "apiKey": "<YOUR_API_KEY>"
  }
}


================================================
File: shared/snippets/minimax/create-video-generation/js.hbs
================================================
const main = async () => {
  const response = await fetch('{{baseUrl}}/generate/video/minimax/generation', {
    method: 'POST',
    headers: {
      Authorization: 'Bearer {{apiKey}}',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: '{{model}}',
      prompt: 'Mona Lisa puts on glasses with her hands.',
      first_frame_image: 'https://s2-111386.kwimgs.com/bs2/mmu-aiplatform-temp/kling/20240620/1.jpeg',
    }),
  }).then((res) => res.json());

  console.log('Generation:', response);
};

main()


================================================
File: shared/snippets/minimax/create-video-generation/py.hbs
================================================
import requests


def main():
    url = "{{baseUrl}}/generate/video/minimax/generation"
    payload = {
        "model": "{{model}}",
        "prompt": "Mona Lisa puts on glasses with her hands.",
        "first_frame_image": "https://s2-111386.kwimgs.com/bs2/mmu-aiplatform-temp/kling/20240620/1.jpeg",
    }
    headers = {"Authorization": "Bearer {{apiKey}}", "Content-Type": "application/json"}

    response = requests.post(url, json=payload, headers=headers)
    print("Generation:", response.json())


if __name__ == "__main__":
    main()


================================================
File: shared/snippets/minimax/fetch-video-generation/config.json
================================================
{
  "title": "Fetch a video",
  "payload": {
    "baseUrl": "https://api.aimlapi.com/v2",
    "apiKey": "<YOUR_API_KEY>"
  }
}


================================================
File: shared/snippets/minimax/fetch-video-generation/js.hbs
================================================
const main = async () => {
  const params = new URLSearchParams({
    generation_id: "<YOUR_GENERATION_ID>"
  });
  const response = await fetch(`{{baseUrl}}/generate/video/minimax/generation?${params.toString()}`, {
    method: 'GET',
    headers: {
      Authorization: 'Bearer {{apiKey}}',
      'Content-Type': 'application/json',
    },
  }).then((res) => res.json());

  console.log('Generation:', response);
};

main()


================================================
File: shared/snippets/minimax/fetch-video-generation/py.hbs
================================================
import requests


def main():
    url = "{{baseUrl}}/generate/video/minimax/generation"
    params = {
        "generation_id": "<YOUR_GENERATION_ID>",
    }
    headers = {"Authorization": "Bearer {{apiKey}}", "Content-Type": "application/json"}

    response = requests.get(url, params=params, headers=headers)
    print("Generation:", response.json())


if __name__ == "__main__":
    main()


================================================
File: shared/snippets/minimax/upload-audio/config.json
================================================
{
  "title": "Upload audio",
  "docs": "https://docs.aimlapi.com/api-overview/audio-models-music-and-vocal/minimax-music",
  "payload": {
    "baseUrl": "https://api.aimlapi.com/v2",
    "apiKey": "<YOUR_API_KEY>"
  }
}


================================================
File: shared/snippets/minimax/upload-audio/js.hbs
================================================
import fs from "fs";
import path from "path";
import fetch from "node-fetch";
import FormData from "form-data";

const main = async () => {
  const url = "{{baseUrl}}/generate/audio/minimax/upload";
  const fileName = "filename.mp3";
  const filePath = path.resolve(fileName);

  const purpose = "song";

  const formData = new FormData();
  formData.append("purpose", purpose);
  formData.append("file", fs.createReadStream(filePath), {
    filename: fileName,
    contentType: "audio/mpeg",
  });

  try {
    const response = await fetch(url, {
      method: "POST",
      headers: {
        Authorization: `Bearer {{apiKey}}`,
        ...formData.getHeaders(),
      },
      body: formData,
    });

    if (response.ok) {
      const data = await response.json();
      console.log("Upload successful:", data);
    } else {
      console.error("Upload failed:", response.status, await response.text());
    }
  } catch (error) {
    console.error("Error during upload:", error.message);
  }
}

main();


================================================
File: shared/snippets/minimax/upload-audio/py.hbs
================================================
import requests

def main():
    url = '{{baseUrl}}/generate/audio/minimax/upload'
    file_name = 'file_name.mp3'
    file_path = 'file_path.mp3'

    purpose = 'song'

    payload = {
        'purpose': purpose,
    }
    files = [
        ('file', (file_name, open(file_path, 'rb'), 'audio/mpeg'))
    ]
    headers = {
        'authorization': 'Bearer {{apiKey}}',
    }

    response = requests.request('POST', url, headers=headers, data=payload, files=files)
    print(response.json())

if __name__ == '__main__':
    main()


================================================
File: shared/snippets/open-ai/chat-completion/config.json
================================================
{
  "title": "Creates a chat completion",
  "docs": "https://docs.aimlapi.com/api-overview/text-models-llm/chat-completion",
  "payload": {
    "baseUrl": "https://api.aimlapi.com/v1",
    "apiKey": "<YOUR_API_KEY>"
  }
}


================================================
File: shared/snippets/open-ai/chat-completion/js.hbs
================================================
const { OpenAI } = require('openai');

const api = new OpenAI({
  baseURL: '{{baseUrl}}',
  apiKey: '{{apiKey}}',
});

const main = async () => {
  const result = await api.chat.completions.create({
    model: '{{model}}',
    messages: [
      {
        role: 'system',
        content: 'You are an AI assistant who knows everything.',
      },
      {
        role: 'user',
        content: 'Tell me, why is the sky blue?'
      }
    ],
  });

  const message = result.choices[0].message.content;
  console.log(`Assistant: ${message}`);
};

main();

================================================
File: shared/snippets/open-ai/chat-completion/py.hbs
================================================
import os
from openai import OpenAI

client = OpenAI(
    base_url="{{baseUrl}}",
    api_key="{{apiKey}}",    
)

response = client.chat.completions.create(
    model="{{model}}",
    messages=[
        {
            "role": "system",
            "content": "You are an AI assistant who knows everything.",
        },
        {
            "role": "user",
            "content": "Tell me, why is the sky blue?"
        },
    ],
)

message = response.choices[0].message.content

print(f"Assistant: {message}")

================================================
File: shared/snippets/open-ai/chat-completion-code/config.json
================================================
{
  "title": "Create a chat completion",
  "docs": "https://docs.aimlapi.com/api-overview/text-models-llm/chat-completion",
  "payload": {
    "baseUrl": "https://api.aimlapi.com/v1",
    "apiKey": "<YOUR_API_KEY>"
  }
}


================================================
File: shared/snippets/open-ai/chat-completion-code/js.hbs
================================================
const { OpenAI } = require('openai');

const api = new OpenAI({
  baseURL: '{{baseUrl}},
  apiKey: '{{apiKey}}',
});

const main = async () => {
  const result = await api.chat.completions.create({
    model: '{{model}}',
    messages: [
      {
        role: 'system',
        content: 'You are SQL code assistant.',
      },
      {
        role: 'user',
        content: 'Could you please provide me with an example of a database structure that I could use for a project in MySQL?'
      }
    ],
  });

  const message = result.choices[0].message.content;
  console.log(\`Assistant: \${message}\`);
};

main();

================================================
File: shared/snippets/open-ai/chat-completion-code/py.hbs
================================================
import os
from openai import OpenAI


def main():
    client = OpenAI(
        api_key="{{apiKey}}",
        base_url="{{baseUrl}}",
    )

    response = client.chat.completions.create(
        model="{{model}}",
        messages=[
            {
                "role": "system",
                "content": "You are SQL code assistant.",
            },
            {
                "role": "user",
                "content": "Could you please provide me with an example of a database structure that I could use for a project in MySQL?",
            },
        ],
    )

    message = response.choices[0].message.content
    print(f"Assistant: {message}")   
    
if __name__ == "__main__":
    main()

================================================
File: shared/snippets/open-ai/chat-completion-o1/config.json
================================================
{
  "title": "Create a chat completion",
  "docs": "https://docs.aimlapi.com/api-overview/text-models-llm/chat-completion",
  "payload": {
    "baseUrl": "https://api.aimlapi.com/v1",
    "apiKey": "<YOUR_API_KEY>"
  }
}


================================================
File: shared/snippets/open-ai/chat-completion-o1/js.hbs
================================================
const { OpenAI } = require('openai');

const api = new OpenAI({
  baseURL: '{{baseUrl}}',
  apiKey: '{{apiKey}}',
});

const main = async () => {
  const result = await api.chat.completions.create({
    model: '{{model}}',
    messages: [
      {
        role: 'user',
        content: 'Tell me, why is the sky blue?'
      }
    ],
  });

  const message = result.choices[0].message.content;
  console.log(`Assistant: ${message}`);
};

main();

================================================
File: shared/snippets/open-ai/chat-completion-o1/py.hbs
================================================
import os
from openai import OpenAI

client = OpenAI(
    base_url="{{baseUrl}}",
    api_key="{{apiKey}}",    
)

response = client.chat.completions.create(
    model="{{model}}",
    messages=[
        {
            "role": "user",
            "content": "Tell me, why is the sky blue?"
        },
    ],
)

message = response.choices[0].message.content

print(f"Assistant: {message}")

================================================
File: shared/snippets/open-ai/completion/config.json
================================================
{
  "title": "Create a chat completion",
  "docs": "https://docs.aimlapi.com/api-overview/text-models-llm/completion-or-chat-models",
  "payload": {
    "baseUrl": "https://api.aimlapi.com/v1",
    "apiKey": "<YOUR_API_KEY>"
  }
}


================================================
File: shared/snippets/open-ai/completion/js.hbs
================================================
const { OpenAI } = require('openai');

const api = new OpenAI({ apiKey: '{{apiKey}}', baseURL: '{{baseUrl}}' });

const main = async () => {
  const prompt = `
All of the states in the USA:
- Alabama, Mongomery;
- Arkansas, Little Rock;
`;
  const response = await api.completions.create({
    prompt,
    model: '{{model}}',
  });
  const text = response.choices[0].text;

  console.log('Completion:', text);
};

main();


================================================
File: shared/snippets/open-ai/completion/py.hbs
================================================
from openai import OpenAI

client = OpenAI(
    api_key="{{apiKey}}",
    base_url="{{baseUrl}}",
)


def main():
    response = client.completions.create(
        model="{{model}}",
        prompt="""
  All of the states in the USA:
  - Alabama, Mongomery;
  - Arkansas, Little Rock;
  """,
    )

    completion = response.choices[0].text
    print(f"Completion: {completion}")


main()

================================================
File: shared/snippets/open-ai/embedding/config.json
================================================
{
  "title": "Create an embedding",
  "docs": "https://docs.aimlapi.com/api-overview/text-models-llm/embeddings",
  "payload": {
    "baseUrl": "https://api.aimlapi.com/v1",
    "apiKey": "<YOUR_API_KEY>"
  }
}


================================================
File: shared/snippets/open-ai/embedding/js.hbs
================================================
const { OpenAI } = require('openai');

const main = async () => {
  const api = new OpenAI({ apiKey: '{{apiKey}}', baseURL: '{{baseUrl}}' });

  const text = 'Your text string goes here';
  const response = await api.embeddings.create({
    input: text,
    model: '{{model}}',
  });
  const embedding = response.data[0].embedding;

  console.log(embedding);
};

main();            

================================================
File: shared/snippets/open-ai/embedding/py.hbs
================================================
import json
from openai import OpenAI


def main():
    client = OpenAI(
        base_url="{{baseUrl}}",
        api_key="{{apiKey}}",
    )

    text = "Your text string goes here"

    response = client.embeddings.create(input=text, model="{{model}}")
    embedding = response.data[0].embedding

    print(json.dumps(embedding, indent=2))


main()   

================================================
File: shared/snippets/open-ai/guard/config.json
================================================
{
    "title": "Content security",
    "payload": {
        "baseUrl": "https://api.aimlapi.com",
        "apiKey": "<YOUR_API_KEY>"
    }
}


================================================
File: shared/snippets/open-ai/guard/js.hbs
================================================
const { OpenAI } = require("openai");

const client = new OpenAI({
    apiKey: "{{apiKey}}",
    baseURL: "{{baseUrl}}",
});

const isPromptSafe = async(prompt) => {
    const response = await client.chat.completions.create({
        model: "{{model}}",
        messages: [
            {
                role: "user",
                content: prompt,
            },
        ],
    });

    const message = response.choices[0].message.content.split(" ")[0];
    return message === "safe";
}

async function main() {
    const prompt = "How to make a homemade bomb?";

    if (await isPromptSafe(prompt)) {
        const response = await client.chat.completions.create({
            model: "<YOUR_MODEL>",
            messages: [
                {
                    role: "user",
                    content: prompt,
                },
            ],
            max_tokens: 1000,
        });

        const message = response.choices[0].message.content;
        console.log(`Assistant: ${message}`);
    } else {
        console.log("Your prompt is not safe");
    }
}


================================================
File: shared/snippets/open-ai/guard/py.hbs
================================================
from openai import OpenAI

client = OpenAI(
    api_key="{{apiKey}}",
    base_url="{{baseUrl}}",
)


def is_prompt_safe(prompt):
    response = client.chat.completions.create(
        model="{{model}}",
        messages=[
            {
                "role": "user",
                "content": prompt
            },
        ],
    )

    message = response.choices[0].message.content.split()[0]
    return True if message == "safe" else False


def main():
    prompt = "How to make a homemade bomb?"

    if is_prompt_safe(prompt):
        response = client.chat.completions.create(
            model="<YOUR_MODEL>",
            messages=[
                {
                    "role": "user",
                    "content": prompt
                },
            ],
            max_tokens=1000,
        )

        message = response.choices[0].message.content
        print(f"Assistant: {message}")
    else:
        print('Your prompt is not safety')


================================================
File: shared/snippets/open-ai/image-generation/config.json
================================================
{
  "title": "Create an image",
  "docs": "https://docs.aimlapi.com/api-overview/image-models/generate-an-image",
  "payload": {
    "baseUrl": "https://api.aimlapi.com/v1",
    "apiKey": "<YOUR_API_KEY>"
  }
}


================================================
File: shared/snippets/open-ai/image-generation/js.hbs
================================================
const axios = require('axios').default;
const fs = require('fs');

const main = async () => {
  const result = await axios.post(
    '{{baseUrl}}/images/generations',
    {
      prompt: 'Hyperrealistic art featuring a cat in costume.',
      model: '{{model}}'
    },
    {
      headers: {
        Authorization: 'Bearer {{apiKey}}',
      },
    },
  );

  fs.writeFileSync('./image.png', result.data.output.choices[0].image_base64, 'base64');
};

main();

================================================
File: shared/snippets/open-ai/image-generation/py.hbs
================================================
import requests
import base64


def main():
    headers = {
        "Authorization": "Bearer {{apiKey}}",
    }

    payload = {
        "prompt": "Hyperrealistic art featuring a cat in costume.",
        "model": "{{model}}",
    }

    response = requests.post(
        "{{baseUrl}}/images/generations", headers=headers, json=payload
    )

    image_base64 = response.json()["output"]["choices"][0]["image_base64"]
    image_data = base64.b64decode(image_base64)

    with open("./image.png", "wb") as file:
        file.write(image_data)

if __name__ == "__main__":
    main()

================================================
File: shared/snippets/runway/create-generation/config.json
================================================
{
  "title": "Generate a video",
  "docs": "https://docs.aimlapi.com/api-overview/video-models/runway/overview",
  "payload": {
    "baseUrl": "https://api.aimlapi.com/v2",
    "apiKey": "<YOUR_API_KEY>"
  }
}


================================================
File: shared/snippets/runway/create-generation/js.hbs
================================================
const main = async () => {
  const response = await fetch('{{baseUrl}}/generate/video/runway/generation', {
    method: 'POST',
    headers: {
      Authorization: 'Bearer {{apiKey}}',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: '{{model}}',
      prompt: 'A jellyfish in the ocean',
      ratio: '16:9',
      image_url: 'https://upload.wikimedia.org/wikipedia/commons/3/35/Maldivesfish2.jpg',
    }),
  }).then((res) => res.json());

  console.log('Generation:', response);
};

main()


================================================
File: shared/snippets/runway/create-generation/py.hbs
================================================
import requests


def main():
    url = "{{baseUrl}}/generate/video/runway/generation"
    payload = {
        "model": "{{model}}",
        "prompt": "A jellyfish in the ocean",
        "ratio": "16:9",
        "image_url": "https://upload.wikimedia.org/wikipedia/commons/3/35/Maldivesfish2.jpg",
    }
    headers = {"Authorization": "Bearer {{apiKey}}", "Content-Type": "application/json"}

    response = requests.post(url, json=payload, headers=headers)
    print("Generation:", response.json())

if __name__ == "__main__":
    main()


================================================
File: shared/snippets/runway/fetch-generation/config.json
================================================
{
  "title": "Fetch a video",
  "docs": "https://docs.aimlapi.com/api-overview/video-models/runway/generate-video-1",
  "payload": {
    "baseUrl": "https://api.aimlapi.com/v2",
    "apiKey": "<YOUR_API_KEY>"
  }
}


================================================
File: shared/snippets/runway/fetch-generation/js.hbs
================================================
const main = async () => {
  const params = new URLSearchParams({
    generation_id: "<YOUR_GENERATION_ID>"
  });
  const response = await fetch(`{{baseUrl}}/generate/video/runway/generation?${params.toString()}`, {
    method: 'GET',
    headers: {
      Authorization: 'Bearer {{apiKey}}',
      'Content-Type': 'application/json',
    },
  }).then((res) => res.json());

  console.log('Generation:', response);
};

main()


================================================
File: shared/snippets/runway/fetch-generation/py.hbs
================================================
import requests


def main():
    url = "{{baseUrl}}/generate/video/runway/generation"
    params = {
        "generation_id": "<YOUR_GENERATION_ID>",
    }
    headers = {"Authorization": "Bearer {{apiKey}}", "Content-Type": "application/json"}

    response = requests.get(url, params=params, headers=headers)
    print("Generation:", response.json())


if __name__ == "__main__":
    main()


================================================
File: shared/snippets/suno-ai/clip-generation/config.json
================================================
{
  "title": "Generate a musical clip",
  "docs": "https://docs.aimlapi.com/api-overview/audio-models-music-and-vocal/suno-ai-v2",
  "payload": {
    "baseUrl": "https://api.aimlapi.com/v2",
    "apiKey": "<YOUR_API_KEY>",
    "model": "chirp-v3-5"
  }
}


================================================
File: shared/snippets/suno-ai/clip-generation/js.hbs
================================================
const main = async () => {
  const prompt = `
    [Verse]
    Silver cities shine brightly
    Skies are painted blue
    Hopes and dreams take flight
    Future starts anew

    [Verse 2]
    Machines hum a new tune
    Worlds we’ve never seen
    Chasing stars so far
    Building our own dream

    [Chorus]
    Future dreams so high
    Touch the endless sky
    Live beyond the now
    Make the future wow

    [Verse 3]
    We create the world
    Technology our guide
    Hearts and minds as one
    Infinite the ride

    [Chorus]
    Future dreams so high
    Touch the endless sky
    Live beyond the now
    Make the future wow

    [Bridge]
    With every beat we rise
    See through wiser eyes
    The places we can go
    A brilliance that will grow
    `;
  const { clip_ids } = await fetch('{{baseUrl}}/generate/audio/suno-ai/clip', {
    method: 'POST',
    headers: {
      Authorization: 'Bearer {{apiKey}}',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      prompt,
      title: 'The Future',
      tags: ['rock'],
    }),
  }).then((res) => res.json());

  console.log('Generated clip ids:', clip_ids);
};

main();


================================================
File: shared/snippets/suno-ai/clip-generation/py.hbs
================================================
import requests


def main():
    prompt = """
    [Verse]
    Silver cities shine brightly
    Skies are painted blue
    Hopes and dreams take flight
    Future starts anew

    [Verse 2]
    Machines hum a new tune
    Worlds we’ve never seen
    Chasing stars so far
    Building our own dream

    [Chorus]
    Future dreams so high
    Touch the endless sky
    Live beyond the now
    Make the future wow

    [Verse 3]
    We create the world
    Technology our guide
    Hearts and minds as one
    Infinite the ride

    [Chorus]
    Future dreams so high
    Touch the endless sky
    Live beyond the now
    Make the future wow

    [Bridge]
    With every beat we rise
    See through wiser eyes
    The places we can go
    A brilliance that will grow
    """

    response = requests.post(
        "{{baseUrl}}/generate/audio/suno-ai/clip",
        headers={
            "Authorization": "Bearer {{apiKey}}",
            "Content-Type": "application/json",
        },
        json={
            "prompt": prompt,
            "title": "The Future",
            "tags": ["rock"],
        },
    )

    response.raise_for_status()
    data = response.json()
    clip_ids = data["clip_ids"]

    print("Generated clip ids:", clip_ids)


if __name__ == "__main__":
    main()


================================================
File: shared/snippets/together-ai/chat-completion-vision/config.json
================================================
{
  "title": "Vision & chat completion",
  "docs": "https://docs.aimlapi.com/api-overview/text-models-llm/function-calling/image-to-text-vision",
  "payload": {
    "baseUrl": "https://api.aimlapi.com/v1",
    "apiKey": "<YOUR_API_KEY>",
    "model": "SG161222/Realistic_Vision_V3.0_VAE"
  }
}


================================================
File: shared/snippets/together-ai/chat-completion-vision/js.hbs
================================================
const main = async () => {
  const result = await fetch('{{baseUrl}}/chat/completions', {
    method: 'POST',
    headers: {
      Authorization: 'Bearer {{apiKey}}',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: '{{model}}',
      max_tokens: 1024,
      messages: [
        {
          role: 'user',
          content: [
            {
              type: 'text',
              text: 'What’s in this image?',
            },
            {
              role: 'user',
              type: 'image_url',
              image_url: {
                url: 'https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg',
              },
            },
          ],
        },
      ],
    }),
  }).then((res) => res.json());

  const message = result.choices[0].message.content;
  console.log(\`Assistant: \${message}\`);
};

main();

================================================
File: shared/snippets/together-ai/chat-completion-vision/py.hbs
================================================
import os
from together import Together

client = Together(base_url="{{baseUrl}}", api_key="{{apiKey}}")

def main():
  response = client.chat.completions.create(
      model="{{model}}",
      messages=[
          {
              "role": "user",
              "content": [
                  {
                      "type": "text",
                      "text": "What sort of animal is in this picture? What is its usual diet? What area is the animal native to? And isn’t there some AI model that’s related to the image?",
                  },
                  {
                      "type": "image_url",
                      "image_url": {
                          "url": "https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/LLama.jpg/444px-LLama.jpg?20050123205659",
                      },
                  },
              ],
          }
      ],
      max_tokens=1024,
  )

  print("Assistant: ", response.choices[0].message.content)

if __name__ == '__main__':
  main()

================================================
File: shared/snippets/video/runway/config.json
================================================
{
    "title": "Generate a video",
    "payload": {
        "baseUrl": "https://api.aimlapi.com/v2",
        "apiKey": "<YOUR_API_KEY>"
    }
}


================================================
File: shared/snippets/video/runway/js.hbs
================================================
const url = "{{baseUrl}}/generate/video/runway/generation";

const headers = {
    "Authorization": `Bearer {{apiKey}}`,
    "Content-Type": "application/json"
};

const generate = async(prompt, imageUrl, duration) => {
    /**
     * Generates a video based on a text prompt and an initial image.
     *
     * @param {string} prompt - The text prompt describing the content or style of the video.
     * @param {string} imageUrl - The URL of the starting image for video generation.
     * @param {string} duration - The duration of the generated video in seconds (e.g., "5" or "10").
     * @returns {Promise<Object>} - The JSON response from the FAL-AI API containing the generated video data.
     * Example:
     * const response = await generate("A bunny eating a carrot in the field.", "https://example.com/image.jpg", "10");
     */

    const payload = {
        provider: "fal-ai",
        model: "{{model}}",
        prompt: prompt,
        image_url: imageUrl,
        last_image_url: imageUrl,
        duration: duration
    };

    const response = await fetch(url, {
        method: 'POST',
        headers: headers,
        body: JSON.stringify(payload)
    });

    return response.json();
}

async function getGenVideo(generationId) {
    /**
     * Retrieves the status or result of a video generation request.
     *
     * @param {string} generationId - The unique identifier for the video generation request.
     * @returns {Promise<void>} - Prints the JSON response containing the status or result of the video generation request.
     * Example:
     * await getGenVideo("999a9a99-a999-9999-99a9-9aa9a99a9aa9:runway-gen3/turbo/image-to-video");
     */

    const payload = {
        generation_id: generationId
    };

    const response = await fetch(url + '?' + new URLSearchParams(payload), {
        method: 'GET',
        headers: headers
    });

    console.log(await response.json());
}


================================================
File: shared/snippets/video/runway/py.hbs
================================================
import requests

url = "{{baseUrl}}/generate/video/runway/generation"
headers = {
    "Authorization": f"Bearer {{apiKey}}",
    "Content-Type": "application/json"
}


def generate(prompt, image_url, duration):
    """
    Generates a video based on a text prompt and an initial image.

    Args:
        prompt (str): The text prompt describing the content or style of the video.
        image_url (str): The URL of the starting image for video generation.
        duration (str): The duration of the generated video in seconds (e.g., "5" or "10").

    Returns:
        dict: The JSON response from the FAL-AI API containing the generated video data.
        {
            "id": "999a9a99-a999-9999-99a9-9aa9a99a9aa9:runway-gen3/turbo/image-to-video",
            "status": "generating"
        }

    Example:
        response = generate("A bunny eating a carrot in the field.", "https://example.com/image.jpg", "10")
    """

    payload = {
        "provider": "fal-ai",
        "model": "{{model}}",
        "prompt": prompt,
        "image_url": image_url,
        "last_image_url": image_url,
        "duration": duration
    }
    response = requests.post(url, json=payload, headers=headers)
    return response.json()


def get_gen_video(generation_id):
    """
    Retrieves the status or result of a video generation request.

    Args:
        generation_id (str): The unique identifier for the video generation request.

    Returns:
        None: Prints the JSON response containing the status or result of the video generation request.
        {
            "id": "904a1d52-f470-4823-85c4-2fb0b84c4bb8:runway-gen3/turbo/image-to-video",
            "status": "completed",
            "video": {
                "url": "https://cdn.aimlapi.com/fal/files/zebra/example_output.mp4",
                "content_type": "video/mp4",
                "file_name": "output.mp4",
                "file_size": 941676
            }
        }
        OR
	    {
		    "status": "generating"
	    }

    Example:
        get_gen_video("999a9a99-a999-9999-99a9-9aa9a99a9aa9:runway-gen3/turbo/image-to-video")
"""
    payload = {'generation_id': generation_id}
    response = requests.get(url, params=payload, headers=headers)
    print(response.json())

================================================
File: shared/snippets/voice/stt/config.json
================================================
{
  "title": "Speech to Text",
  "docs": "https://docs.aimlapi.com/api-overview/speech-models/speech-to-text-stt",
  "payload": {
    "baseUrl": "https://api.aimlapi.com/v1",
    "apiKey": "<YOUR_API_KEY>"
  }
}


================================================
File: shared/snippets/voice/stt/js.hbs
================================================
const axios = require('axios').default;

const api = new axios.create({
  baseURL: '{{baseUrl}}',
  headers: { Authorization: 'Bearer {{apiKey}}' },
});

const main = async () => {
  const response = await api.post('/stt', {
    model: '{{model}}',
    url: 'https://audio-samples.github.io/samples/mp3/blizzard_unconditional/sample-0.mp3',
  });

  console.log('[transcription]', response.data.results.channels[0].alternatives[0].transcript);
};

main();

================================================
File: shared/snippets/voice/stt/py.hbs
================================================
import requests


headers = {"Authorization": "Bearer {{apiKey}}"}


def main():
    url = f"{{baseUrl}}/stt"
    data = {
        "model": "{{model}}",
        "url": "https://audio-samples.github.io/samples/mp3/blizzard_unconditional/sample-0.mp3",
    }

    response = requests.post(url, json=data, headers=headers)

    if response.status_code >= 400:
        print(f"Error: {response.status_code} - {response.text}")
    else:
        response_data = response.json()
        transcript = response_data["results"]["channels"][0]["alternatives"][0][
            "transcript"
        ]
        print("[transcription]", transcript)

if __name__ == "__main__":
    main()

================================================
File: shared/snippets/voice/tts/config.json
================================================
{
  "title": "Text to Speech",
  "docs": "https://docs.aimlapi.com/api-overview/speech-models/text-to-speech-tts",
  "payload": {
    "baseUrl": "https://api.aimlapi.com/v1",
    "apiKey": "<YOUR_API_KEY>"
  }
}


================================================
File: shared/snippets/voice/tts/js.hbs
================================================
const fs = require('fs');
const path = require('path');

const axios = require('axios').default;
const api = new axios.create({
  baseURL: '{{baseUrl}}',
  headers: { Authorization: 'Bearer {{apiKey}}' },
});

const main = async () => {
  const response = await api.post(
    '/tts',
    {
      model: '{{model}}',
      text: 'Hi! What are you doing today?',
    },
    { responseType: 'stream' },
  );

  const dist = path.resolve(__dirname, './audio.wav');
  const writeStream = fs.createWriteStream(dist);

  response.data.pipe(writeStream);

  writeStream.on('close', () => console.log('Audio saved to:', dist));
};

main();

================================================
File: shared/snippets/voice/tts/py.hbs
================================================
import os
import requests


def main():
    url = "{{baseUrl}}/tts"
    headers = {
        "Authorization": "Bearer {{apiKey}}",
    }
    payload = {
        "model": "{{model}}",
        "text": "Hi! What are you doing today?",
    }

    response = requests.post(url, headers=headers, json=payload, stream=True)
    dist = os.path.join(os.path.dirname(__file__), "audio.wav")

    with open(dist, "wb") as write_stream:
        for chunk in response.iter_content(chunk_size=8192):
            if chunk:
                write_stream.write(chunk)

    print("Audio saved to:", dist)


main()
    

================================================
File: .github/workflows/deploy-snippets-assets.yml
================================================
name: Deploy Snippets Assets

on:
  push:
    branches:
      - main

jobs:
  build-and-deploy:
    runs-on: ubuntu-latest
    environment: production
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

      - name: Setup Node.js
        uses: actions/setup-node@v3
        with:
          node-version: 20

      - name: Install pnpm
        run: npm install -g pnpm

      - name: Install dependencies
        working-directory: ./packages/snippets
        run: pnpm install

      - name: Build project
        working-directory: ./packages/snippets
        env:
          SNIPPETS_CDN_PREFIX: ${{ secrets.SNIPPETS_CDN_PREFIX }}
        run: pnpm run build

      - name: Configure AWS for MinIO
        env:
          S3_ACCESS_KEY_ID: ${{ secrets.SNIPPETS_S3_ACCESS_KEY_ID }}
          S3_SECRET_ACCESS_KEY: ${{ secrets.SNIPPETS_S3_SECRET_ACCESS_KEY }}

        run: |
          aws configure set aws_access_key_id "$S3_ACCESS_KEY_ID"
          aws configure set aws_secret_access_key "$S3_SECRET_ACCESS_KEY"
          aws configure set default.region "us-east-1"

      - name: Upload to MinIO S3 using AWS CLI
        env:
          S3_BUCKET: ${{ secrets.SNIPPETS_S3_BUCKET }}
          S3_PREFIX: ${{ secrets.SNIPPETS_S3_PREFIX }}
          S3_ENDPOINT: ${{ secrets.SNIPPETS_S3_ENDPOINT }}
        run: |
          aws --endpoint-url ${S3_ENDPOINT} s3 sync ./packages/snippets/dist s3://${S3_BUCKET}/${S3_PREFIX}/ --delete --exact-timestamps

      - name: Trigger Cache Purge
        env:
          CLOUDFLARE_API_TOKEN: ${{ secrets.SNIPPETS_CLOUDFLARE_API_TOKEN }}
          CLOUDFLARE_ZONE_ID: ${{ secrets.SNIPPETS_CLOUDFLARE_ZONE_ID }}
          CDN_PURGE_PAYLOAD: ${{ secrets.SNIPPETS_CDN_PURGE_PAYLOAD }}
        run: |
          curl -X POST "https://api.cloudflare.com/client/v4/zones/$CLOUDFLARE_ZONE_ID/purge_cache" \
          -H "Authorization: Bearer $CLOUDFLARE_API_TOKEN" \
          -H "Content-Type: application/json" \
          --data "$CDN_PURGE_PAYLOAD"


